{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote 10 EPD records to '../data/pipeline2/json/edited_epds.jsonl' in JSONLines format.\n"
     ]
    }
   ],
   "source": [
    "# Add The International EPD System\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import jsonlines\n",
    "import re\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. PARSE THE XML TO BUILD A CATEGORY LOOKUP STRUCTURE\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_category_dict(xml_element):\n",
    "    category_id = xml_element.get('id')\n",
    "    category_name = xml_element.get('name')\n",
    "    \n",
    "    children_dict = {}\n",
    "    for child_elem in xml_element.findall('{*}category'):\n",
    "        child_data = build_category_dict(child_elem)\n",
    "        children_dict[child_data['name']] = child_data\n",
    "\n",
    "    return {\n",
    "        'id': category_id,\n",
    "        'name': category_name,\n",
    "        'children': children_dict\n",
    "    }\n",
    "\n",
    "def parse_category_xml(xml_path):\n",
    "    tree = ET.parse(xml_path, parser=ET.XMLParser(encoding='utf-8'))\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    category_system = root.find('.//{*}categories[@dataType=\"Process\"]')\n",
    "    top_level_dict = {}\n",
    "    if category_system is not None:\n",
    "        for cat_elem in category_system.findall('{*}category'):\n",
    "            cat_data = build_category_dict(cat_elem)\n",
    "            top_level_dict[cat_data['name']] = cat_data\n",
    "    else:\n",
    "        print(\"Warning: Could not find <categories dataType='Process'> in XML.\")\n",
    "    \n",
    "    return top_level_dict\n",
    "\n",
    "def get_category_path_info(category_hierarchy, categories_dict):\n",
    "    path_info = []\n",
    "    if not category_hierarchy:\n",
    "        return path_info\n",
    "\n",
    "    current_dict = categories_dict\n",
    "    for i, cat_name in enumerate(category_hierarchy):\n",
    "        if cat_name not in current_dict:\n",
    "            raise ValueError(f\"Category '{cat_name}' not found at level {i}.\")\n",
    "        this_cat = current_dict[cat_name]\n",
    "        path_info.append((this_cat['name'], this_cat['id']))\n",
    "        current_dict = this_cat['children']\n",
    "    return path_info\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STEP 1.1: SELECT + CHANGE CATEGORY, PRIORITIZING \"compressive\" OR \"density\"\n",
    "#           Then rewrite materialProperties to a standard form and output to JSONLines\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "xml_path = \"../data/pipeline2/xml/OEKOBAU.DAT_Categories_EN_API.xml\"\n",
    "category_dict = parse_category_xml(xml_path)\n",
    "\n",
    "db_path = \"../data/pipeline2/sql/epd_database.sqlite\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "csv_path = \"../data/pipeline2/sql/regex_classified/filtered_epd_data02_classified_concrete03.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "num_epd_files = 10  # limit to 50 EPDs\n",
    "df_limited = df  # We'll do our own selection logic\n",
    "\n",
    "modified_json_docs = {}\n",
    "TARGET_CLASSIFICATION = \"Mineral building products > Mortar and Concrete > Ready mixed concrete\"\n",
    "selected_uuids = []\n",
    "\n",
    "# Helper function to unify materialProperties\n",
    "def unify_material_property(mp):\n",
    "    \"\"\"\n",
    "    If mp[\"name\"] contains 'compressive', rename to 'compressive strength',\n",
    "       unit->'MPa', unitDescription->'megapascals'\n",
    "    If mp[\"name\"] contains 'density', rename to 'gross density',\n",
    "       unit->'kg/m^3', unitDescription->'kilograms per cubic metre'\n",
    "    Remove trailing parentheses from original name if it helps match.\n",
    "\n",
    "    Note: We keep mp[\"value\"] as is. Adjust if you want different numeric behavior.\n",
    "    \"\"\"\n",
    "    name_lower = mp[\"name\"].lower()\n",
    "\n",
    "    # Remove any trailing parentheses chunk (e.g. ' (kg/m^3)')\n",
    "    # This is optional if you want to remove that from the property name.\n",
    "    \n",
    "    mp[\"name\"] = re.sub(r\"\\(.*?\\)\", \"\", mp[\"name\"]).strip()\n",
    "\n",
    "    if \"compressive\" in name_lower:\n",
    "        mp[\"name\"] = \"compressive strength\"\n",
    "        mp[\"unit\"] = \"MPa\"\n",
    "        mp[\"unitDescription\"] = \"megapascals\"\n",
    "    elif \"density\" in name_lower:\n",
    "        mp[\"name\"] = \"gross density\"\n",
    "        mp[\"unit\"] = \"kg/m^3\"\n",
    "        mp[\"unitDescription\"] = \"kilograms per cubic metre\"\n",
    "\n",
    "for index, row in df_limited.iterrows():\n",
    "    if len(selected_uuids) >= num_epd_files:\n",
    "        break\n",
    "\n",
    "    uuid_val = row[\"UUID\"]\n",
    "    classification_str = row[\"RegEx Classification\"]\n",
    "\n",
    "    # Only process rows with the target classification\n",
    "    if classification_str == TARGET_CLASSIFICATION:\n",
    "        # 1) Fetch JSON from epd_documents\n",
    "        cursor.execute(\"SELECT document FROM epd_documents WHERE uuid = ?\", (uuid_val,))\n",
    "        result = cursor.fetchone()\n",
    "        if not result:\n",
    "            continue\n",
    "\n",
    "        json_text = result[0]\n",
    "        try:\n",
    "            json_data = json.loads(json_text)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        # 2) Check if there's a \"compressive\" and \"density\" entry in materialProperties\n",
    "        found_property_of_interest = False\n",
    "        compressive_found = False\n",
    "        density_found = False\n",
    "\n",
    "        exchanges = json_data.get(\"exchanges\", {}).get(\"exchange\", [])\n",
    "        for exch in exchanges:\n",
    "            mat_props = exch.get(\"materialProperties\", [])\n",
    "            for mp in mat_props:\n",
    "                name_lower = mp.get(\"name\", \"\").lower()\n",
    "                # Check for \"compressive\" only in this property\n",
    "                if \"compressive\" in name_lower and \"density\" not in name_lower:\n",
    "                    compressive_found = True\n",
    "                # Check for \"density\" only in this property\n",
    "                if \"density\" in name_lower and \"compressive\" not in name_lower:\n",
    "                    density_found = True\n",
    "                # If both have been found in two separate properties, we can stop searching\n",
    "                if compressive_found and density_found:\n",
    "                    found_property_of_interest = True\n",
    "                    break\n",
    "            if found_property_of_interest:\n",
    "                break\n",
    "\n",
    "        if not found_property_of_interest:\n",
    "            continue  # skip EPD if no two separate relevant property names\n",
    "\n",
    "        # 3) Build new classification array from the CSV \"RegEx Classification\"\n",
    "        category_names = [x.strip() for x in classification_str.split(\">\")]\n",
    "        try:\n",
    "            path_info = get_category_path_info(category_names, category_dict)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        class_array = []\n",
    "        for level, (cat_name, cat_id) in enumerate(path_info):\n",
    "            class_array.append({\n",
    "                \"value\": cat_name,\n",
    "                \"level\": level,\n",
    "                \"classId\": cat_id\n",
    "            })\n",
    "\n",
    "        new_classification_item = {\n",
    "            \"class\": class_array,\n",
    "            \"name\": \"OEKOBAU.DAT\"\n",
    "        }\n",
    "        \n",
    "        # 4) Inject new classification while preserving existing\n",
    "        process_info = json_data.setdefault(\"processInformation\", {})\n",
    "        data_set_info = process_info.setdefault(\"dataSetInformation\", {})\n",
    "        classification_info = data_set_info.setdefault(\"classificationInformation\", {})\n",
    "        existing_classifications = classification_info.get(\"classification\")\n",
    "        if not isinstance(existing_classifications, list):\n",
    "            existing_classifications = []\n",
    "        existing_classifications.append(new_classification_item)\n",
    "        classification_info[\"classification\"] = existing_classifications\n",
    "\n",
    "        # 5) Now unify each relevant material property\n",
    "        #    We'll loop through all exchanges, all mat_props\n",
    "        #    rewriting as needed\n",
    "        for exch in exchanges:\n",
    "            mat_props = exch.get(\"materialProperties\", [])\n",
    "            for mp in mat_props:\n",
    "                if \"compressive\" in mp.get(\"name\", \"\").lower() or \"density\" in mp.get(\"name\", \"\").lower():\n",
    "                    unify_material_property(mp)\n",
    "\n",
    "        # Remove unwanted exchanges entries\n",
    "        # For each exchange, filter out any anies item with module \"A1\", \"A2\", or \"A3\"\n",
    "            other = exch.get(\"other\", {})\n",
    "            anies = other.get(\"anies\", [])\n",
    "            filtered_anies = [item for item in anies if item.get(\"module\") not in [\"A1\", \"A2\", \"A3\"]]\n",
    "            other[\"anies\"] = filtered_anies\n",
    "\n",
    "        # 6) Remove unwanted LCIAResults entries\n",
    "        #    For each LCIAResult, filter out any anies item with module \"A1\", \"A2\", or \"A3\"\n",
    "        lcia_results = json_data.get(\"LCIAResults\", {}).get(\"LCIAResult\", [])\n",
    "        for lcia in lcia_results:\n",
    "            other = lcia.get(\"other\", {})\n",
    "            anies = other.get(\"anies\", [])\n",
    "            filtered_anies = [item for item in anies if item.get(\"module\") not in [\"A1\", \"A2\", \"A3\"]]\n",
    "            other[\"anies\"] = filtered_anies\n",
    "\n",
    "        # 7) Store the updated JSON\n",
    "        modified_json_docs[uuid_val] = json_data\n",
    "        selected_uuids.append(uuid_val)\n",
    "        # print(f\"Selected EPD with UUID={uuid_val}: found 'compressive' or 'density'. Classification updated.\")\n",
    "        # print(\"--------------------------------------------------\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# At this point, \"modified_json_docs\" has up to 50 EPDs that we updated.\n",
    "\n",
    "# Write out the results to JSONLines\n",
    "output_jsonl = \"../data/pipeline2/json/edited_epds.jsonl\"\n",
    "\n",
    "with jsonlines.open(output_jsonl, mode='w') as writer:\n",
    "    idx = 1\n",
    "    for epd_uuid in selected_uuids:\n",
    "        doc = modified_json_docs[epd_uuid]\n",
    "        # We can try to get the classification system from the doc:\n",
    "        classifications = (\n",
    "            doc.get(\"processInformation\", {})\n",
    "               .get(\"dataSetInformation\", {})\n",
    "               .get(\"classificationInformation\", {})\n",
    "               .get(\"classification\", [])\n",
    "        )\n",
    "        existing_class_sys = classifications[0].get(\"name\") if classifications else \"Unknown\"\n",
    "\n",
    "        # product name from baseName\n",
    "        base_names = (\n",
    "            doc.get(\"processInformation\", {})\n",
    "               .get(\"dataSetInformation\", {})\n",
    "               .get(\"name\", {})\n",
    "               .get(\"baseName\", [])\n",
    "        )\n",
    "        epd_name = base_names[0].get(\"value\", \"\") if base_names else \"\"\n",
    "\n",
    "        record = {\n",
    "            \"id\": idx,\n",
    "            \"uuid\": epd_uuid,\n",
    "            \"epd_name\": epd_name,\n",
    "            \"classificationSys\": existing_class_sys,\n",
    "            \"document\": doc\n",
    "        }\n",
    "        writer.write(record)\n",
    "        idx += 1\n",
    "\n",
    "print(f\"\\nWrote {len(selected_uuids)} EPD records to '{output_jsonl}' in JSONLines format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OEKOBAU.DAT\n",
      "Found 260 matching EPD(s) overall. Now parsing compressive strength...\n",
      "Total EPDs after compressive strength parse: 245\n",
      "Will output 10 EPDs. (Needed at least 10.)\n",
      "\n",
      "IBUCategories\n",
      "Found 334 matching EPD(s) overall. Now parsing compressive strength...\n",
      "Total EPDs after compressive strength parse: 290\n",
      "Will output 10 EPDs. (Needed at least 10.)\n",
      "\n",
      "Total EPDs => OEKOBAU.DAT: 10, IBUCategories: 10, Combined: 20\n",
      "\n",
      "Added 20 to modified_json_docs\n",
      "\n",
      "Final combined JSONLines => '../data/pipeline2/json/edited_epds.jsonl' with 20 records.\n"
     ]
    }
   ],
   "source": [
    "# Add OEKOBAU.DAT and IBUCategories\n",
    "\n",
    "import re\n",
    "import json\n",
    "import sqlite3\n",
    "import jsonlines\n",
    "\n",
    "db_path = \"../data/pipeline2/sql/epd_database.sqlite\"\n",
    "category_val = \"Beton\"\n",
    "max_items = 10\n",
    "\n",
    "\n",
    "# This function remains unchanged except we add a param classification_sys_label\n",
    "# so we know which classification system we processed.\n",
    "def fetch_well_defined_epds(conn, classification_system, category_value):\n",
    "    \"\"\"\n",
    "    Fetch EPDs from epd_documents + epd_metadata where\n",
    "      m.classification_system (case-insensitive) = classification_system,\n",
    "      the JSON classification array has {\"value\": category_value},\n",
    "      and materialProperties[*].name includes 'compressive' or 'density'.\n",
    "\n",
    "    Returns: (classification_name_in_json, results_list)\n",
    "    where results_list is [(uuid, doc_data)].\n",
    "    \"\"\"\n",
    "    sql = \"\"\"\n",
    "    SELECT d.uuid, d.document\n",
    "    FROM epd_documents d\n",
    "    JOIN epd_metadata m ON d.uuid = m.uuid\n",
    "    WHERE m.classification_system COLLATE NOCASE = ?\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql, (classification_system,))\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    results = []\n",
    "    classification_system_in_json = None\n",
    "\n",
    "    for doc_uuid, doc_text in rows:\n",
    "        if not doc_text:\n",
    "            continue\n",
    "        try:\n",
    "            doc_data = json.loads(doc_text)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        # 1) Must have \"Beton\" in classification\n",
    "        classifications = (\n",
    "            doc_data.get(\"processInformation\", {})\n",
    "            .get(\"dataSetInformation\", {})\n",
    "            .get(\"classificationInformation\", {})\n",
    "            .get(\"classification\", [])\n",
    "        )\n",
    "        found_beton = False\n",
    "        classification_name_for_this_doc = None\n",
    "\n",
    "        for classification_item in classifications:\n",
    "            for cls_obj in classification_item.get(\"class\", []):\n",
    "                if cls_obj.get(\"value\") == category_value:\n",
    "                    found_beton = True\n",
    "                    classification_name_for_this_doc = classification_item.get(\"name\")\n",
    "                    break\n",
    "            if found_beton:\n",
    "                break\n",
    "        if not found_beton:\n",
    "            continue\n",
    "\n",
    "        # 2) Must have 'compressive' or 'density' in materialProperties\n",
    "        exchanges = doc_data.get(\"exchanges\", {}).get(\"exchange\", [])\n",
    "        found_property = False\n",
    "        for exch in exchanges:\n",
    "            mat_props = exch.get(\"materialProperties\", [])\n",
    "            for mp in mat_props:\n",
    "                name_lower = mp.get(\"name\", \"\").lower()\n",
    "                if \"compressive\" in name_lower or \"density\" in name_lower:\n",
    "                    found_property = True\n",
    "                    break\n",
    "            if found_property:\n",
    "                break\n",
    "        if not found_property:\n",
    "            continue\n",
    "\n",
    "        # Record doc\n",
    "        results.append((doc_uuid, doc_data))\n",
    "        if classification_name_for_this_doc:\n",
    "            classification_system_in_json = classification_name_for_this_doc\n",
    "\n",
    "    return classification_system_in_json, results\n",
    "\n",
    "\n",
    "def process_epds_and_limit(epds, classification_name_in_json, classification_label):\n",
    "    \"\"\"\n",
    "    Parse compressive strength from baseName, insert 'compressive strength' property,\n",
    "    exclude if missing pattern or no exchange, then slice to 50.\n",
    "    Return the final processed list of (uuid, doc, classification_label).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    print(f\"\\n{classification_label}\")\n",
    "    print(f\"Found {len(epds)} matching EPD(s) overall. Now parsing compressive strength...\")\n",
    "\n",
    "    pattern = r\"C\\s*(\\d+)\\s*/\\s*(\\d+)\"\n",
    "    final_list = []\n",
    "\n",
    "    for epd_uuid, epd_doc in epds:\n",
    "        # 1) product_name from baseName[0].value\n",
    "        base_names = (\n",
    "            epd_doc.get(\"processInformation\", {})\n",
    "            .get(\"dataSetInformation\", {})\n",
    "            .get(\"name\", {})\n",
    "            .get(\"baseName\", [])\n",
    "        )\n",
    "        if not base_names or not isinstance(base_names, list):\n",
    "            continue\n",
    "        product_name = base_names[0].get(\"value\", \"\")\n",
    "        if not product_name:\n",
    "            continue\n",
    "\n",
    "        # 2) parse \"Cxx/yy\"\n",
    "        match = re.search(pattern, product_name, flags=re.IGNORECASE)\n",
    "        if not match:\n",
    "            continue\n",
    "        compressive_strength_value = match.group(2)\n",
    "\n",
    "        # 3) Insert property in first exchange\n",
    "        exchanges = epd_doc.get(\"exchanges\", {}).get(\"exchange\", [])\n",
    "        if not exchanges:\n",
    "            continue\n",
    "        first_exchange = exchanges[0]\n",
    "        mat_props = first_exchange.setdefault(\"materialProperties\", [])\n",
    "        cs_entry = {\n",
    "            \"name\": \"compressive strength\",\n",
    "            \"value\": compressive_strength_value,\n",
    "            \"unit\": \"MPa\",\n",
    "            \"unitDescription\": \"megapascals\",\n",
    "        }\n",
    "        mat_props.append(cs_entry)\n",
    "\n",
    "        # 4) classification => fallback if we don't have classification_name_in_json\n",
    "        classification_to_store = (\n",
    "            classification_name_in_json\n",
    "            if classification_name_in_json\n",
    "            else classification_label\n",
    "        )\n",
    "\n",
    "        final_list.append((epd_uuid, epd_doc, classification_to_store))\n",
    "\n",
    "    # slice to 50\n",
    "    print(f\"Total EPDs after compressive strength parse: {len(final_list)}\")\n",
    "    final_list = final_list[:max_items]\n",
    "    print(f\"Will output {len(final_list)} EPDs. (Needed at least {max_items}.)\")\n",
    "    return final_list\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# 1) For OEKOBAU.DAT\n",
    "classification_in_json_oeko, all_epds_oeko = fetch_well_defined_epds(\n",
    "    conn, classification_system=\"OEKOBAU.DAT\", category_value=category_val\n",
    ")\n",
    "final_epds_oeko = process_epds_and_limit(\n",
    "    all_epds_oeko, classification_in_json_oeko, classification_label=\"OEKOBAU.DAT\"\n",
    ")\n",
    "\n",
    "# 2) For IBUCategories\n",
    "classification_in_json_ibu, all_epds_ibu = fetch_well_defined_epds(\n",
    "    conn, classification_system=\"IBUCategories\", category_value=category_val\n",
    ")\n",
    "final_epds_ibu = process_epds_and_limit(\n",
    "    all_epds_ibu, classification_in_json_ibu, classification_label=\"IBUCategories\"\n",
    ")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Merge them. We'll store them all in a single JSONLines file\n",
    "combined_epds = final_epds_oeko + final_epds_ibu\n",
    "print(\n",
    "    f\"\\nTotal EPDs => OEKOBAU.DAT: {len(final_epds_oeko)}, IBUCategories: {len(final_epds_ibu)}, Combined: {len(combined_epds)}\"\n",
    ")\n",
    "\n",
    "\n",
    "modified_json_docs_nr = len(modified_json_docs)\n",
    "\n",
    "# Write to JSONLines\n",
    "output_jsonl = \"../data/pipeline2/json/edited_epds.jsonl\"\n",
    "with jsonlines.open(output_jsonl, mode=\"a\") as writer:\n",
    "    idx = len(modified_json_docs) + 1\n",
    "    for epd_uuid, epd_doc, classification_str in combined_epds:\n",
    "        # product name\n",
    "        base_names = (\n",
    "            epd_doc.get(\"processInformation\", {})\n",
    "            .get(\"dataSetInformation\", {})\n",
    "            .get(\"name\", {})\n",
    "            .get(\"baseName\", [])\n",
    "        )\n",
    "        product_name = base_names[0].get(\"value\", \"\") if base_names else \"\"\n",
    "\n",
    "        # Add to modified dict\n",
    "        modified_json_docs[epd_uuid] = epd_doc\n",
    "        # print(f\"Added {epd_uuid} to modified_json_docs\")\n",
    "\n",
    "        record = {\n",
    "            \"id\": idx,\n",
    "            \"uuid\": epd_uuid,\n",
    "            \"epd_name\": product_name,\n",
    "            \"classificationSys\": classification_str,\n",
    "            \"document\": epd_doc,\n",
    "        }\n",
    "        writer.write(record)\n",
    "        idx += 1\n",
    "\n",
    "    print(f\"\\nAdded {len(modified_json_docs) - modified_json_docs_nr} to modified_json_docs\\n\")\n",
    "\n",
    "print(\n",
    "    f\"Final combined JSONLines => '{output_jsonl}' with {len(combined_epds)} records.\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# To do: Refactor to add compressive strength and gross density\n",
    "# Add from EPDNorge\n",
    "import json\n",
    "import jsonlines\n",
    "import sqlite3\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# =============================================================================\n",
    "# Part 1: Build the results list from JSON files with added ids\n",
    "# =============================================================================\n",
    "\n",
    "# File paths (adjust if needed)\n",
    "context_path = \"../data/pipeline2/json/EPDNorge_context.json\"\n",
    "batch_output_path = \"../data/pipeline2/json/openai/EPDNorge_concrete_batch_output.jsonl\"\n",
    "\n",
    "# 1. Read context data\n",
    "with open(context_path, 'r', encoding='utf-8') as f:\n",
    "    context_data = json.load(f)\n",
    "\n",
    "# 2. Prepare a list to hold the resulting records with added ids\n",
    "results = []\n",
    "with jsonlines.open(batch_output_path) as reader:\n",
    "    for idx, batch_record in enumerate(reader):\n",
    "        # Extract the \"content\" field, which is a JSON string\n",
    "        content_str = batch_record[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        # Parse the content string into a JSON object\n",
    "        content_json = json.loads(content_str)\n",
    "        best_category = content_json[\"best_category\"]\n",
    "\n",
    "        # Create the new entry with an added \"id\" key\n",
    "        new_entry = {\n",
    "            \"id\": idx,\n",
    "            \"Product\": context_data[idx][\"Product\"],\n",
    "            \"UUID\": context_data[idx][\"UUID\"],\n",
    "            \"best_category\": best_category\n",
    "        }\n",
    "        results.append(new_entry)\n",
    "\n",
    "# print(\"---- Results JSON ----\")\n",
    "# print(json.dumps(results, indent=2, ensure_ascii=False))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Part 2: Extract unique categories and filter by the target classification\n",
    "# =============================================================================\n",
    "\n",
    "# Extract unique categories\n",
    "unique_categories = {entry[\"best_category\"] for entry in results}\n",
    "# print(\"\\n---- Unique Categories ----\")\n",
    "# print(json.dumps(list(unique_categories), indent=2, ensure_ascii=False))\n",
    "\n",
    "# Filter the results by the specified category\n",
    "TARGET_CLASSIFICATION = \"Mineral building products > Mortar and Concrete > Ready mixed concrete\"\n",
    "filtered_results = [\n",
    "    entry for entry in results \n",
    "    if entry[\"best_category\"] == TARGET_CLASSIFICATION\n",
    "]\n",
    "# print(\"\\n---- Filtered Results ----\")\n",
    "# print(json.dumps(filtered_results, indent=2, ensure_ascii=False))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Part 3: Update JSON documents from SQLite based on filtered results\n",
    "#         Prioritizing EPDs with \"compressive\" or \"density\" in \n",
    "#         exchanges.exchange.materialProperties[*].name;\n",
    "#         If we don't reach 50 that way, fill from others.\n",
    "# =============================================================================\n",
    "\n",
    "# --- XML Parsing Functions ---\n",
    "def build_category_dict(xml_element):\n",
    "    category_id = xml_element.get('id')\n",
    "    category_name = xml_element.get('name')\n",
    "    \n",
    "    children_dict = {}\n",
    "    for child_elem in xml_element.findall('{*}category'):\n",
    "        child_data = build_category_dict(child_elem)\n",
    "        children_dict[child_data['name']] = child_data\n",
    "\n",
    "    return {\n",
    "        'id': category_id,\n",
    "        'name': category_name,\n",
    "        'children': children_dict\n",
    "    }\n",
    "\n",
    "def parse_category_xml(xml_path):\n",
    "    tree = ET.parse(xml_path, parser=ET.XMLParser(encoding='utf-8'))\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    category_system = root.find('.//{*}categories[@dataType=\"Process\"]')\n",
    "    top_level_dict = {}\n",
    "    if category_system is not None:\n",
    "        for cat_elem in category_system.findall('{*}category'):\n",
    "            cat_data = build_category_dict(cat_elem)\n",
    "            top_level_dict[cat_data['name']] = cat_data\n",
    "    else:\n",
    "        print(\"Warning: Could not find <categories dataType='Process'> in XML.\")\n",
    "    \n",
    "    return top_level_dict\n",
    "\n",
    "def get_category_path_info(category_hierarchy, categories_dict):\n",
    "    \"\"\"Given a list of category names and your category_dict, \n",
    "       return [(name, id), (name, id), ...] for each level.\"\"\"\n",
    "    path_info = []\n",
    "    if not category_hierarchy:\n",
    "        return path_info\n",
    "\n",
    "    current_dict = categories_dict\n",
    "    for i, cat_name in enumerate(category_hierarchy):\n",
    "        if cat_name not in current_dict:\n",
    "            raise ValueError(f\"Category '{cat_name}' not found at level {i}.\")\n",
    "        this_cat = current_dict[cat_name]\n",
    "        path_info.append((this_cat['name'], this_cat['id']))\n",
    "        current_dict = this_cat['children']\n",
    "    return path_info\n",
    "\n",
    "# Parse the XML to build the category lookup structure\n",
    "xml_path = \"../data/pipeline2/xml/OEKOBAU.DAT_Categories_EN_API.xml\"\n",
    "category_dict = parse_category_xml(xml_path)\n",
    "\n",
    "# Connect to the SQLite DB\n",
    "db_path = \"../data/pipeline2/sql/epd_database.sqlite\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# We'll separate EPDs into two lists: \n",
    "#   priority_list = those that have \"compressive\" or \"density\" \n",
    "#   secondary_list = the rest\n",
    "priority_list = []\n",
    "secondary_list = []\n",
    "\n",
    "for entry in filtered_results:\n",
    "    uuid_val = entry[\"UUID\"]\n",
    "\n",
    "    # Fetch JSON from epd_documents table by UUID\n",
    "    cursor.execute(\"SELECT document FROM epd_documents WHERE uuid = ?\", (uuid_val,))\n",
    "    row = cursor.fetchone()\n",
    "    if not row:\n",
    "        print(f\"No document found for UUID: {uuid_val}\")\n",
    "        continue\n",
    "\n",
    "    doc_text = row[0]\n",
    "    try:\n",
    "        doc_data = json.loads(doc_text)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Failed to parse JSON for UUID: {uuid_val}\")\n",
    "        continue\n",
    "\n",
    "    # Check if \"compressive\" or \"density\" is present\n",
    "    has_compressive_or_density = False\n",
    "    exchanges = doc_data.get(\"exchanges\", {}).get(\"exchange\", [])\n",
    "    for exch in exchanges:\n",
    "        mat_props = exch.get(\"materialProperties\", [])\n",
    "        for mp in mat_props:\n",
    "            name_lower = mp.get(\"name\", \"\").lower()\n",
    "            if \"compressive\" in name_lower or \"density\" in name_lower:\n",
    "                has_compressive_or_density = True\n",
    "                break\n",
    "        if has_compressive_or_density:\n",
    "            break\n",
    "\n",
    "    # We'll store all relevant info needed to do the classification injection later\n",
    "    # e.g., (uuid, doc_data)\n",
    "    if has_compressive_or_density:\n",
    "        priority_list.append((uuid_val, doc_data))\n",
    "    else:\n",
    "        secondary_list.append((uuid_val, doc_data))\n",
    "\n",
    "conn.close()\n",
    "\n",
    "#  We'll pick up to 50 EPDs:\n",
    "#     1) all from priority_list (or as many as we have),\n",
    "#     2) if still under 50, fill from secondary_list\n",
    "final_list = []\n",
    "MAX_EPD_COUNT = 50\n",
    "\n",
    "# Add from priority_list first\n",
    "for (u, d) in priority_list:\n",
    "    if len(final_list) >= MAX_EPD_COUNT:\n",
    "        break\n",
    "    final_list.append((u, d))\n",
    "\n",
    "# If not enough, fill from secondary_list\n",
    "if len(final_list) < MAX_EPD_COUNT:\n",
    "    needed = MAX_EPD_COUNT - len(final_list)\n",
    "    final_list.extend(secondary_list[:needed])\n",
    "\n",
    "print(f\"\\nCollected {len(final_list)} EPDs. \"\n",
    "      f\"({len(priority_list)} had 'compressive/density', \"\n",
    "      f\"{len(secondary_list)} did not.)\")\n",
    "\n",
    "# Hold the data if not defined in a previous cell\n",
    "modified_json_docs = {}\n",
    "\n",
    "# Now, we inject the classification for each doc.\n",
    "classification_system_in_json = None\n",
    "for (uuid_val, json_data) in final_list:\n",
    "    # Because all these were filtered by best_category = TARGET_CLASSIFICATION,\n",
    "    # we can build the classification array from that same string.\n",
    "    classification_str = TARGET_CLASSIFICATION\n",
    "    category_names = [x.strip() for x in classification_str.split(\">\")]\n",
    "    try:\n",
    "        path_info = get_category_path_info(category_names, category_dict)\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: Could not resolve categories for UUID: {uuid_val}. {e}\")\n",
    "        continue\n",
    "\n",
    "    class_array = []\n",
    "    for level, (cat_name, cat_id) in enumerate(path_info):\n",
    "        class_array.append({\n",
    "            \"value\": cat_name,\n",
    "            \"level\": level,\n",
    "            \"classId\": cat_id\n",
    "        })\n",
    "\n",
    "    # Possibly read the doc's existing classification system from the JSON\n",
    "    # (like in earlier code)\n",
    "    classifications = (\n",
    "        json_data\n",
    "        .get(\"processInformation\", {})\n",
    "        .get(\"dataSetInformation\", {})\n",
    "        .get(\"classificationInformation\", {})\n",
    "        .get(\"classification\", [])\n",
    "    )\n",
    "    for classification_item in classifications:\n",
    "        possible_name = classification_item.get(\"name\")\n",
    "        if possible_name:\n",
    "            classification_system_in_json = possible_name\n",
    "            break\n",
    "\n",
    "    # Insert new classification\n",
    "    new_classification_item = {\n",
    "        \"class\": class_array,\n",
    "        \"name\": \"OEKOBAU.DAT\"\n",
    "    }\n",
    "    process_info = json_data.setdefault(\"processInformation\", {})\n",
    "    data_set_info = process_info.setdefault(\"dataSetInformation\", {})\n",
    "    classification_info = data_set_info.setdefault(\"classificationInformation\", {})\n",
    "    existing_classifications = classification_info.get(\"classification\")\n",
    "    if not isinstance(existing_classifications, list):\n",
    "        existing_classifications = []\n",
    "    existing_classifications.append(new_classification_item)\n",
    "    classification_info[\"classification\"] = existing_classifications\n",
    "\n",
    "    # Store\n",
    "    modified_json_docs[uuid_val] = json_data\n",
    "\n",
    "# print(\"\\n---- Modified JSON Documents ----\")\n",
    "# print(json.dumps(modified_json_docs, indent=2, ensure_ascii=False))\n",
    "\n",
    "# =============================================================================\n",
    "# Part 4: Write out the final classification system name and the selected UUIDs\n",
    "#         to 'selected_epds_norge.json'\n",
    "# =============================================================================\n",
    "\n",
    "output_data = {\n",
    "    \"classificationSystem\": classification_system_in_json,\n",
    "    \"uuids\": list(modified_json_docs.keys())  # i.e., final_list's UUIDs\n",
    "}\n",
    "\n",
    "output_file = \"selected_epds_norge.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "    json.dump(output_data, out_f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nWrote {len(modified_json_docs)} EPD UUIDs to '{output_file}' using classification system: {classification_system_in_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(modified_json_docs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Query all \"Beton\" with \"OEKOBAU.DAT\" classification in the DB\n",
    "\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "db_path = \"../data/pipeline2/sql/epd_database.sqlite\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT d.uuid,\n",
    "       d.document\n",
    "FROM epd_documents d\n",
    "JOIN epd_metadata m \n",
    "   ON d.uuid = m.uuid\n",
    "JOIN json_each(d.document, '$.processInformation.dataSetInformation.classificationInformation.classification') classification\n",
    "JOIN json_each(classification.value, '$.class') class_item\n",
    "WHERE m.classification_system = 'OEKOBAU.DAT'\n",
    "  AND json_extract(class_item.value, '$.value') = 'Beton'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "docs_with_beton = {}\n",
    "for (epd_uuid, doc_text) in rows:\n",
    "    # parse the JSON for further processing in Python\n",
    "    try:\n",
    "        doc_data = json.loads(doc_text)\n",
    "        docs_with_beton[epd_uuid] = doc_data\n",
    "    except json.JSONDecodeError:\n",
    "        # skip invalid JSON\n",
    "        continue\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(f\"Found {len(docs_with_beton)} documents that truly have 'Beton' in their classification array.\")\n",
    "for u in docs_with_beton:\n",
    "    print(\" -\", u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Rename JSON keys (updated with new logic)\n",
    "\n",
    "import json\n",
    "\n",
    "def recursive_rename_uri(obj):\n",
    "    \"\"\"\n",
    "    Recursively rename any key 'uri' to 'refObjectUri' in the given object (dict/list).\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        new_obj = {}\n",
    "        for key, value in obj.items():\n",
    "            new_key = \"refObjectUri\" if key == \"uri\" else key\n",
    "            new_obj[new_key] = recursive_rename_uri(value)\n",
    "        return new_obj\n",
    "    elif isinstance(obj, list):\n",
    "        return [recursive_rename_uri(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def remove_raw_strings_in_anies(obj):\n",
    "    \"\"\"\n",
    "    Recursively traverse the object and remove any raw string elements from lists\n",
    "    that belong to a key named 'anies'.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        new_obj = {}\n",
    "        for key, value in obj.items():\n",
    "            if key == \"anies\" and isinstance(value, list):\n",
    "                # Filter out raw string elements from this list.\n",
    "                new_obj[key] = [\n",
    "                    remove_raw_strings_in_anies(item)\n",
    "                    for item in value\n",
    "                    if not isinstance(item, str)\n",
    "                ]\n",
    "            else:\n",
    "                new_obj[key] = remove_raw_strings_in_anies(value)\n",
    "        return new_obj\n",
    "    elif isinstance(obj, list):\n",
    "        return [remove_raw_strings_in_anies(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def transform_json(data):\n",
    "    \"\"\"\n",
    "    Transform an Environmental Product Declaration (EPD) JSON instance by renaming keys\n",
    "    and restructuring its contents to standardize the schema.\n",
    "    \n",
    "    Major changes in this version:\n",
    "      - Globally rename every \"uri\" to \"refObjectUri\"\n",
    "      - Globally remove raw string elements from any 'anies' lists\n",
    "      - Keep the rest of your original rename logic: processInformation, modellingAndValidation, \n",
    "        administrativeInformation, exchanges, LCIAResults, etc.\n",
    "      - Exclude the key \"relativeStandardDeviation95In\" from LCIAResults.LCIAResult and exchanges.exchange.\n",
    "\n",
    "    Returns: The updated dictionary (in-memory).\n",
    "    \"\"\"\n",
    "    # --- processInformation transformations ---\n",
    "    process_info = data.get(\"processInformation\", {})\n",
    "    data_set_info = process_info.get(\"dataSetInformation\", {})\n",
    "\n",
    "    # Rename \"dataSetInformation.name\" -> \"dataSetName\"\n",
    "    if \"name\" in data_set_info:\n",
    "        data_set_info[\"dataSetName\"] = data_set_info.pop(\"name\")\n",
    "\n",
    "    # Rename dataSetInformation.other -> otherDSI; remove 'componentsAndMaterialsAndSubstances' if found\n",
    "    if \"other\" in data_set_info:\n",
    "        data_set_info[\"otherDSI\"] = data_set_info.pop(\"other\")\n",
    "        # If any item in otherDSI.anies contains \"componentsAndMaterialsAndSubstances\", remove the entire \"anies\" key.\n",
    "        if \"anies\" in data_set_info[\"otherDSI\"]:\n",
    "            for item in data_set_info[\"otherDSI\"][\"anies\"]:\n",
    "                if (\n",
    "                    isinstance(item, dict)\n",
    "                    and \"componentsAndMaterialsAndSubstances\" in item\n",
    "                ):\n",
    "                    data_set_info[\"otherDSI\"].pop(\"anies\")\n",
    "                    break\n",
    "            # Rename dataSetInformation.other.anies.scenario -> objectScenario;\n",
    "            if \"scenario\" in data_set_info[\"otherDSI\"]:\n",
    "                data_set_info[\"objectScenario\"] = data_set_info[\"otherDSI\"].pop(\"scenario\")\n",
    "\n",
    "    # For classification entries, rename \"class\" -> \"classEntries\"\n",
    "    classification_info = data_set_info.get(\"classificationInformation\", {})\n",
    "    classifications = classification_info.get(\"classification\", [])\n",
    "    for cls_obj in classifications:\n",
    "        if \"class\" in cls_obj:\n",
    "            cls_obj[\"classEntries\"] = cls_obj.pop(\"class\")\n",
    "\n",
    "    # Rename \"time\" -> \"timeInformation\", then rename \"other\" -> \"otherTime\", and \"value\" -> \"timestampValue\"\n",
    "    if \"time\" in process_info:\n",
    "        process_info[\"timeInformation\"] = process_info.pop(\"time\")\n",
    "        time_info = process_info[\"timeInformation\"]\n",
    "        if \"other\" in time_info:\n",
    "            time_info[\"otherTime\"] = time_info.pop(\"other\")\n",
    "            for item in time_info[\"otherTime\"].get(\"anies\", []):\n",
    "                if isinstance(item, dict) and \"value\" in item:\n",
    "                    item[\"timestampValue\"] = item.pop(\"value\")\n",
    "\n",
    "    # --- modellingAndValidation transformations ---\n",
    "    mod_val = data.get(\"modellingAndValidation\", {})\n",
    "\n",
    "    # LCIMethodAndAllocation.other -> otherMAA\n",
    "    lci_method = mod_val.get(\"LCIMethodAndAllocation\", {})\n",
    "    if \"other\" in lci_method:\n",
    "        lci_method[\"otherMAA\"] = lci_method.pop(\"other\")\n",
    "\n",
    "    # dataSourcesTreatmentAndRepresentativeness.other -> otherDSTAR\n",
    "    dstar = mod_val.get(\"dataSourcesTreatmentAndRepresentativeness\", {})\n",
    "    if \"other\" in dstar:\n",
    "        dstar[\"otherDSTAR\"] = dstar.pop(\"other\")\n",
    "        if \"anies\" in dstar[\"otherDSTAR\"]:\n",
    "            dstar[\"otherDSTAR\"][\"aniesDSTAR\"] = dstar[\"otherDSTAR\"].pop(\"anies\")\n",
    "            for item in dstar[\"otherDSTAR\"][\"aniesDSTAR\"]:\n",
    "                if isinstance(item, dict) and \"value\" in item and isinstance(item[\"value\"], dict):\n",
    "                    # rename \"value\" -> \"valueDSTAR\", then rename subkeys\n",
    "                    item[\"valueDSTAR\"] = item.pop(\"value\")\n",
    "                    val = item[\"valueDSTAR\"]\n",
    "                    if \"shortDescription\" in val:\n",
    "                        val[\"shortDescriptionExtended\"] = val.pop(\"shortDescription\")\n",
    "                    if \"version\" in val:\n",
    "                        version = val.pop(\"version\")\n",
    "                        if \"version\" in version:\n",
    "                            version[\"versionInt\"] = version.pop(\"version\")\n",
    "                        val[\"versionDict\"] = version\n",
    "                    if \"uuid\" in val:\n",
    "                        uuid_obj = val.pop(\"uuid\")\n",
    "                        if \"uuid\" in uuid_obj:\n",
    "                            uuid_obj[\"uuidValue\"] = uuid_obj.pop(\"uuid\")\n",
    "                        val[\"uuidDict\"] = uuid_obj\n",
    "\n",
    "    # Rename \"validation\" -> \"validationInfo\"\n",
    "    if \"validation\" in mod_val:\n",
    "        mod_val[\"validationInfo\"] = mod_val.pop(\"validation\")\n",
    "\n",
    "    # modellingAndValidation.other -> otherMAV; if \"value\" is a dict, rename it to \"objectValue\"\n",
    "    if \"other\" in mod_val:\n",
    "        mod_val[\"otherMAV\"] = mod_val.pop(\"other\")\n",
    "        for item in mod_val[\"otherMAV\"].get(\"anies\", []):\n",
    "            if isinstance(item, dict) and \"value\" in item and isinstance(item[\"value\"], dict):\n",
    "                item[\"objectValue\"] = item.pop(\"value\")\n",
    "\n",
    "    # --- administrativeInformation transformations ---\n",
    "    admin_info = data.get(\"administrativeInformation\", {})\n",
    "\n",
    "    # publicationAndOwnership.other -> otherPAO; rename \"value\" -> \"objectValue\" if it's a dict\n",
    "    pub_own = admin_info.get(\"publicationAndOwnership\", {})\n",
    "    if \"other\" in pub_own:\n",
    "        pub_own[\"otherPAO\"] = pub_own.pop(\"other\")\n",
    "        for item in pub_own[\"otherPAO\"].get(\"anies\", []):\n",
    "            if isinstance(item, dict) and \"value\" in item and isinstance(item[\"value\"], dict):\n",
    "                item[\"objectValue\"] = item.pop(\"value\")\n",
    "\n",
    "    # --- exchanges transformations ---\n",
    "    exchanges = data.get(\"exchanges\", {}).get(\"exchange\", [])\n",
    "    for exchange in exchanges:\n",
    "        # flowProperties: rename name->nameFP, uuid->uuidFP\n",
    "        for fp in exchange.get(\"flowProperties\", []):\n",
    "            if \"name\" in fp:\n",
    "                fp[\"nameFP\"] = fp.pop(\"name\")\n",
    "            if \"uuid\" in fp:\n",
    "                fp[\"uuidFP\"] = fp.pop(\"uuid\")\n",
    "\n",
    "        # rename \"exchange direction\" -> \"exchangeDirection\"\n",
    "        if \"exchange direction\" in exchange:\n",
    "            exchange[\"exchangeDirection\"] = exchange.pop(\"exchange direction\")\n",
    "\n",
    "        # rename \"other\" -> \"otherEx\", if \"value\" is dict -> \"objectValue\"\n",
    "        if \"other\" in exchange:\n",
    "            exchange[\"otherEx\"] = exchange.pop(\"other\")\n",
    "            for item in exchange[\"otherEx\"].get(\"anies\", []):\n",
    "                if isinstance(item, dict) and \"value\" in item and isinstance(item[\"value\"], dict):\n",
    "                    item[\"objectValue\"] = item.pop(\"value\")\n",
    "\n",
    "        # rename \"classification\" -> \"classificationEx\" and inside, rename \"name\"->\"nameClass\"\n",
    "        if \"classification\" in exchange:\n",
    "            exchange[\"classificationEx\"] = exchange.pop(\"classification\")\n",
    "            if \"name\" in exchange[\"classificationEx\"]:\n",
    "                exchange[\"classificationEx\"][\"nameClass\"] = exchange[\"classificationEx\"].pop(\"name\")\n",
    "        \n",
    "        # exclude relativeStandardDeviation95In from exchanges.exchange ---\n",
    "        if \"relativeStandardDeviation95In\" in exchange:\n",
    "            exchange.pop(\"relativeStandardDeviation95In\")\n",
    "\n",
    "    # --- LCIAResults transformations ---\n",
    "    # rename \"LCIAResults\" -> \"lciaResults\" if present\n",
    "    if \"LCIAResults\" in data:\n",
    "        data[\"lciaResults\"] = data.pop(\"LCIAResults\")\n",
    "\n",
    "    lcia_results = data.get(\"lciaResults\", {}).get(\"LCIAResult\", [])\n",
    "    for result in lcia_results:\n",
    "        # rename \"other\" -> \"otherLCIA\", if \"value\" is dict -> \"objectValue\"\n",
    "        if \"other\" in result:\n",
    "            result[\"otherLCIA\"] = result.pop(\"other\")\n",
    "            for item in result[\"otherLCIA\"].get(\"anies\", []):\n",
    "                if isinstance(item, dict) and \"value\" in item and isinstance(item[\"value\"], dict):\n",
    "                    item[\"objectValue\"] = item.pop(\"value\")\n",
    "        \n",
    "        # exclude relativeStandardDeviation95In from each LCIA result ---\n",
    "        if \"relativeStandardDeviation95In\" in result:\n",
    "            result.pop(\"relativeStandardDeviation95In\")\n",
    "\n",
    "    # --- Removal ---\n",
    "    # Remove top-level \"otherAttributes\" key if it exists\n",
    "    if \"otherAttributes\" in data:\n",
    "        data.pop(\"otherAttributes\")\n",
    "    \n",
    "    # if \"modellingAndValidation\" in data:\n",
    "    #     data.pop(\"modellingAndValidation\")\n",
    "    \n",
    "    # if \"administrativeInformation\" in data:\n",
    "    #     data.pop(\"administrativeInformation\")\n",
    "    \n",
    "    # if \"exchanges\" in data:\n",
    "    #     data.pop(\"exchanges\")\n",
    "    \n",
    "    # if \"lciaResults\" in data:\n",
    "    #     data.pop(\"lciaResults\")\n",
    "    \n",
    "    if \"locations\" in data:\n",
    "        data.pop(\"locations\")\n",
    "\n",
    "\n",
    "    # --- Global step: remove raw string elements from any 'anies' list\n",
    "    data = remove_raw_strings_in_anies(data)\n",
    "\n",
    "    # --- Global step: rename \"uri\" -> \"refObjectUri\"\n",
    "    data = recursive_rename_uri(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Example pipeline usage (in memory, no disk I/O):\n",
    "# -----------------------------------------------------------\n",
    "for uuid_val, original_doc in modified_json_docs.items():\n",
    "    final_doc = transform_json(original_doc)\n",
    "\n",
    "    # If you want to store the transformed version back in the dictionary:\n",
    "    modified_json_docs[uuid_val] = final_doc\n",
    "\n",
    "    # Just print for verification\n",
    "    # print(f\"\\nTransformed JSON for UUID = {uuid_val}:\")\n",
    "    # print(json.dumps(final_doc, indent=2))\n",
    "    # print(\"--------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Add ids\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "# --------------------------- Utility functions ---------------------------\n",
    "\n",
    "def load_yaml_schema(file_path):\n",
    "    \"\"\"Load the LinkML YAML schema from disk.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def reorder_dict_keys(d):\n",
    "    \"\"\"\n",
    "    Reorders a dictionary so that if 'id' exists, it appears as the first key.\n",
    "    (For human readability.)\n",
    "    \"\"\"\n",
    "    if \"id\" in d:\n",
    "        id_value = d.pop(\"id\")\n",
    "        new_d = {\"id\": id_value}\n",
    "        new_d.update(d)\n",
    "        d.clear()\n",
    "        d.update(new_d)\n",
    "\n",
    "def clean_epd_name(name):\n",
    "    \"\"\"\n",
    "    Cleans the EPD name by replacing non-alphanumeric characters with underscores,\n",
    "    collapsing multiple underscores, and stripping leading/trailing underscores.\n",
    "    \"\"\"\n",
    "    cleaned = re.sub(r\"[^A-Za-z0-9]\", \"_\", name)\n",
    "    return re.sub(r\"_+\", \"_\", cleaned).strip(\"_\")\n",
    "\n",
    "# ------------------ New ID Creation Logic -------------------\n",
    "\n",
    "def generate_id_from_path(acc_path, prefix=\"ilcd\"):\n",
    "    \"\"\"\n",
    "    Given an accumulated path, returns the full ID as: prefix:acc_path\n",
    "    \"\"\"\n",
    "    return f\"{prefix}:{acc_path}\"\n",
    "\n",
    "def get_suffix(item, index):\n",
    "    \"\"\"\n",
    "    If the list element (item) is a dict with a 'module' field,\n",
    "    return 'module' + its value (dashes removed);\n",
    "    otherwise, return the 1-based index as a string.\n",
    "    \"\"\"\n",
    "    if isinstance(item, dict) and \"module\" in item:\n",
    "        return f\"module{item['module'].replace('-', '')}\"\n",
    "    return str(index + 1)\n",
    "\n",
    "def assign_ids_by_path(obj, epd_uuid, acc_path, parent_is_list, prefix=\"ilcd\"):\n",
    "    \"\"\"\n",
    "    Recursively assigns IDs based on the accumulated path.\n",
    "\n",
    "    Parameters:\n",
    "      obj          : current object (dict, list, or primitive)\n",
    "      epd_uuid     : the top-level EPD UUID (dashes removed), used as a base\n",
    "      acc_path     : the accumulated path string\n",
    "      parent_is_list : bool indicating whether the parent container was a list\n",
    "      prefix       : the string prefix to use, e.g. \"ilcd\"\n",
    "    \"\"\"\n",
    "    # If this is a dict and has no 'id', generate one\n",
    "    if isinstance(obj, dict) and \"id\" not in obj:\n",
    "        # ID => prefix:epd_uuid_accPath\n",
    "        obj[\"id\"] = generate_id_from_path(f\"{epd_uuid}_{acc_path}\", prefix)\n",
    "        reorder_dict_keys(obj)\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        # Traverse each key\n",
    "        for key, value in obj.items():\n",
    "            if key == \"id\":\n",
    "                continue\n",
    "            if isinstance(value, dict):\n",
    "                # If parent is a list, extend path with \"_key\"\n",
    "                new_acc = f\"{acc_path}_{key}\" if parent_is_list else key\n",
    "                assign_ids_by_path(value, epd_uuid, new_acc, parent_is_list=False, prefix=prefix)\n",
    "            elif isinstance(value, list):\n",
    "                # For a list, extend path with \"_key\"\n",
    "                new_acc = f\"{acc_path}_{key}\"\n",
    "                for i, item in enumerate(value):\n",
    "                    suffix = get_suffix(item, i)\n",
    "                    # element path => new_acc + \"_\" + suffix\n",
    "                    element_acc = f\"{new_acc}_{suffix}\"\n",
    "                    assign_ids_by_path(item, epd_uuid, element_acc, parent_is_list=True, prefix=prefix)\n",
    "            # If it's a primitive, do nothing\n",
    "    elif isinstance(obj, list):\n",
    "        # If this is a list, iterate elements\n",
    "        for i, item in enumerate(obj):\n",
    "            suffix = get_suffix(item, i)\n",
    "            new_acc = f\"{acc_path}_{suffix}\"\n",
    "            assign_ids_by_path(item, epd_uuid, new_acc, parent_is_list=True, prefix=prefix)\n",
    "\n",
    "\n",
    "# --------------------- MAIN LOGIC: In-memory Example ---------------------\n",
    "\n",
    "SCHEMA_PATH = \"../linkml/data/yaml/linkml_ILCDmergedSchemas_schema.yaml\"\n",
    "schema = load_yaml_schema(SCHEMA_PATH)\n",
    "\n",
    "# Fallback if your schema has a default prefix; else use \"ilcd\"\n",
    "default_prefix = schema.get(\"default_prefix\", \"ilcd\")\n",
    "\n",
    "# \n",
    "# We assume `modified_json_docs` is already defined in memory:\n",
    "# e.g. modified_json_docs = {\n",
    "#     \"uuid1\": {...final JSON doc...},\n",
    "#     \"uuid2\": {...final JSON doc...},\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "for uuid_val, doc in modified_json_docs.items():\n",
    "    # 1) Get the \"real\" UUID from the doc (with dashes)\n",
    "    try:\n",
    "        raw_uuid = doc[\"processInformation\"][\"dataSetInformation\"][\"UUID\"]\n",
    "    except KeyError as e:\n",
    "        raise KeyError(f\"Missing processInformation.dataSetInformation.UUID in doc {uuid_val}\") from e\n",
    "    \n",
    "    # 2) Remove dashes to build a base\n",
    "    epd_uuid = raw_uuid.replace(\"-\", \"\")\n",
    "\n",
    "    # 3) Assign top-level doc ID (prefix:epd_uuid)\n",
    "    doc[\"id\"] = f\"{default_prefix.lower()}:{epd_uuid}\"\n",
    "\n",
    "    # 4) For each top-level key (besides 'id'/'version'), set a sub-ID and recursively assign deeper IDs\n",
    "    for top_key, top_obj in doc.items():\n",
    "        if top_key in [\"id\", \"version\"]:\n",
    "            continue\n",
    "        if isinstance(top_obj, dict):\n",
    "            # e.g. \"processInformation\" => prefix:epd_uuid_processInformation\n",
    "            top_obj[\"id\"] = f\"{default_prefix.lower()}:{epd_uuid}_{top_key}\"\n",
    "            reorder_dict_keys(top_obj)\n",
    "            assign_ids_by_path(\n",
    "                top_obj,\n",
    "                epd_uuid=epd_uuid,\n",
    "                acc_path=top_key,\n",
    "                parent_is_list=False,\n",
    "                prefix=default_prefix.lower()\n",
    "            )\n",
    "        elif isinstance(top_obj, list):\n",
    "            # If it's a list at top level, handle each item\n",
    "            for i, item in enumerate(top_obj):\n",
    "                suffix = get_suffix(item, i)\n",
    "                # top path => top_key + \"_\" + suffix\n",
    "                top_path = f\"{top_key}_{suffix}\"\n",
    "                assign_ids_by_path(\n",
    "                    item,\n",
    "                    epd_uuid=epd_uuid,\n",
    "                    acc_path=top_path,\n",
    "                    parent_is_list=True,\n",
    "                    prefix=default_prefix.lower()\n",
    "                )\n",
    "\n",
    "    # 5) Print final JSON to confirm\n",
    "    # print(f\"\\n=== JSON with newly assigned IDs for doc (pipeline UUID): {uuid_val} ===\")\n",
    "    # print(json.dumps(doc, indent=2))\n",
    "    # print(\"-----------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALIDATION] Document 63a79af1-1ab0-4677-45a8-08dc6fc9d4ca is valid according to the schema.\n",
      "[VALIDATION] Document 46d205a5-8dcd-4d37-c910-08dc77c8811e is valid according to the schema.\n",
      "[VALIDATION] Document e79a42aa-51cb-4d28-2308-08dc685f6882 is valid according to the schema.\n",
      "[VALIDATION] Document 32426332-ecca-4bd9-1d3f-08dca1505d10 is valid according to the schema.\n",
      "[VALIDATION] Document 4f634da7-bbe1-421d-2310-08dc685f6882 is valid according to the schema.\n",
      "[VALIDATION] Document 14358dd4-831f-40ab-d98d-08dd1f3f8b97 is valid according to the schema.\n",
      "[VALIDATION] Document 827fdb94-0b81-4936-231c-08dc685f6882 is valid according to the schema.\n",
      "[VALIDATION] Document bbc48265-7458-427c-2341-08dc685f6882 is valid according to the schema.\n",
      "[VALIDATION] Document 44afd3a1-ee17-4870-991c-08dc65c063c1 is valid according to the schema.\n",
      "[VALIDATION] Document c4b20fb9-0065-4551-5864-08dc3866f1ae is valid according to the schema.\n",
      "[VALIDATION] Document 4d614cb9-2751-47de-9913-8090006ad5ab is valid according to the schema.\n",
      "[VALIDATION] Document 5111b232-8956-494f-98de-f3ae24de2c40 is valid according to the schema.\n",
      "[VALIDATION] Document 69cb90b8-560a-4bce-b4f8-7fa5220eccdc is valid according to the schema.\n",
      "[VALIDATION] Document 0a7ca5e9-a056-48a0-8cd6-db0b10d2005c is valid according to the schema.\n",
      "[VALIDATION] Document 75db1c10-6002-4fa2-9cc9-aae4fb7e8345 is valid according to the schema.\n",
      "[VALIDATION] Document cb928d86-f997-495b-81ad-9fa5f37ccc86 is valid according to the schema.\n",
      "[VALIDATION] Document bb6970aa-9d71-430f-8b81-930b9fe467c8 is valid according to the schema.\n",
      "[VALIDATION] Document dbbd0cb2-5244-42b6-b5ab-8a00ca54aec7 is valid according to the schema.\n",
      "[VALIDATION] Document 335241f9-db84-486c-9a19-cd5ebb791903 is valid according to the schema.\n",
      "[VALIDATION] Document 7503bc8c-e613-4c10-9014-c08d9d0cc55d is valid according to the schema.\n",
      "[VALIDATION] Document 4bcaab52-0a78-48d6-9a28-2f748744c2f6 is valid according to the schema.\n",
      "[VALIDATION] Document a3d44cbb-cfee-44b5-9657-3693466393cf is valid according to the schema.\n",
      "[VALIDATION] Document fbcd6661-dfc9-4312-9758-b932ed9e9c83 is valid according to the schema.\n",
      "[VALIDATION] Document 64dd7b28-d60c-4e4d-87ea-d45b450299d8 is valid according to the schema.\n",
      "[VALIDATION] Document 07040b57-15be-493c-b6d8-d044f5bd1f39 is valid according to the schema.\n",
      "[VALIDATION] Document cbb208a1-6240-4c8c-a812-489025c85c05 is valid according to the schema.\n",
      "[VALIDATION] Document e389c1eb-50e9-4c5e-ba58-50be75a60bc2 is valid according to the schema.\n",
      "[VALIDATION] Document fad86362-345a-40e4-a951-c0f1786c6b62 is valid according to the schema.\n",
      "[VALIDATION] Document 0643c049-1046-4d51-8ddc-760f32642a72 is valid according to the schema.\n",
      "[VALIDATION] Document 8170aa37-5b14-4cbb-9c02-80f79dbb5623 is valid according to the schema.\n",
      "\n",
      "Successfully wrote 30 instances to the Turtle file:\n",
      "  ../linkml/data/rdf/epd_rdf_instance_datastore.ttl\n"
     ]
    }
   ],
   "source": [
    "# Convert to RDF\n",
    "\n",
    "import rdflib\n",
    "from linkml.validator import Validator\n",
    "from linkml_runtime.loaders import YAMLLoader\n",
    "from linkml_runtime.dumpers import RDFLibDumper\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "\n",
    "# Import your generated Python dataclass for the schema\n",
    "# e.g., from data.py.linkml_processDataSet_schema import ProcessDataSet\n",
    "from data.py.linkml_processDataSet_schema import ProcessDataSet\n",
    "\n",
    "\n",
    "def generate_turtle_from_docs(\n",
    "    docs_dict,\n",
    "    schema_path,\n",
    "    turtle_output_path,\n",
    "    validate: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a dictionary of JSON documents (each conforming to 'ProcessDataSet'),\n",
    "    generate a single TTL file that contains all instances. If loading fails\n",
    "    for any doc, store it in a separate dictionary `failed_docs` for debugging.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    docs_dict : dict\n",
    "        A dict of {UUID: JSON-Dict} storing your EPD JSON objects in memory.\n",
    "    schema_path : str\n",
    "        Path to the LinkML YAML schema (e.g. '../linkml/data/yaml/linkml_processDataSet_schema.yaml').\n",
    "    turtle_output_path : str\n",
    "        Where to write the combined Turtle RDF graph (overwrites each run).\n",
    "    validate : bool\n",
    "        If True, runs the LinkML Validator on each instance before RDF conversion.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    failed_docs : dict\n",
    "        A dictionary of {UUID: JSON-Dict} for any documents that failed to load.\n",
    "    \"\"\"\n",
    "    # 1) Create an empty rdflib graph to combine all instance graphs\n",
    "    combined_graph = rdflib.Graph()\n",
    "\n",
    "    # 2) Load the schema into a SchemaView for RDF generation\n",
    "    sv = SchemaView(schema_path)\n",
    "\n",
    "    # (Optional) set up a validator if needed\n",
    "    validator = None\n",
    "    if validate:\n",
    "        validator = Validator(schema_path, strict=False)\n",
    "\n",
    "    # A container to track documents that fail to load\n",
    "    failed_docs = {}\n",
    "\n",
    "    # 3) For each doc, wrap in a top-level \"processDataSet\" key,\n",
    "    #    load as an object, optionally validate, and convert to RDF.\n",
    "    dumper = RDFLibDumper()\n",
    "\n",
    "    success_count = 0\n",
    "\n",
    "    for uuid_val, json_doc in docs_dict.items():\n",
    "        yaml_wrapper = {\"processDataSet\": json_doc}\n",
    "\n",
    "        # (A) Validate the doc if requested\n",
    "        if validator:\n",
    "            report = validator.validate(yaml_wrapper, \"ProcessDataSet\")\n",
    "            if report.results:\n",
    "                print(f\"[VALIDATION] Errors for UUID={uuid_val}:\")\n",
    "                for result in report.results:\n",
    "                    print(\"  -\", result.message)\n",
    "                # We can decide to skip, but let's let the user decide:\n",
    "                # continue\n",
    "            else:\n",
    "                print(f\"[VALIDATION] Document {uuid_val} is valid according to the schema.\")\n",
    "\n",
    "        # (B) Attempt to load as a ProcessDataSet\n",
    "        try:\n",
    "            instance_obj = YAMLLoader().load(yaml_wrapper[\"processDataSet\"], target_class=ProcessDataSet)\n",
    "        except (ValueError, TypeError) as e:\n",
    "            print(f\"[ERROR] Failed to load doc {uuid_val} as ProcessDataSet. Reason:\\n  {e}\")\n",
    "            failed_docs[uuid_val] = json_doc\n",
    "            continue  # Skip adding to the graph\n",
    "\n",
    "        # (C) Convert to RDF (rdflib.Graph) and accumulate\n",
    "        instance_graph = dumper.as_rdf_graph(instance_obj, schemaview=sv)\n",
    "        combined_graph += instance_graph\n",
    "        success_count += 1\n",
    "\n",
    "    # 4) Write the combined graph to Turtle\n",
    "    combined_graph.serialize(destination=turtle_output_path, format=\"turtle\")\n",
    "    print(f\"\\nSuccessfully wrote {success_count} instances to the Turtle file:\\n  {turtle_output_path}\")\n",
    "    if failed_docs:\n",
    "        print(f\"{len(failed_docs)} documents failed to load and were skipped.\")\n",
    "\n",
    "    # Return the dictionary of failed docs for further handling or debugging\n",
    "    return failed_docs\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# USAGE EXAMPLE (in the same or next cell)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Suppose you have a dict `modified_json_docs` from prior steps:\n",
    "# modified_json_docs = {\n",
    "#     \"uuid1\": {... doc ...},\n",
    "#     \"uuid2\": {... doc ...},\n",
    "#     # ...\n",
    "# }\n",
    "\n",
    "# Then do:\n",
    "schema_file = \"../linkml/data/yaml/linkml_processDataSet_schema.yaml\"\n",
    "ttl_output = \"../linkml/data/rdf/epd_rdf_instance_datastore.ttl\"\n",
    "\n",
    "failed = generate_turtle_from_docs(\n",
    "    docs_dict=modified_json_docs,\n",
    "    schema_path=schema_file,\n",
    "    turtle_output_path=ttl_output,\n",
    "    validate=True\n",
    ")\n",
    "\n",
    "if failed:\n",
    "    print(\"\\nFailed doc details:\")\n",
    "    for bad_uuid, bad_doc in failed.items():\n",
    "        print(\" -\", bad_uuid, \"(Doc not loaded successfully)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
