{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Change category (keeping original classifications if present)\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. PARSE THE XML TO BUILD A CATEGORY LOOKUP STRUCTURE\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_category_dict(xml_element):\n",
    "    category_id = xml_element.get('id')\n",
    "    category_name = xml_element.get('name')\n",
    "    \n",
    "    children_dict = {}\n",
    "    for child_elem in xml_element.findall('{*}category'):\n",
    "        child_data = build_category_dict(child_elem)\n",
    "        children_dict[child_data['name']] = child_data\n",
    "\n",
    "    return {\n",
    "        'id': category_id,\n",
    "        'name': category_name,\n",
    "        'children': children_dict\n",
    "    }\n",
    "\n",
    "def parse_category_xml(xml_path):\n",
    "    tree = ET.parse(xml_path, parser=ET.XMLParser(encoding='utf-8'))\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    category_system = root.find('.//{*}categories[@dataType=\"Process\"]')\n",
    "    top_level_dict = {}\n",
    "    if category_system is not None:\n",
    "        for cat_elem in category_system.findall('{*}category'):\n",
    "            cat_data = build_category_dict(cat_elem)\n",
    "            top_level_dict[cat_data['name']] = cat_data\n",
    "    else:\n",
    "        print(\"Warning: Could not find <categories dataType='Process'> in XML.\")\n",
    "    \n",
    "    return top_level_dict\n",
    "\n",
    "def get_category_path_info(category_hierarchy, categories_dict):\n",
    "    path_info = []\n",
    "    if not category_hierarchy:\n",
    "        return path_info\n",
    "\n",
    "    current_dict = categories_dict\n",
    "    for i, cat_name in enumerate(category_hierarchy):\n",
    "        if cat_name not in current_dict:\n",
    "            raise ValueError(f\"Category '{cat_name}' not found at level {i}.\")\n",
    "        this_cat = current_dict[cat_name]\n",
    "        path_info.append((this_cat['name'], this_cat['id']))\n",
    "        current_dict = this_cat['children']\n",
    "    return path_info\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1.1. CHANGE CATEGORY FOR EPDs FROM CSV, preserving existing classification entries\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "xml_path = \"../data/pipeline2/xml/OEKOBAU.DAT_Categories_EN_API.xml\"\n",
    "category_dict = parse_category_xml(xml_path)\n",
    "\n",
    "db_path = \"../data/pipeline2/sql/epd_database.sqlite\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "csv_path = \"../data/pipeline2/sql/regex_classified/filtered_epd_data02_classified_concrete03.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "num_epd_files = 2000  # adjust as needed\n",
    "df_limited = df.head(num_epd_files)\n",
    "\n",
    "# Create a dictionary to hold JSON objects in memory\n",
    "modified_json_docs = {}\n",
    "\n",
    "TARGET_CLASSIFICATION = \"Mineral building products > Mortar and Concrete > Ready mixed concrete\"\n",
    "\n",
    "for index, row in df_limited.iterrows():\n",
    "    uuid_val = row[\"UUID\"]\n",
    "    classification_str = row[\"RegEx Classification\"]\n",
    "\n",
    "    # Only process rows with the target classification\n",
    "    if classification_str == TARGET_CLASSIFICATION:\n",
    "        # Fetch JSON from epd_documents\n",
    "        cursor.execute(\"SELECT document FROM epd_documents WHERE uuid = ?\", (uuid_val,))\n",
    "        result = cursor.fetchone()\n",
    "        if not result:\n",
    "            print(f\"No document found for UUID: {uuid_val}\")\n",
    "            continue\n",
    "\n",
    "        json_text = result[0]\n",
    "        try:\n",
    "            json_data = json.loads(json_text)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to decode JSON for UUID: {uuid_val}\")\n",
    "            continue\n",
    "        \n",
    "        # Build the new classification array from the CSV \"RegEx Classification\"\n",
    "        category_names = [x.strip() for x in classification_str.split(\">\")]\n",
    "        try:\n",
    "            path_info = get_category_path_info(category_names, category_dict)\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: Could not resolve categories for UUID: {uuid_val}. {e}\")\n",
    "            continue\n",
    "\n",
    "        class_array = []\n",
    "        for level, (cat_name, cat_id) in enumerate(path_info):\n",
    "            class_array.append({\n",
    "                \"value\": cat_name,\n",
    "                \"level\": level,\n",
    "                \"classId\": cat_id\n",
    "            })\n",
    "\n",
    "        # Prepare new classification item; set its \"name\" to a chosen value (here, \"OEKOBAU.DAT\")\n",
    "        new_classification_item = {\n",
    "            \"class\": class_array,\n",
    "            \"name\": \"OEKOBAU.DAT\"\n",
    "        }\n",
    "        \n",
    "        # Inject the new classification item while keeping original ones\n",
    "        process_info = json_data.setdefault(\"processInformation\", {})\n",
    "        data_set_info = process_info.setdefault(\"dataSetInformation\", {})\n",
    "        classification_info = data_set_info.setdefault(\"classificationInformation\", {})\n",
    "\n",
    "        # Retrieve any existing classifications; if missing, use an empty list.\n",
    "        existing_classifications = classification_info.get(\"classification\")\n",
    "        if not isinstance(existing_classifications, list):\n",
    "            existing_classifications = []\n",
    "        # Append the new classification\n",
    "        existing_classifications.append(new_classification_item)\n",
    "        classification_info[\"classification\"] = existing_classifications\n",
    "        \n",
    "        # Store the updated JSON in our in-memory dict\n",
    "        modified_json_docs[uuid_val] = json_data\n",
    "\n",
    "        print(f\"Appended new category for UUID {uuid_val}: {classification_str}\")\n",
    "        print(\"Full classification now:\")\n",
    "        print(json.dumps(classification_info[\"classification\"], indent=2))\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add from OEKOBAU.DAT\n",
    "classification_sys = \"OEKOBAU.DAT\"\n",
    "category_val = \"Beton\"\n",
    "limit_to_fetch = 500  # e.g., fetch up to 5 documents\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "def fetch_well_defined_epds(\n",
    "    conn,\n",
    "    classification_system=\"OEKOBAU.DAT\",\n",
    "    category_value=\"Beton\",\n",
    "    max_items=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch up to 'max_items' EPDs from epd_documents + epd_metadata \n",
    "    where m.classification_system = classification_system,\n",
    "    and the JSON classification array truly has {\"value\": category_value}.\n",
    "\n",
    "    Returns a list of (uuid, doc_data) tuples.\n",
    "    If none match, returns [] (empty list).\n",
    "    \"\"\"\n",
    "\n",
    "    # We'll do a broad SELECT for this classification_system,\n",
    "    # then parse each doc in Python to check for category_value.\n",
    "    sql = f\"\"\"\n",
    "    SELECT d.uuid, d.document\n",
    "    FROM epd_documents d\n",
    "    JOIN epd_metadata m \n",
    "        ON d.uuid = m.uuid\n",
    "    WHERE m.classification_system = ?\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql, (classification_system,))\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    results = []\n",
    "    for (doc_uuid, doc_text) in rows:\n",
    "        # Skip empty or invalid JSON\n",
    "        if not doc_text:\n",
    "            continue\n",
    "        try:\n",
    "            doc_data = json.loads(doc_text)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        # Safely retrieve classification array\n",
    "        classifications = (\n",
    "            doc_data.get(\"processInformation\", {})\n",
    "                    .get(\"dataSetInformation\", {})\n",
    "                    .get(\"classificationInformation\", {})\n",
    "                    .get(\"classification\", [])\n",
    "        )\n",
    "\n",
    "        # Check the \"class\" array for the desired category_value\n",
    "        found_match = False\n",
    "        for classification_item in classifications:\n",
    "            # if you do NOT need to verify classification_item[\"name\"] == classification_system \n",
    "            # (since epd_metadata enforces it), skip that check.\n",
    "            for cls_obj in classification_item.get(\"class\", []):\n",
    "                if cls_obj.get(\"value\") == category_value:\n",
    "                    found_match = True\n",
    "                    break\n",
    "            if found_match:\n",
    "                break\n",
    "        \n",
    "        if found_match:\n",
    "            results.append((doc_uuid, doc_data))\n",
    "            # If we only want up to 'max_items', break early:\n",
    "            if len(results) >= max_items:\n",
    "                break\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Step #1.2: Add well-defined EPD(s) to the pipeline\n",
    "found_epds = fetch_well_defined_epds(\n",
    "    conn,\n",
    "    classification_system=classification_sys,\n",
    "    category_value=category_val,\n",
    "    max_items=limit_to_fetch\n",
    ")\n",
    "\n",
    "if not found_epds:\n",
    "    print(f\"No EPD found for classification_system='{classification_sys}' and category_value='{category_val}'.\")\n",
    "else:\n",
    "    print(f\"Found {len(found_epds)} EPD(s) with '{category_val}' in classification system '{classification_sys}':\")\n",
    "    for (u, doc) in found_epds:\n",
    "        print(\"  -\", u)\n",
    "\n",
    "    # Now you can choose which one(s) you want to add to modified_json_docs.\n",
    "    # Example: pick the first\n",
    "    # chosen_uuid, chosen_doc = found_epds[1]\n",
    "    # modified_json_docs[chosen_uuid] = chosen_doc\n",
    "    # print(f\"\\nAdded well-defined EPD with UUID={chosen_uuid} to modified_json_docs.\")\n",
    "\n",
    "    for found_epd in found_epds:\n",
    "        chosen_uuid, chosen_doc = found_epd\n",
    "        modified_json_docs[chosen_uuid] = chosen_doc\n",
    "        print(f\"\\nAdded well-defined EPD with UUID={chosen_uuid} to modified_json_docs.\")\n",
    "\n",
    "# Optionally close conn if you're done\n",
    "conn.close()\n",
    "\n",
    "# Show final dictionary\n",
    "print(\"\\nFinal EPDs in `modified_json_docs` after Steps #1.1 and #1.2:\")\n",
    "for epd_uuid in modified_json_docs:\n",
    "    print(\" -\", epd_uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add from oekobau.dat\n",
    "\n",
    "classification_sys = \"oekobau.dat\"\n",
    "category_val = \"Beton\"\n",
    "limit_to_fetch = 500  # e.g., fetch up to 5 documents\n",
    "\n",
    "import json\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "def fetch_well_defined_epds(\n",
    "    conn,\n",
    "    classification_system=\"OEKOBAU.DAT\",\n",
    "    category_value=\"Beton\",\n",
    "    max_items=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch up to 'max_items' EPDs from epd_documents + epd_metadata \n",
    "    where m.classification_system = classification_system,\n",
    "    and the JSON classification array truly has {\"value\": category_value}.\n",
    "\n",
    "    Returns a list of (uuid, doc_data) tuples.\n",
    "    If none match, returns [] (empty list).\n",
    "    \"\"\"\n",
    "\n",
    "    # We'll do a broad SELECT for this classification_system,\n",
    "    # then parse each doc in Python to check for category_value.\n",
    "    sql = f\"\"\"\n",
    "    SELECT d.uuid, d.document\n",
    "    FROM epd_documents d\n",
    "    JOIN epd_metadata m \n",
    "        ON d.uuid = m.uuid\n",
    "    WHERE m.classification_system = ?\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql, (classification_system,))\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    results = []\n",
    "    for (doc_uuid, doc_text) in rows:\n",
    "        # Skip empty or invalid JSON\n",
    "        if not doc_text:\n",
    "            continue\n",
    "        try:\n",
    "            doc_data = json.loads(doc_text)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        # Safely retrieve classification array\n",
    "        classifications = (\n",
    "            doc_data.get(\"processInformation\", {})\n",
    "                    .get(\"dataSetInformation\", {})\n",
    "                    .get(\"classificationInformation\", {})\n",
    "                    .get(\"classification\", [])\n",
    "        )\n",
    "\n",
    "        # Check the \"class\" array for the desired category_value\n",
    "        found_match = False\n",
    "        for classification_item in classifications:\n",
    "            # if you do NOT need to verify classification_item[\"name\"] == classification_system \n",
    "            # (since epd_metadata enforces it), skip that check.\n",
    "            for cls_obj in classification_item.get(\"class\", []):\n",
    "                if cls_obj.get(\"value\") == category_value:\n",
    "                    found_match = True\n",
    "                    break\n",
    "            if found_match:\n",
    "                break\n",
    "        \n",
    "        if found_match:\n",
    "            results.append((doc_uuid, doc_data))\n",
    "            # If we only want up to 'max_items', break early:\n",
    "            if len(results) >= max_items:\n",
    "                break\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Step #1.2: Add well-defined EPD(s) to the pipeline\n",
    "found_epds = fetch_well_defined_epds(\n",
    "    conn,\n",
    "    classification_system=classification_sys,\n",
    "    category_value=category_val,\n",
    "    max_items=limit_to_fetch\n",
    ")\n",
    "\n",
    "if not found_epds:\n",
    "    print(f\"No EPD found for classification_system='{classification_sys}' and category_value='{category_val}'.\")\n",
    "else:\n",
    "    print(f\"Found {len(found_epds)} EPD(s) with '{category_val}' in classification system '{classification_sys}':\")\n",
    "    for (u, doc) in found_epds:\n",
    "        print(\"  -\", u)\n",
    "\n",
    "    # Now you can choose which one(s) you want to add to modified_json_docs.\n",
    "    # Example: pick the first\n",
    "    # chosen_uuid, chosen_doc = found_epds[1]\n",
    "    # modified_json_docs[chosen_uuid] = chosen_doc\n",
    "    # print(f\"\\nAdded well-defined EPD with UUID={chosen_uuid} to modified_json_docs.\")\n",
    "\n",
    "    for found_epd in found_epds:\n",
    "        chosen_uuid, chosen_doc = found_epd\n",
    "        modified_json_docs[chosen_uuid] = chosen_doc\n",
    "        print(f\"\\nAdded well-defined EPD with UUID={chosen_uuid} to modified_json_docs.\")\n",
    "\n",
    "# Optionally close conn if you're done\n",
    "# conn.close()\n",
    "\n",
    "# Show final dictionary\n",
    "print(\"\\nFinal EPDs in `modified_json_docs` after Steps #1.1 and #1.2:\")\n",
    "for epd_uuid in modified_json_docs:\n",
    "    print(\" -\", epd_uuid)\n",
    "\n",
    "# Optionally close conn if you're done\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add from EPDNorge\n",
    "import json\n",
    "import jsonlines\n",
    "import sqlite3\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# =============================================================================\n",
    "# Part 1: Build the results list from JSON files with added ids\n",
    "# =============================================================================\n",
    "\n",
    "# File paths (adjust if needed)\n",
    "context_path = \"../data/pipeline2/json/EPDNorge_context.json\"\n",
    "batch_output_path = \"../data/pipeline2/json/openai/EPDNorge_concrete_batch_output.jsonl\"\n",
    "\n",
    "# 1. Read context data\n",
    "with open(context_path, 'r', encoding='utf-8') as f:\n",
    "    context_data = json.load(f)\n",
    "\n",
    "# 2. Prepare a list to hold the resulting records with added ids\n",
    "results = []\n",
    "with jsonlines.open(batch_output_path) as reader:\n",
    "    for idx, batch_record in enumerate(reader):\n",
    "        # Extract the \"content\" field, which is a JSON string\n",
    "        content_str = batch_record[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        # Parse the content string to a JSON object\n",
    "        content_json = json.loads(content_str)\n",
    "        best_category = content_json[\"best_category\"]\n",
    "\n",
    "        # Create the new entry with an added \"id\" key\n",
    "        new_entry = {\n",
    "            \"id\": idx,\n",
    "            \"Product\": context_data[idx][\"Product\"],\n",
    "            \"UUID\": context_data[idx][\"UUID\"],\n",
    "            \"best_category\": best_category\n",
    "        }\n",
    "        results.append(new_entry)\n",
    "\n",
    "print(\"---- Results JSON ----\")\n",
    "print(json.dumps(results, indent=2, ensure_ascii=False))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Part 2: Extract unique categories and filter by the target classification\n",
    "# =============================================================================\n",
    "\n",
    "# Extract unique categories\n",
    "unique_categories = {entry[\"best_category\"] for entry in results}\n",
    "print(\"\\n---- Unique Categories ----\")\n",
    "print(json.dumps(list(unique_categories), indent=2, ensure_ascii=False))\n",
    "\n",
    "# Filter the results by the specified category\n",
    "TARGET_CLASSIFICATION = \"Mineral building products > Mortar and Concrete > Ready mixed concrete\"\n",
    "filtered_results = [\n",
    "    entry for entry in results \n",
    "    if entry[\"best_category\"] == TARGET_CLASSIFICATION\n",
    "]\n",
    "print(\"\\n---- Filtered Results ----\")\n",
    "print(json.dumps(filtered_results, indent=2, ensure_ascii=False))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Part 3: Update JSON documents from SQLite based on filtered results\n",
    "# =============================================================================\n",
    "\n",
    "# --- XML Parsing Functions ---\n",
    "def build_category_dict(xml_element):\n",
    "    category_id = xml_element.get('id')\n",
    "    category_name = xml_element.get('name')\n",
    "    \n",
    "    children_dict = {}\n",
    "    for child_elem in xml_element.findall('{*}category'):\n",
    "        child_data = build_category_dict(child_elem)\n",
    "        children_dict[child_data['name']] = child_data\n",
    "\n",
    "    return {\n",
    "        'id': category_id,\n",
    "        'name': category_name,\n",
    "        'children': children_dict\n",
    "    }\n",
    "\n",
    "def parse_category_xml(xml_path):\n",
    "    tree = ET.parse(xml_path, parser=ET.XMLParser(encoding='utf-8'))\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    category_system = root.find('.//{*}categories[@dataType=\"Process\"]')\n",
    "    top_level_dict = {}\n",
    "    if category_system is not None:\n",
    "        for cat_elem in category_system.findall('{*}category'):\n",
    "            cat_data = build_category_dict(cat_elem)\n",
    "            top_level_dict[cat_data['name']] = cat_data\n",
    "    else:\n",
    "        print(\"Warning: Could not find <categories dataType='Process'> in XML.\")\n",
    "    \n",
    "    return top_level_dict\n",
    "\n",
    "def get_category_path_info(category_hierarchy, categories_dict):\n",
    "    \"\"\"Given a list of category names and your category_dict, \n",
    "       return [(name, id), (name, id), ...] for each level.\"\"\"\n",
    "    path_info = []\n",
    "    if not category_hierarchy:\n",
    "        return path_info\n",
    "\n",
    "    current_dict = categories_dict\n",
    "    for i, cat_name in enumerate(category_hierarchy):\n",
    "        if cat_name not in current_dict:\n",
    "            raise ValueError(f\"Category '{cat_name}' not found at level {i}.\")\n",
    "        this_cat = current_dict[cat_name]\n",
    "        path_info.append((this_cat['name'], this_cat['id']))\n",
    "        current_dict = this_cat['children']\n",
    "    return path_info\n",
    "\n",
    "# Parse the XML to build the category lookup structure\n",
    "xml_path = \"../data/pipeline2/xml/OEKOBAU.DAT_Categories_EN_API.xml\"\n",
    "category_dict = parse_category_xml(xml_path)\n",
    "\n",
    "# Connect to the SQLite DB\n",
    "db_path = \"../data/pipeline2/sql/epd_database.sqlite\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "for entry in filtered_results:\n",
    "    uuid_val = entry[\"UUID\"]\n",
    "    classification_str = entry[\"best_category\"]\n",
    "\n",
    "    # Only process those that match the target classification\n",
    "    if classification_str == TARGET_CLASSIFICATION:\n",
    "        # Fetch JSON from epd_documents table by UUID\n",
    "        cursor.execute(\"SELECT document FROM epd_documents WHERE uuid = ?\", (uuid_val,))\n",
    "        result = cursor.fetchone()\n",
    "        if not result:\n",
    "            print(f\"No document found for UUID: {uuid_val}\")\n",
    "            continue\n",
    "\n",
    "        json_text = result[0]\n",
    "        try:\n",
    "            json_data = json.loads(json_text)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to decode JSON for UUID: {uuid_val}\")\n",
    "            continue\n",
    "        \n",
    "        # Split the classification string into separate category names\n",
    "        category_names = [x.strip() for x in classification_str.split(\">\")]\n",
    "        try:\n",
    "            path_info = get_category_path_info(category_names, category_dict)\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: Could not resolve categories for UUID: {uuid_val}. {e}\")\n",
    "            continue\n",
    "\n",
    "        # Build an array describing each category level\n",
    "        class_array = []\n",
    "        for level, (cat_name, cat_id) in enumerate(path_info):\n",
    "            class_array.append({\n",
    "                \"value\": cat_name,\n",
    "                \"level\": level,\n",
    "                \"classId\": cat_id\n",
    "            })\n",
    "\n",
    "        # Prepare a new classification object (with \"name\" = \"OEKOBAU.DAT\" as example)\n",
    "        new_classification_item = {\n",
    "            \"class\": class_array,\n",
    "            \"name\": \"OEKOBAU.DAT\"\n",
    "        }\n",
    "        \n",
    "        # Inject the new classification while preserving existing ones\n",
    "        process_info = json_data.setdefault(\"processInformation\", {})\n",
    "        data_set_info = process_info.setdefault(\"dataSetInformation\", {})\n",
    "        classification_info = data_set_info.setdefault(\"classificationInformation\", {})\n",
    "\n",
    "        # Retrieve existing classifications; if missing, initialize an empty list\n",
    "        existing_classifications = classification_info.get(\"classification\")\n",
    "        if not isinstance(existing_classifications, list):\n",
    "            existing_classifications = []\n",
    "        # Append the new classification\n",
    "        existing_classifications.append(new_classification_item)\n",
    "        classification_info[\"classification\"] = existing_classifications\n",
    "        \n",
    "        # Save the updated JSON in our in-memory dict\n",
    "        modified_json_docs[uuid_val] = json_data\n",
    "\n",
    "        print(f\"\\nAppended new category for UUID {uuid_val}: {classification_str}\")\n",
    "        print(\"Full classification now:\")\n",
    "        print(json.dumps(classification_info[\"classification\"], indent=2))\n",
    "        print(\"--------------------------------------------------\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(\"\\n---- Modified JSON Documents ----\")\n",
    "print(json.dumps(modified_json_docs, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(modified_json_docs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Query all \"Beton\" with \"OEKOBAU.DAT\" classification in the DB\n",
    "\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "db_path = \"../data/pipeline2/sql/epd_database.sqlite\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT d.uuid,\n",
    "       d.document\n",
    "FROM epd_documents d\n",
    "JOIN epd_metadata m \n",
    "   ON d.uuid = m.uuid\n",
    "JOIN json_each(d.document, '$.processInformation.dataSetInformation.classificationInformation.classification') classification\n",
    "JOIN json_each(classification.value, '$.class') class_item\n",
    "WHERE m.classification_system = 'OEKOBAU.DAT'\n",
    "  AND json_extract(class_item.value, '$.value') = 'Beton'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(query)\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "docs_with_beton = {}\n",
    "for (epd_uuid, doc_text) in rows:\n",
    "    # parse the JSON for further processing in Python\n",
    "    try:\n",
    "        doc_data = json.loads(doc_text)\n",
    "        docs_with_beton[epd_uuid] = doc_data\n",
    "    except json.JSONDecodeError:\n",
    "        # skip invalid JSON\n",
    "        continue\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(f\"Found {len(docs_with_beton)} documents that truly have 'Beton' in their classification array.\")\n",
    "for u in docs_with_beton:\n",
    "    print(\" -\", u)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Rename JSON keys (updated with new logic)\n",
    "\n",
    "import json\n",
    "\n",
    "def recursive_rename_uri(obj):\n",
    "    \"\"\"\n",
    "    Recursively rename any key 'uri' to 'refObjectUri' in the given object (dict/list).\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        new_obj = {}\n",
    "        for key, value in obj.items():\n",
    "            new_key = \"refObjectUri\" if key == \"uri\" else key\n",
    "            new_obj[new_key] = recursive_rename_uri(value)\n",
    "        return new_obj\n",
    "    elif isinstance(obj, list):\n",
    "        return [recursive_rename_uri(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def remove_raw_strings_in_anies(obj):\n",
    "    \"\"\"\n",
    "    Recursively traverse the object and remove any raw string elements from lists\n",
    "    that belong to a key named 'anies'.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        new_obj = {}\n",
    "        for key, value in obj.items():\n",
    "            if key == \"anies\" and isinstance(value, list):\n",
    "                # Filter out raw string elements from this list.\n",
    "                new_obj[key] = [\n",
    "                    remove_raw_strings_in_anies(item)\n",
    "                    for item in value\n",
    "                    if not isinstance(item, str)\n",
    "                ]\n",
    "            else:\n",
    "                new_obj[key] = remove_raw_strings_in_anies(value)\n",
    "        return new_obj\n",
    "    elif isinstance(obj, list):\n",
    "        return [remove_raw_strings_in_anies(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def transform_json(data):\n",
    "    \"\"\"\n",
    "    Transform an Environmental Product Declaration (EPD) JSON instance by renaming keys\n",
    "    and restructuring its contents to standardize the schema.\n",
    "    \n",
    "    Major changes in this version:\n",
    "      - Globally rename every \"uri\" to \"refObjectUri\"\n",
    "      - Globally remove raw string elements from any 'anies' lists\n",
    "      - Keep the rest of your original rename logic: processInformation, modellingAndValidation, \n",
    "        administrativeInformation, exchanges, LCIAResults, etc.\n",
    "      - Exclude the key \"relativeStandardDeviation95In\" from LCIAResults.LCIAResult and exchanges.exchange.\n",
    "\n",
    "    Returns: The updated dictionary (in-memory).\n",
    "    \"\"\"\n",
    "    # --- processInformation transformations ---\n",
    "    process_info = data.get(\"processInformation\", {})\n",
    "    data_set_info = process_info.get(\"dataSetInformation\", {})\n",
    "\n",
    "    # Rename \"dataSetInformation.name\" -> \"dataSetName\"\n",
    "    if \"name\" in data_set_info:\n",
    "        data_set_info[\"dataSetName\"] = data_set_info.pop(\"name\")\n",
    "\n",
    "    # Rename dataSetInformation.other -> otherDSI; remove 'componentsAndMaterialsAndSubstances' if found\n",
    "    if \"other\" in data_set_info:\n",
    "        data_set_info[\"otherDSI\"] = data_set_info.pop(\"other\")\n",
    "        # If any item in otherDSI.anies contains \"componentsAndMaterialsAndSubstances\", remove the entire \"anies\" key.\n",
    "        if \"anies\" in data_set_info[\"otherDSI\"]:\n",
    "            for item in data_set_info[\"otherDSI\"][\"anies\"]:\n",
    "                if (\n",
    "                    isinstance(item, dict)\n",
    "                    and \"componentsAndMaterialsAndSubstances\" in item\n",
    "                ):\n",
    "                    data_set_info[\"otherDSI\"].pop(\"anies\")\n",
    "                    break\n",
    "            # Rename dataSetInformation.other.anies.scenario -> objectScenario;\n",
    "            if \"scenario\" in data_set_info[\"otherDSI\"]:\n",
    "                data_set_info[\"objectScenario\"] = data_set_info[\"otherDSI\"].pop(\"scenario\")\n",
    "\n",
    "    # For classification entries, rename \"class\" -> \"classEntries\"\n",
    "    classification_info = data_set_info.get(\"classificationInformation\", {})\n",
    "    classifications = classification_info.get(\"classification\", [])\n",
    "    for cls_obj in classifications:\n",
    "        if \"class\" in cls_obj:\n",
    "            cls_obj[\"classEntries\"] = cls_obj.pop(\"class\")\n",
    "\n",
    "    # Rename \"time\" -> \"timeInformation\", then rename \"other\" -> \"otherTime\", and \"value\" -> \"timestampValue\"\n",
    "    if \"time\" in process_info:\n",
    "        process_info[\"timeInformation\"] = process_info.pop(\"time\")\n",
    "        time_info = process_info[\"timeInformation\"]\n",
    "        if \"other\" in time_info:\n",
    "            time_info[\"otherTime\"] = time_info.pop(\"other\")\n",
    "            for item in time_info[\"otherTime\"].get(\"anies\", []):\n",
    "                if isinstance(item, dict) and \"value\" in item:\n",
    "                    item[\"timestampValue\"] = item.pop(\"value\")\n",
    "\n",
    "    # --- modellingAndValidation transformations ---\n",
    "    mod_val = data.get(\"modellingAndValidation\", {})\n",
    "\n",
    "    # LCIMethodAndAllocation.other -> otherMAA\n",
    "    lci_method = mod_val.get(\"LCIMethodAndAllocation\", {})\n",
    "    if \"other\" in lci_method:\n",
    "        lci_method[\"otherMAA\"] = lci_method.pop(\"other\")\n",
    "\n",
    "    # dataSourcesTreatmentAndRepresentativeness.other -> otherDSTAR\n",
    "    dstar = mod_val.get(\"dataSourcesTreatmentAndRepresentativeness\", {})\n",
    "    if \"other\" in dstar:\n",
    "        dstar[\"otherDSTAR\"] = dstar.pop(\"other\")\n",
    "        if \"anies\" in dstar[\"otherDSTAR\"]:\n",
    "            dstar[\"otherDSTAR\"][\"aniesDSTAR\"] = dstar[\"otherDSTAR\"].pop(\"anies\")\n",
    "            for item in dstar[\"otherDSTAR\"][\"aniesDSTAR\"]:\n",
    "                if isinstance(item, dict) and \"value\" in item and isinstance(item[\"value\"], dict):\n",
    "                    # rename \"value\" -> \"valueDSTAR\", then rename subkeys\n",
    "                    item[\"valueDSTAR\"] = item.pop(\"value\")\n",
    "                    val = item[\"valueDSTAR\"]\n",
    "                    if \"shortDescription\" in val:\n",
    "                        val[\"shortDescriptionExtended\"] = val.pop(\"shortDescription\")\n",
    "                    if \"version\" in val:\n",
    "                        version = val.pop(\"version\")\n",
    "                        if \"version\" in version:\n",
    "                            version[\"versionInt\"] = version.pop(\"version\")\n",
    "                        val[\"versionDict\"] = version\n",
    "                    if \"uuid\" in val:\n",
    "                        uuid_obj = val.pop(\"uuid\")\n",
    "                        if \"uuid\" in uuid_obj:\n",
    "                            uuid_obj[\"uuidValue\"] = uuid_obj.pop(\"uuid\")\n",
    "                        val[\"uuidDict\"] = uuid_obj\n",
    "\n",
    "    # Rename \"validation\" -> \"validationInfo\"\n",
    "    if \"validation\" in mod_val:\n",
    "        mod_val[\"validationInfo\"] = mod_val.pop(\"validation\")\n",
    "\n",
    "    # modellingAndValidation.other -> otherMAV; if \"value\" is a dict, rename it to \"objectValue\"\n",
    "    if \"other\" in mod_val:\n",
    "        mod_val[\"otherMAV\"] = mod_val.pop(\"other\")\n",
    "        for item in mod_val[\"otherMAV\"].get(\"anies\", []):\n",
    "            if isinstance(item, dict) and \"value\" in item and isinstance(item[\"value\"], dict):\n",
    "                item[\"objectValue\"] = item.pop(\"value\")\n",
    "\n",
    "    # --- administrativeInformation transformations ---\n",
    "    admin_info = data.get(\"administrativeInformation\", {})\n",
    "\n",
    "    # publicationAndOwnership.other -> otherPAO; rename \"value\" -> \"objectValue\" if it's a dict\n",
    "    pub_own = admin_info.get(\"publicationAndOwnership\", {})\n",
    "    if \"other\" in pub_own:\n",
    "        pub_own[\"otherPAO\"] = pub_own.pop(\"other\")\n",
    "        for item in pub_own[\"otherPAO\"].get(\"anies\", []):\n",
    "            if isinstance(item, dict) and \"value\" in item and isinstance(item[\"value\"], dict):\n",
    "                item[\"objectValue\"] = item.pop(\"value\")\n",
    "\n",
    "    # --- exchanges transformations ---\n",
    "    exchanges = data.get(\"exchanges\", {}).get(\"exchange\", [])\n",
    "    for exchange in exchanges:\n",
    "        # flowProperties: rename name->nameFP, uuid->uuidFP\n",
    "        for fp in exchange.get(\"flowProperties\", []):\n",
    "            if \"name\" in fp:\n",
    "                fp[\"nameFP\"] = fp.pop(\"name\")\n",
    "            if \"uuid\" in fp:\n",
    "                fp[\"uuidFP\"] = fp.pop(\"uuid\")\n",
    "\n",
    "        # rename \"exchange direction\" -> \"exchangeDirection\"\n",
    "        if \"exchange direction\" in exchange:\n",
    "            exchange[\"exchangeDirection\"] = exchange.pop(\"exchange direction\")\n",
    "\n",
    "        # rename \"other\" -> \"otherEx\", if \"value\" is dict -> \"objectValue\"\n",
    "        if \"other\" in exchange:\n",
    "            exchange[\"otherEx\"] = exchange.pop(\"other\")\n",
    "            for item in exchange[\"otherEx\"].get(\"anies\", []):\n",
    "                if isinstance(item, dict) and \"value\" in item and isinstance(item[\"value\"], dict):\n",
    "                    item[\"objectValue\"] = item.pop(\"value\")\n",
    "\n",
    "        # rename \"classification\" -> \"classificationEx\" and inside, rename \"name\"->\"nameClass\"\n",
    "        if \"classification\" in exchange:\n",
    "            exchange[\"classificationEx\"] = exchange.pop(\"classification\")\n",
    "            if \"name\" in exchange[\"classificationEx\"]:\n",
    "                exchange[\"classificationEx\"][\"nameClass\"] = exchange[\"classificationEx\"].pop(\"name\")\n",
    "        \n",
    "        # exclude relativeStandardDeviation95In from exchanges.exchange ---\n",
    "        if \"relativeStandardDeviation95In\" in exchange:\n",
    "            exchange.pop(\"relativeStandardDeviation95In\")\n",
    "\n",
    "    # --- LCIAResults transformations ---\n",
    "    # rename \"LCIAResults\" -> \"lciaResults\" if present\n",
    "    if \"LCIAResults\" in data:\n",
    "        data[\"lciaResults\"] = data.pop(\"LCIAResults\")\n",
    "\n",
    "    lcia_results = data.get(\"lciaResults\", {}).get(\"LCIAResult\", [])\n",
    "    for result in lcia_results:\n",
    "        # rename \"other\" -> \"otherLCIA\", if \"value\" is dict -> \"objectValue\"\n",
    "        if \"other\" in result:\n",
    "            result[\"otherLCIA\"] = result.pop(\"other\")\n",
    "            for item in result[\"otherLCIA\"].get(\"anies\", []):\n",
    "                if isinstance(item, dict) and \"value\" in item and isinstance(item[\"value\"], dict):\n",
    "                    item[\"objectValue\"] = item.pop(\"value\")\n",
    "        \n",
    "        # exclude relativeStandardDeviation95In from each LCIA result ---\n",
    "        if \"relativeStandardDeviation95In\" in result:\n",
    "            result.pop(\"relativeStandardDeviation95In\")\n",
    "\n",
    "    # --- Removal ---\n",
    "    # Remove top-level \"otherAttributes\" key if it exists\n",
    "    if \"otherAttributes\" in data:\n",
    "        data.pop(\"otherAttributes\")\n",
    "    \n",
    "    # if \"modellingAndValidation\" in data:\n",
    "    #     data.pop(\"modellingAndValidation\")\n",
    "    \n",
    "    # if \"administrativeInformation\" in data:\n",
    "    #     data.pop(\"administrativeInformation\")\n",
    "    \n",
    "    # if \"exchanges\" in data:\n",
    "    #     data.pop(\"exchanges\")\n",
    "    \n",
    "    # if \"lciaResults\" in data:\n",
    "    #     data.pop(\"lciaResults\")\n",
    "    \n",
    "    if \"locations\" in data:\n",
    "        data.pop(\"locations\")\n",
    "\n",
    "\n",
    "    # --- Global step: remove raw string elements from any 'anies' list\n",
    "    data = remove_raw_strings_in_anies(data)\n",
    "\n",
    "    # --- Global step: rename \"uri\" -> \"refObjectUri\"\n",
    "    data = recursive_rename_uri(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Example pipeline usage (in memory, no disk I/O):\n",
    "# -----------------------------------------------------------\n",
    "for uuid_val, original_doc in modified_json_docs.items():\n",
    "    final_doc = transform_json(original_doc)\n",
    "\n",
    "    # If you want to store the transformed version back in the dictionary:\n",
    "    modified_json_docs[uuid_val] = final_doc\n",
    "\n",
    "    # Just print for verification\n",
    "    # print(f\"\\nTransformed JSON for UUID = {uuid_val}:\")\n",
    "    # print(json.dumps(final_doc, indent=2))\n",
    "    # print(\"--------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Add ids\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "# --------------------------- Utility functions ---------------------------\n",
    "\n",
    "def load_yaml_schema(file_path):\n",
    "    \"\"\"Load the LinkML YAML schema from disk.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def reorder_dict_keys(d):\n",
    "    \"\"\"\n",
    "    Reorders a dictionary so that if 'id' exists, it appears as the first key.\n",
    "    (For human readability.)\n",
    "    \"\"\"\n",
    "    if \"id\" in d:\n",
    "        id_value = d.pop(\"id\")\n",
    "        new_d = {\"id\": id_value}\n",
    "        new_d.update(d)\n",
    "        d.clear()\n",
    "        d.update(new_d)\n",
    "\n",
    "def clean_epd_name(name):\n",
    "    \"\"\"\n",
    "    Cleans the EPD name by replacing non-alphanumeric characters with underscores,\n",
    "    collapsing multiple underscores, and stripping leading/trailing underscores.\n",
    "    \"\"\"\n",
    "    cleaned = re.sub(r\"[^A-Za-z0-9]\", \"_\", name)\n",
    "    return re.sub(r\"_+\", \"_\", cleaned).strip(\"_\")\n",
    "\n",
    "# ------------------ New ID Creation Logic -------------------\n",
    "\n",
    "def generate_id_from_path(acc_path, prefix=\"ilcd\"):\n",
    "    \"\"\"\n",
    "    Given an accumulated path, returns the full ID as: prefix:acc_path\n",
    "    \"\"\"\n",
    "    return f\"{prefix}:{acc_path}\"\n",
    "\n",
    "def get_suffix(item, index):\n",
    "    \"\"\"\n",
    "    If the list element (item) is a dict with a 'module' field,\n",
    "    return 'module' + its value (dashes removed);\n",
    "    otherwise, return the 1-based index as a string.\n",
    "    \"\"\"\n",
    "    if isinstance(item, dict) and \"module\" in item:\n",
    "        return f\"module{item['module'].replace('-', '')}\"\n",
    "    return str(index + 1)\n",
    "\n",
    "def assign_ids_by_path(obj, epd_uuid, acc_path, parent_is_list, prefix=\"ilcd\"):\n",
    "    \"\"\"\n",
    "    Recursively assigns IDs based on the accumulated path.\n",
    "\n",
    "    Parameters:\n",
    "      obj          : current object (dict, list, or primitive)\n",
    "      epd_uuid     : the top-level EPD UUID (dashes removed), used as a base\n",
    "      acc_path     : the accumulated path string\n",
    "      parent_is_list : bool indicating whether the parent container was a list\n",
    "      prefix       : the string prefix to use, e.g. \"ilcd\"\n",
    "    \"\"\"\n",
    "    # If this is a dict and has no 'id', generate one\n",
    "    if isinstance(obj, dict) and \"id\" not in obj:\n",
    "        # ID => prefix:epd_uuid_accPath\n",
    "        obj[\"id\"] = generate_id_from_path(f\"{epd_uuid}_{acc_path}\", prefix)\n",
    "        reorder_dict_keys(obj)\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        # Traverse each key\n",
    "        for key, value in obj.items():\n",
    "            if key == \"id\":\n",
    "                continue\n",
    "            if isinstance(value, dict):\n",
    "                # If parent is a list, extend path with \"_key\"\n",
    "                new_acc = f\"{acc_path}_{key}\" if parent_is_list else key\n",
    "                assign_ids_by_path(value, epd_uuid, new_acc, parent_is_list=False, prefix=prefix)\n",
    "            elif isinstance(value, list):\n",
    "                # For a list, extend path with \"_key\"\n",
    "                new_acc = f\"{acc_path}_{key}\"\n",
    "                for i, item in enumerate(value):\n",
    "                    suffix = get_suffix(item, i)\n",
    "                    # element path => new_acc + \"_\" + suffix\n",
    "                    element_acc = f\"{new_acc}_{suffix}\"\n",
    "                    assign_ids_by_path(item, epd_uuid, element_acc, parent_is_list=True, prefix=prefix)\n",
    "            # If it's a primitive, do nothing\n",
    "    elif isinstance(obj, list):\n",
    "        # If this is a list, iterate elements\n",
    "        for i, item in enumerate(obj):\n",
    "            suffix = get_suffix(item, i)\n",
    "            new_acc = f\"{acc_path}_{suffix}\"\n",
    "            assign_ids_by_path(item, epd_uuid, new_acc, parent_is_list=True, prefix=prefix)\n",
    "\n",
    "\n",
    "# --------------------- MAIN LOGIC: In-memory Example ---------------------\n",
    "\n",
    "SCHEMA_PATH = \"../linkml/data/yaml/linkml_ILCDmergedSchemas_schema.yaml\"\n",
    "schema = load_yaml_schema(SCHEMA_PATH)\n",
    "\n",
    "# Fallback if your schema has a default prefix; else use \"ilcd\"\n",
    "default_prefix = schema.get(\"default_prefix\", \"ilcd\")\n",
    "\n",
    "# \n",
    "# We assume `modified_json_docs` is already defined in memory:\n",
    "# e.g. modified_json_docs = {\n",
    "#     \"uuid1\": {...final JSON doc...},\n",
    "#     \"uuid2\": {...final JSON doc...},\n",
    "#     ...\n",
    "# }\n",
    "\n",
    "for uuid_val, doc in modified_json_docs.items():\n",
    "    # 1) Get the \"real\" UUID from the doc (with dashes)\n",
    "    try:\n",
    "        raw_uuid = doc[\"processInformation\"][\"dataSetInformation\"][\"UUID\"]\n",
    "    except KeyError as e:\n",
    "        raise KeyError(f\"Missing processInformation.dataSetInformation.UUID in doc {uuid_val}\") from e\n",
    "    \n",
    "    # 2) Remove dashes to build a base\n",
    "    epd_uuid = raw_uuid.replace(\"-\", \"\")\n",
    "\n",
    "    # 3) Assign top-level doc ID (prefix:epd_uuid)\n",
    "    doc[\"id\"] = f\"{default_prefix.lower()}:{epd_uuid}\"\n",
    "\n",
    "    # 4) For each top-level key (besides 'id'/'version'), set a sub-ID and recursively assign deeper IDs\n",
    "    for top_key, top_obj in doc.items():\n",
    "        if top_key in [\"id\", \"version\"]:\n",
    "            continue\n",
    "        if isinstance(top_obj, dict):\n",
    "            # e.g. \"processInformation\" => prefix:epd_uuid_processInformation\n",
    "            top_obj[\"id\"] = f\"{default_prefix.lower()}:{epd_uuid}_{top_key}\"\n",
    "            reorder_dict_keys(top_obj)\n",
    "            assign_ids_by_path(\n",
    "                top_obj,\n",
    "                epd_uuid=epd_uuid,\n",
    "                acc_path=top_key,\n",
    "                parent_is_list=False,\n",
    "                prefix=default_prefix.lower()\n",
    "            )\n",
    "        elif isinstance(top_obj, list):\n",
    "            # If it's a list at top level, handle each item\n",
    "            for i, item in enumerate(top_obj):\n",
    "                suffix = get_suffix(item, i)\n",
    "                # top path => top_key + \"_\" + suffix\n",
    "                top_path = f\"{top_key}_{suffix}\"\n",
    "                assign_ids_by_path(\n",
    "                    item,\n",
    "                    epd_uuid=epd_uuid,\n",
    "                    acc_path=top_path,\n",
    "                    parent_is_list=True,\n",
    "                    prefix=default_prefix.lower()\n",
    "                )\n",
    "\n",
    "    # 5) Print final JSON to confirm\n",
    "    # print(f\"\\n=== JSON with newly assigned IDs for doc (pipeline UUID): {uuid_val} ===\")\n",
    "    # print(json.dumps(doc, indent=2))\n",
    "    # print(\"-----------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to RDF\n",
    "\n",
    "import rdflib\n",
    "from linkml.validator import Validator\n",
    "from linkml_runtime.loaders import YAMLLoader\n",
    "from linkml_runtime.dumpers import RDFLibDumper\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "\n",
    "# Import your generated Python dataclass for the schema\n",
    "# e.g., from data.py.linkml_processDataSet_schema import ProcessDataSet\n",
    "from data.py.linkml_processDataSet_schema import ProcessDataSet\n",
    "\n",
    "\n",
    "def generate_turtle_from_docs(\n",
    "    docs_dict,\n",
    "    schema_path,\n",
    "    turtle_output_path,\n",
    "    validate: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a dictionary of JSON documents (each conforming to 'ProcessDataSet'),\n",
    "    generate a single TTL file that contains all instances. If loading fails\n",
    "    for any doc, store it in a separate dictionary `failed_docs` for debugging.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    docs_dict : dict\n",
    "        A dict of {UUID: JSON-Dict} storing your EPD JSON objects in memory.\n",
    "    schema_path : str\n",
    "        Path to the LinkML YAML schema (e.g. '../linkml/data/yaml/linkml_processDataSet_schema.yaml').\n",
    "    turtle_output_path : str\n",
    "        Where to write the combined Turtle RDF graph (overwrites each run).\n",
    "    validate : bool\n",
    "        If True, runs the LinkML Validator on each instance before RDF conversion.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    failed_docs : dict\n",
    "        A dictionary of {UUID: JSON-Dict} for any documents that failed to load.\n",
    "    \"\"\"\n",
    "    # 1) Create an empty rdflib graph to combine all instance graphs\n",
    "    combined_graph = rdflib.Graph()\n",
    "\n",
    "    # 2) Load the schema into a SchemaView for RDF generation\n",
    "    sv = SchemaView(schema_path)\n",
    "\n",
    "    # (Optional) set up a validator if needed\n",
    "    validator = None\n",
    "    if validate:\n",
    "        validator = Validator(schema_path, strict=False)\n",
    "\n",
    "    # A container to track documents that fail to load\n",
    "    failed_docs = {}\n",
    "\n",
    "    # 3) For each doc, wrap in a top-level \"processDataSet\" key,\n",
    "    #    load as an object, optionally validate, and convert to RDF.\n",
    "    dumper = RDFLibDumper()\n",
    "\n",
    "    success_count = 0\n",
    "\n",
    "    for uuid_val, json_doc in docs_dict.items():\n",
    "        yaml_wrapper = {\"processDataSet\": json_doc}\n",
    "\n",
    "        # (A) Validate the doc if requested\n",
    "        if validator:\n",
    "            report = validator.validate(yaml_wrapper, \"ProcessDataSet\")\n",
    "            if report.results:\n",
    "                print(f\"[VALIDATION] Errors for UUID={uuid_val}:\")\n",
    "                for result in report.results:\n",
    "                    print(\"  -\", result.message)\n",
    "                # We can decide to skip, but let's let the user decide:\n",
    "                # continue\n",
    "            else:\n",
    "                print(f\"[VALIDATION] Document {uuid_val} is valid according to the schema.\")\n",
    "\n",
    "        # (B) Attempt to load as a ProcessDataSet\n",
    "        try:\n",
    "            instance_obj = YAMLLoader().load(yaml_wrapper[\"processDataSet\"], target_class=ProcessDataSet)\n",
    "        except (ValueError, TypeError) as e:\n",
    "            print(f\"[ERROR] Failed to load doc {uuid_val} as ProcessDataSet. Reason:\\n  {e}\")\n",
    "            failed_docs[uuid_val] = json_doc\n",
    "            continue  # Skip adding to the graph\n",
    "\n",
    "        # (C) Convert to RDF (rdflib.Graph) and accumulate\n",
    "        instance_graph = dumper.as_rdf_graph(instance_obj, schemaview=sv)\n",
    "        combined_graph += instance_graph\n",
    "        success_count += 1\n",
    "\n",
    "    # 4) Write the combined graph to Turtle\n",
    "    combined_graph.serialize(destination=turtle_output_path, format=\"turtle\")\n",
    "    print(f\"\\nSuccessfully wrote {success_count} instances to the Turtle file:\\n  {turtle_output_path}\")\n",
    "    if failed_docs:\n",
    "        print(f\"{len(failed_docs)} documents failed to load and were skipped.\")\n",
    "\n",
    "    # Return the dictionary of failed docs for further handling or debugging\n",
    "    return failed_docs\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# USAGE EXAMPLE (in the same or next cell)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Suppose you have a dict `modified_json_docs` from prior steps:\n",
    "# modified_json_docs = {\n",
    "#     \"uuid1\": {... doc ...},\n",
    "#     \"uuid2\": {... doc ...},\n",
    "#     # ...\n",
    "# }\n",
    "\n",
    "# Then do:\n",
    "schema_file = \"../linkml/data/yaml/linkml_processDataSet_schema.yaml\"\n",
    "ttl_output = \"../linkml/data/rdf/epd_rdf_instance_datastore.ttl\"\n",
    "\n",
    "failed = generate_turtle_from_docs(\n",
    "    docs_dict=modified_json_docs,\n",
    "    schema_path=schema_file,\n",
    "    turtle_output_path=ttl_output,\n",
    "    validate=True\n",
    ")\n",
    "\n",
    "if failed:\n",
    "    print(\"\\nFailed doc details:\")\n",
    "    for bad_uuid, bad_doc in failed.items():\n",
    "        print(\" -\", bad_uuid, \"(Doc not loaded successfully)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
