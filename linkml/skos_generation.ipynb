{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up LinkML section creation prefixes\n",
    "\n",
    "import rdflib\n",
    "\n",
    "def unify_ns_prefixes(\n",
    "    input_turtle: str,\n",
    "    output_turtle: str,\n",
    "    base_uri: str = \"https://example.org/ilcd/\",\n",
    "    old_ns_list = (\n",
    "        \"ILCDex:\",\n",
    "        \"ILCDsd:\",\n",
    "        \"ILCDlcia:\",\n",
    "        \"ILCDmav:\",\n",
    "        \"ILCDadmin:\",\n",
    "        \"ILCDpi:\",\n",
    "    )\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a Turtle file containing multiple 'ns' prefixes and unify them\n",
    "    into a single 'ilcd:' prefix, rewriting URIs that match old_ns_list\n",
    "    to the 'base_uri'.\n",
    "\n",
    "    :param input_turtle: Path to the original .ttl file (with multiple prefixes)\n",
    "    :param output_turtle: Path to the new .ttl file (with a single prefix \"ilcd\")\n",
    "    :param base_uri: URI used for rewriting old prefixes (ends with '/')\n",
    "    :param old_ns_list: Tuple of strings to unify. Each string is the old namespace up to but not including local name\n",
    "                       E.g. \"ILCDex:\", \"ILCDsd:\", or the generated \"ns1:\" etc.\n",
    "                       You can also match the expanded forms if you prefer.\n",
    "    \"\"\"\n",
    "    # 1) Parse the old Turtle file into an rdflib Graph\n",
    "    old_g = rdflib.Graph()\n",
    "    old_g.parse(input_turtle, format=\"turtle\")\n",
    "    print(f\"Loaded graph with {len(old_g)} triples from {input_turtle}\")\n",
    "\n",
    "    # 2) Create a new empty Graph\n",
    "    new_g = rdflib.Graph()\n",
    "\n",
    "    # 3) Bind ONLY the single 'ilcd' prefix to your base_uri\n",
    "    new_g.bind(\"ilcd\", rdflib.URIRef(base_uri))\n",
    "    # Optionally bind other well-known prefixes like xsd, rdf, skos, etc. if you want\n",
    "    new_g.bind(\"xsd\", rdflib.URIRef(\"http://www.w3.org/2001/XMLSchema#\"))\n",
    "    # new_g.bind(\"skos\", rdflib.URIRef(\"http://www.w3.org/2004/02/skos/core#\"))\n",
    "    # etc.\n",
    "\n",
    "    def unify_uri(u):\n",
    "        \"\"\"\n",
    "        If `u` is a URIRef matching one of our old namespaces,\n",
    "        rewrite it to the new base_uri + localname.\n",
    "        Otherwise, return as-is.\n",
    "        \"\"\"\n",
    "        if not isinstance(u, rdflib.URIRef):\n",
    "            return u\n",
    "\n",
    "        # Try computing qname from old_g. This helps us get local name easily.\n",
    "        # If the URI is completely unknown, we fallback to raw rewriting.\n",
    "        try:\n",
    "            prefix, ns, local = old_g.compute_qname(u)\n",
    "        except Exception:\n",
    "            # If compute_qname fails, you can keep it as is or do custom logic\n",
    "            return u\n",
    "\n",
    "        # Check if the prefix is one of the old ns (ns1, ns2, ILCDex, etc.)\n",
    "        # or if `ns` matches a known old namespace URI. You can decide your logic here.\n",
    "        # We'll do a simple approach: if prefix starts with \"ns\" or in old_ns_list, unify it\n",
    "        if prefix.startswith(\"ns\") or prefix + \":\" in old_ns_list:\n",
    "            return rdflib.URIRef(base_uri + local)\n",
    "        if prefix in old_ns_list:\n",
    "            return rdflib.URIRef(base_uri + local)\n",
    "\n",
    "        # else keep as-is (e.g., xsd:, rdf:, skos:)\n",
    "        return u\n",
    "\n",
    "    # 4) Iterate over all triples in old_g, rewrite them, and add to new_g\n",
    "    for s, p, o in old_g:\n",
    "        s_new = unify_uri(s)\n",
    "        p_new = unify_uri(p)\n",
    "        o_new = unify_uri(o)\n",
    "        new_g.add((s_new, p_new, o_new))\n",
    "\n",
    "    # 5) Serialize the new graph to Turtle\n",
    "    new_g.serialize(destination=output_turtle, format=\"turtle\")\n",
    "    print(f\"Rewrote {len(new_g)} triples to {output_turtle} with a single 'ilcd:' prefix.\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# USAGE:\n",
    "unify_ns_prefixes(\n",
    "    input_turtle=\"data/rdf/epd_rdf_instance_datastore.ttl\",\n",
    "    output_turtle=\"data/rdf/epd_rdf_instance_datastore.ttl\",\n",
    "    base_uri=\"https://example.org/ilcd/\",\n",
    "    old_ns_list=(\"ILCDex:\", \"ILCDsd:\", \"ILCDlcia:\", \"ILCDmav:\", \"ILCDadmin:\", \"ILCDpi:\")  # etc.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKOS Material Category\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from rdflib import Graph, Namespace, Literal, RDF\n",
    "from rdflib.namespace import SKOS\n",
    "\n",
    "# Create a new graph and load the existing RDF data.\n",
    "g = Graph()\n",
    "\n",
    "# Define namespaces.\n",
    "ILCD = Namespace(\"https://example.org/ilcd/\")\n",
    "OBD = Namespace(\"https://example.org/obd/\")  # use OBD instead of EX\n",
    "g.bind(\"ilcd\", ILCD)\n",
    "g.bind(\"obd\", OBD)\n",
    "g.bind(\"skos\", SKOS)\n",
    "\n",
    "# Load the initial RDF data from file (which uses the unified \"ilcd:\" prefix).\n",
    "rdf_file_path = \"data/rdf/epd_rdf_instance_datastore.ttl\"\n",
    "g.parse(rdf_file_path, format=\"turtle\")\n",
    "print(f\"Loaded RDF data from {rdf_file_path}\")\n",
    "\n",
    "# File paths for the English and German XML category files.\n",
    "file_en = \"../data/pipeline2/xml/OEKOBAU.DAT_Categories_EN_aligned_temp.xml\"\n",
    "file_de = \"../data/pipeline2/xml/OEKOBAU.DAT_Categories.xml\"\n",
    "\n",
    "# --- PART 1: Parse the XML files and extract only the target categories ---\n",
    "# We care only about these categories (normalized to lowercase):\n",
    "target_keys = {\n",
    "    \"mineral building products\",\n",
    "    \"mineralische baustoffe\",\n",
    "    \"mortar and concrete\",\n",
    "    \"mörtel und beton\",\n",
    "    \"ready mixed concrete\",\n",
    "    \"beton\",\n",
    "}\n",
    "\n",
    "def parse_category_system(file_path):\n",
    "    \"\"\"Parse the XML file and return the <categories> element.\"\"\"\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    ns = {\"cat\": \"http://lca.jrc.it/ILCD/Categories\"}\n",
    "    # If the root is the CategorySystem with name \"OEKOBAU.DAT\", use it.\n",
    "    if root.tag.endswith(\"CategorySystem\") and root.get(\"name\") == \"OEKOBAU.DAT\":\n",
    "        cs = root\n",
    "    else:\n",
    "        cs = root.find(\".//cat:CategorySystem[@name='OEKOBAU.DAT']\", ns)\n",
    "    if cs is None:\n",
    "        raise ValueError(f\"CategorySystem 'OEKOBAU.DAT' not found in {file_path}\")\n",
    "    categories_elem = cs.find(\"cat:categories\", ns)\n",
    "    return categories_elem, ns\n",
    "\n",
    "def extract_target_categories(categories_elem, ns):\n",
    "    \"\"\"\n",
    "    Traverse the XML category tree and return:\n",
    "      - targets: a dict mapping category id to its label (if its normalized name is in target_keys),\n",
    "      - relations: a list of tuples (parent_id, child_id) for target categories.\n",
    "    \"\"\"\n",
    "    targets = {}\n",
    "    relations = []\n",
    "    def traverse(elem, parent_id=None):\n",
    "        cat_id = elem.get(\"id\")\n",
    "        cat_name = elem.get(\"name\")\n",
    "        if cat_name:\n",
    "            norm = cat_name.lower().strip()\n",
    "            if norm in target_keys:\n",
    "                targets[cat_id] = cat_name\n",
    "                # Only record the relationship if the parent was also a target.\n",
    "                if parent_id is not None and parent_id in targets:\n",
    "                    relations.append((parent_id, cat_id))\n",
    "        for child in elem.findall(\"cat:category\", ns):\n",
    "            traverse(child, parent_id=cat_id)\n",
    "    for child in categories_elem.findall(\"cat:category\", ns):\n",
    "        traverse(child)\n",
    "    return targets, relations\n",
    "\n",
    "# Parse English and German files.\n",
    "categories_en_elem, ns_en = parse_category_system(file_en)\n",
    "categories_de_elem, ns_de = parse_category_system(file_de)\n",
    "\n",
    "# Extract target categories (id → label) and relationships from both files.\n",
    "targets_en, relations_en = extract_target_categories(categories_en_elem, ns_en)\n",
    "targets_de, _ = extract_target_categories(categories_de_elem, ns_de)\n",
    "\n",
    "# Merge the dictionaries using the category id as key.\n",
    "merged_targets = {}\n",
    "for cat_id, en_label in targets_en.items():\n",
    "    merged_targets[cat_id] = {\"en\": en_label}\n",
    "for cat_id, de_label in targets_de.items():\n",
    "    if cat_id in merged_targets:\n",
    "        merged_targets[cat_id][\"de\"] = de_label\n",
    "    else:\n",
    "        merged_targets[cat_id] = {\"de\": de_label}\n",
    "\n",
    "# --- PART 2: Create SKOS concepts from the XML data using OBD ---\n",
    "# Create a SKOS ConceptScheme for the categorization system.\n",
    "cat_scheme = OBD[\"OEKOBAU_DAT\"]\n",
    "g.add((cat_scheme, RDF.type, SKOS.ConceptScheme))\n",
    "g.add((cat_scheme, SKOS.prefLabel, Literal(\"OEKOBAU.DAT\", lang=\"en\")))\n",
    "g.add((cat_scheme, SKOS.prefLabel, Literal(\"OEKOBAU.DAT\", lang=\"de\")))\n",
    "\n",
    "# For each merged target category, create a SKOS Concept in the OBD namespace.\n",
    "for cat_id, labels in merged_targets.items():\n",
    "    # Build a URI for the category (replace dots with underscores for clarity).\n",
    "    cat_uri = OBD[\"Category_\" + cat_id.replace(\".\", \"_\")]\n",
    "    g.add((cat_uri, RDF.type, SKOS.Concept))\n",
    "    if \"en\" in labels:\n",
    "        g.add((cat_uri, SKOS.prefLabel, Literal(labels[\"en\"], lang=\"en\")))\n",
    "    if \"de\" in labels:\n",
    "        g.add((cat_uri, SKOS.prefLabel, Literal(labels[\"de\"], lang=\"de\")))\n",
    "    # Link the concept to the categorization scheme.\n",
    "    g.add((cat_uri, SKOS.inScheme, cat_scheme))\n",
    "\n",
    "# --- PART 3: Link existing ClassificationEntry resources to these concepts ---\n",
    "for entry in g.subjects(RDF.type, ILCD.ClassificationEntry):\n",
    "    value = g.value(entry, ILCD.value)\n",
    "    if value is not None:\n",
    "        norm_value = str(value).lower().strip()\n",
    "        if norm_value in target_keys:\n",
    "            for cat_id, labels in merged_targets.items():\n",
    "                if ((\"en\" in labels and labels[\"en\"].lower().strip() == norm_value)\n",
    "                    or (\"de\" in labels and labels[\"de\"].lower().strip() == norm_value)):\n",
    "                    cat_uri = OBD[\"Category_\" + cat_id.replace(\".\", \"_\")]\n",
    "                    g.add((entry, OBD.hasCanonicalCategory, cat_uri))\n",
    "                    break\n",
    "\n",
    "print(\"Canonical SKOS relationships created using XML data.\")\n",
    "\n",
    "# --- PART 4: Add hierarchical relationships based on the XML hierarchy.\n",
    "# We use the relationships extracted from the English XML.\n",
    "for parent_id, child_id in relations_en:\n",
    "    # Only add the relationship if both parent and child are in our merged targets.\n",
    "    if parent_id in merged_targets and child_id in merged_targets:\n",
    "        parent_uri = OBD[\"Category_\" + parent_id.replace(\".\", \"_\")]\n",
    "        child_uri = OBD[\"Category_\" + child_id.replace(\".\", \"_\")]\n",
    "        g.add((child_uri, SKOS.broader, parent_uri))\n",
    "        g.add((parent_uri, SKOS.narrower, child_uri))\n",
    "\n",
    "# --- PART 5: Serialize the enriched graph.\n",
    "output_file_path = \"data/rdf/epd_rdf_instance_datastore_canonical_skos.ttl\"\n",
    "g.serialize(destination=output_file_path, format=\"turtle\")\n",
    "print(f\"Graph saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIN 276\n",
    "\n",
    "import json\n",
    "import csv\n",
    "from rdflib import Graph, Namespace, Literal, URIRef\n",
    "from rdflib.namespace import RDF, SKOS\n",
    "\n",
    "# Define Namespaces.\n",
    "ILCD = Namespace(\"https://example.org/ilcd/\")\n",
    "DIN = Namespace(\"https://example.org/din276/\")\n",
    "LINKML_UUID = URIRef(\"https://w3id.org/linkml/UUIDType\")\n",
    "\n",
    "# Read the EPD Turtle file and bind the DIN prefix.\n",
    "g = Graph()\n",
    "g.parse(\"data/rdf/epd_rdf_instance_datastore_canonical_skos.ttl\", format=\"turtle\")\n",
    "g.bind(\"din\", DIN)\n",
    "\n",
    "# Build dictionary from EPD JSON: epd_id -> epd_uuid.\n",
    "epd_id_to_uuid = {}\n",
    "with open(\"../data/pipeline2/json/edited_epds.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line.strip())\n",
    "        epd_id = str(data[\"id\"])\n",
    "        epd_uuid = data[\"uuid\"]\n",
    "        epd_id_to_uuid[epd_id] = epd_uuid\n",
    "\n",
    "# Build dictionary from DIN 276 JSON: custom_id -> [cost_group_codes].\n",
    "din_cost_groups = {}\n",
    "used_codes = set()  # collect all cost codes referenced in DIN JSON\n",
    "with open(\"../data/pipeline2/json/openai/batch_67d5a00f7f2c8190a0e2cdc3cf04382b_output_+EPDNorge.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line.strip())\n",
    "        custom_id = str(data[\"custom_id\"])\n",
    "        content_str = data[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        parsed = json.loads(content_str)\n",
    "        cost_codes = parsed[\"cost_group_codes\"]\n",
    "        din_cost_groups[custom_id] = cost_codes\n",
    "        used_codes.update(cost_codes)\n",
    "\n",
    "# Build a SKOS vocabulary for DIN 276 cost groups from CSV,\n",
    "# but only add rows whose cost code (Nr) is in used_codes.\n",
    "# Also add the cost group number using skos:notation.\n",
    "din_code_to_concept = {}\n",
    "with open(\"../data/pipeline2/csv/din276_concrete_sub.csv\", \"r\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)  # Expected columns: Nr, Cost group (CG), Notes\n",
    "    for row in reader:\n",
    "        nr = row[\"Nr\"].strip()  # e.g., \"310\"\n",
    "        # Only include cost groups referenced in DIN JSON.\n",
    "        if nr not in used_codes:\n",
    "            continue\n",
    "\n",
    "        label_en = row[\"Cost group (CG)\"].strip()  # e.g., \"Trenchwork/Earthworks\"\n",
    "        notes_en = row[\"Notes\"].strip()            # e.g., description\n",
    "\n",
    "        # Create a SKOS concept URI, e.g., DIN:costgroup_310.\n",
    "        concept_uri = DIN[f\"costgroup_{nr}\"]\n",
    "        g.add((concept_uri, RDF.type, SKOS.Concept))\n",
    "        g.add((concept_uri, SKOS.prefLabel, Literal(label_en, lang=\"en\")))\n",
    "        g.add((concept_uri, SKOS.note, Literal(notes_en, lang=\"en\")))\n",
    "        # Add the cost group number as a notation.\n",
    "        g.add((concept_uri, SKOS.notation, Literal(nr)))\n",
    "        din_code_to_concept[nr] = concept_uri\n",
    "\n",
    "# For each DIN 276 entry in the DIN JSON, find the corresponding top EPD node and link it\n",
    "# to the SKOS concept(s) for the cost codes.\n",
    "for custom_id, cost_codes in din_cost_groups.items():\n",
    "    if custom_id not in epd_id_to_uuid:\n",
    "        print(f\"No EPD found for custom_id: {custom_id}\")\n",
    "        continue\n",
    "    epd_uuid = epd_id_to_uuid[custom_id]\n",
    "    uuid_literal = Literal(epd_uuid, datatype=LINKML_UUID)\n",
    "    \n",
    "    # Walk from dataSetInformation up to the top EPD node.\n",
    "    found_epd = None\n",
    "    for dsi_node in g.subjects(predicate=ILCD.UUID, object=uuid_literal):\n",
    "        for pi_node in g.subjects(predicate=ILCD.dataSetInformation, object=dsi_node):\n",
    "            for epd_node in g.subjects(predicate=ILCD.processInformation, object=pi_node):\n",
    "                found_epd = epd_node\n",
    "                break\n",
    "            if found_epd:\n",
    "                break\n",
    "        if found_epd:\n",
    "            break\n",
    "\n",
    "    if not found_epd:\n",
    "        print(f\"No top EPD node found for UUID: {epd_uuid}\")\n",
    "        continue\n",
    "\n",
    "    # Link each cost code that is present.\n",
    "    for code in cost_codes:\n",
    "        concept_uri = din_code_to_concept.get(code)\n",
    "        if concept_uri is None:\n",
    "            print(f\"Cost code {code} not in CSV vocabulary; skipping.\")\n",
    "            continue\n",
    "        # Link using the DIN namespace property.\n",
    "        g.add((found_epd, DIN.hasDIN276CostGroup, concept_uri))\n",
    "    print(f\"Linked EPD node {found_epd} to cost codes: {cost_codes}\")\n",
    "\n",
    "# Add broader/narrower relationships among cost group concepts.\n",
    "for code_parent, concept_parent in din_code_to_concept.items():\n",
    "    if not code_parent.endswith(\"0\"):\n",
    "        continue\n",
    "    for code_child, concept_child in din_code_to_concept.items():\n",
    "        if code_child == code_parent:\n",
    "            continue\n",
    "        if code_child.startswith(code_parent[:2]) and not code_child.endswith(\"0\"):\n",
    "            g.add((concept_child, SKOS.broader, concept_parent))\n",
    "            g.add((concept_parent, SKOS.narrower, concept_child))\n",
    "\n",
    "# --- Model DIN 276 as a SKOS ConceptScheme using the DIN namespace.\n",
    "din_scheme = DIN[\"DIN276\"]\n",
    "g.add((din_scheme, RDF.type, SKOS.ConceptScheme))\n",
    "g.add((din_scheme, SKOS.prefLabel, Literal(\"DIN 276\", lang=\"en\")))\n",
    "g.add((din_scheme, SKOS.note, Literal(\"DIN 276:2018-12 – Cost planning in building\", lang=\"en\")))\n",
    "\n",
    "# For each DIN cost group concept, assert that it is part of the DIN 276 scheme.\n",
    "for concept in din_code_to_concept.values():\n",
    "    g.add((concept, SKOS.inScheme, din_scheme))\n",
    "\n",
    "# Serialize updated RDF.\n",
    "output_file_path = \"data/rdf/epd_rdf_instance_datastore_canonical_skos_din.ttl\"\n",
    "g.serialize(destination=output_file_path, format=\"turtle\")\n",
    "print(\"Done! Updated RDF with DIN 276 SKOS cost groups and hierarchical relationships.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BKI\n",
    "\n",
    "from rdflib import Graph, Namespace, URIRef, Literal, BNode\n",
    "from rdflib.namespace import RDF, RDFS, SKOS, XSD\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Namespaces used in your KG and for BKI data\n",
    "BKI = Namespace(\"https://example.org/bki/\")  # for your BKI elements\n",
    "DIN = Namespace(\"https://example.org/din276/\")  # existing DIN cost groups\n",
    "\n",
    "# Load your existing SKOS-based DIN 276 cost-group ontology\n",
    "# The triple store includes definitions like din:costgroup_322, etc.\n",
    "\n",
    "g = Graph()\n",
    "g.parse(\"data/rdf/epd_rdf_instance_datastore_canonical_skos_din.ttl\", format=\"turtle\")\n",
    "\n",
    "# Register custom namespaces in the graph for clarity\n",
    "g.bind(\"bki\", BKI)\n",
    "g.bind(\"din\", DIN)\n",
    "\n",
    "def integrate_bki_xml(g, xml_file_path):\n",
    "    \"\"\"\n",
    "    Parse a BKI XML file (level 2 or 3),\n",
    "    create RDF triples for each <element> that has a component containing 'transportbeton',\n",
    "    and link them to the DIN 276 cost groups in the existing graph.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Starting graph size: {len(g)} triples.\")\n",
    "    \n",
    "    # Register namespace to avoid parser warnings\n",
    "    ET.register_namespace('', 'https://www.bauteileditor.de')\n",
    "    \n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Define XML namespaces\n",
    "    ns = {'elca': 'https://www.bauteileditor.de'}\n",
    "\n",
    "    # Iterate over all element nodes\n",
    "    for element_node in root.findall('.//elca:element', ns):\n",
    "        component_nodes = element_node.findall('.//elca:component', ns)\n",
    "        # Check if any component contains 'transportbeton'\n",
    "        has_transportbeton = any('transportbeton' in comp.get('processConfigName', '').lower() for comp in component_nodes)\n",
    "        \n",
    "        if not has_transportbeton:\n",
    "            continue  # Skip elements without 'transportbeton' in components\n",
    "\n",
    "        # Extract the element's UUID and DIN code\n",
    "        element_uuid = element_node.get('uuid', 'unknown').replace('-', '')\n",
    "        din_code = element_node.get('din276Code', 'unknown')\n",
    "        element_uri = BKI[f'element_{element_uuid}']\n",
    "\n",
    "        # Add RDF triples to the graph\n",
    "        g.add((element_uri, RDF.type, BKI.BKIElement))\n",
    "        costgroup_uri = DIN[f'costgroup_{din_code}']\n",
    "        g.add((element_uri, DIN.hasDIN276CostGroup, costgroup_uri))\n",
    "\n",
    "        # Extract name and description from <elementInfo>\n",
    "        elem_info = element_node.find('.//elca:elementInfo', ns)\n",
    "        if elem_info is not None:\n",
    "            name_node = elem_info.find('elca:name', ns)\n",
    "            desc_node = elem_info.find('elca:description', ns)\n",
    "            \n",
    "            if name_node is not None and name_node.text:\n",
    "                g.add((element_uri, BKI.name, Literal(name_node.text.strip(), datatype=XSD.string)))\n",
    "            if desc_node is not None and desc_node.text:\n",
    "                g.add((element_uri, BKI.description, Literal(desc_node.text.strip(), datatype=XSD.string)))\n",
    "\n",
    "        # Extract details for each component layer\n",
    "        for comp_node in component_nodes:\n",
    "            process_config_uuid = comp_node.get('processConfigUuid', '').replace('-', '')\n",
    "            layer_size = comp_node.get('layerSize', '')\n",
    "            layer_size_str = layer_size.replace('.', '').ljust(3, '0')[:3]  # e.g., \"0.15\" -> \"015\"\n",
    "            layer_uri = BKI[f'layer_{process_config_uuid}_{layer_size_str}']\n",
    "\n",
    "            process_config_name = comp_node.get('processConfigName', '')\n",
    "            life_time = comp_node.get('lifeTime', '')\n",
    "            \n",
    "            g.add((layer_uri, RDF.type, BKI.Layer))\n",
    "            g.add((element_uri, BKI.hasLayer, layer_uri))\n",
    "            g.add((layer_uri, BKI.processConfigName, Literal(process_config_name, datatype=XSD.string)))\n",
    "            \n",
    "            if life_time.isdigit():\n",
    "                g.add((layer_uri, BKI.lifeTime, Literal(int(life_time), datatype=XSD.integer)))\n",
    "            else:\n",
    "                g.add((layer_uri, BKI.lifeTime, Literal(life_time, datatype=XSD.string)))\n",
    "            \n",
    "            try:\n",
    "                layer_size_float = float(layer_size)\n",
    "                g.add((layer_uri, BKI.layerSize, Literal(layer_size_float, datatype=XSD.float)))\n",
    "            except ValueError:\n",
    "                g.add((layer_uri, BKI.layerSize, Literal(layer_size, datatype=XSD.string)))\n",
    "\n",
    "    print(f\"Integrated data from {xml_file_path}. Current graph size: {len(g)} triples.\")\n",
    "\n",
    "integrate_bki_xml(g, \"../data/pipeline2/xml/BKI_Bauteilaufbauten_ 2_Ebene_DIN_276.xml\")\n",
    "integrate_bki_xml(g, \"../data/pipeline2/xml/BKI_Bauteilaufbauten_ 3_Ebene_DIN_276.xml\")\n",
    "\n",
    "# Serialize updated RDF\n",
    "g.serialize(destination=\"data/rdf/epd_rdf_instance_datastore_canonical_skos_din_bki.ttl\", format=\"turtle\")\n",
    "print(\"Done! Updated RDF with BKI elements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_bki_by_costgroups(g, costgroup_notations):\n",
    "    \"\"\"\n",
    "    Returns all BKI elements linked to a DIN 276 cost group whose notation \n",
    "    is in costgroup_notations, and also whose layers contain 'beton'.\n",
    "    \"\"\"\n",
    "    # Build the string list for SPARQL: e.g. (\"322\",\"330\",\"340\")\n",
    "    notations_list = '\",\"'.join(costgroup_notations)  # e.g. '322\",\"330\",\"340'\n",
    "    \n",
    "    # For the SPARQL, we can do: FILTER(?notation IN (\"322\",\"330\",\"340\"))\n",
    "    # We'll avoid backslashes around quotes by using triple single-quotes:\n",
    "    \n",
    "    query_str = f'''\n",
    "PREFIX bki: <{BKI}>\n",
    "PREFIX din: <{DIN}>\n",
    "PREFIX skos: <{SKOS}>\n",
    "\n",
    "SELECT ?element ?name ?description ?layerName ?layerLife ?layerSize ?notation\n",
    "WHERE {{\n",
    "  ?element a bki:BKIElement ;\n",
    "           din:hasDIN276CostGroup ?cg ;\n",
    "           bki:name ?name ;\n",
    "           bki:description ?description ;\n",
    "           bki:hasLayer ?layer .\n",
    "  \n",
    "  ?layer bki:processConfigName ?layerName .\n",
    "  OPTIONAL {{ ?layer bki:lifeTime ?layerLife . }}\n",
    "  OPTIONAL {{ ?layer bki:layerSize ?layerSize . }}\n",
    "  \n",
    "  ?cg skos:notation ?notation .\n",
    "  \n",
    "  FILTER(?notation IN (\"{notations_list}\"))\n",
    "  FILTER regex(?layerName, \"beton\", \"i\")\n",
    "}}\n",
    "ORDER BY ?element\n",
    "'''\n",
    "    \n",
    "    results = g.query(query_str)\n",
    "    print(query_str)\n",
    "    \n",
    "    # Convert results to a Python structure\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Dictionary keyed by element URI\n",
    "    output = defaultdict(lambda: {\n",
    "        'name': '',\n",
    "        'description': '',\n",
    "        'notation': '',\n",
    "        'layers': []\n",
    "    })\n",
    "    \n",
    "    for row in results:\n",
    "        elem_uri = str(row.element)\n",
    "        \n",
    "        if not output[elem_uri]['name']:\n",
    "            output[elem_uri]['name'] = str(row.name)\n",
    "        if not output[elem_uri]['description']:\n",
    "            output[elem_uri]['description'] = str(row.description)\n",
    "        if not output[elem_uri]['notation']:\n",
    "            output[elem_uri]['notation'] = str(row.notation)\n",
    "        \n",
    "        # Layers\n",
    "        layer_data = {\n",
    "            'processConfigName': str(row.layerName),\n",
    "            'lifeTime': str(row.layerLife) if row.layerLife else None,\n",
    "            'layerSize': str(row.layerSize) if row.layerSize else None\n",
    "        }\n",
    "        output[elem_uri]['layers'].append(layer_data)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "result = query_bki_by_costgroups(g, [\"322\",\"330\",\"331\",\"340\"])\n",
    "for elem_uri, info in result.items():\n",
    "    print(f\"Element: {elem_uri}\")\n",
    "    print(f\"  DIN 276 group notation: {info['notation']}\")\n",
    "    print(f\"  Name: {info['name']}\")\n",
    "    print(f\"  Description: {info['description']}\")\n",
    "    print(\"  Layers containing 'beton':\")\n",
    "    for layer in info['layers']:\n",
    "        print(f\"    * processConfigName: {layer['processConfigName']}\")\n",
    "        print(f\"      lifeTime: {layer['lifeTime']}\")\n",
    "        print(f\"      layerSize: {layer['layerSize']}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import rdflib\n",
    "\n",
    "def extract_related_triples(graph, node, visited, subgraph):\n",
    "    \"\"\"\n",
    "    Recursively extracts all triples starting from 'node' into 'subgraph'.\n",
    "    Prevents cycles using the 'visited' set.\n",
    "    \"\"\"\n",
    "    if node in visited:\n",
    "        return\n",
    "    visited.add(node)\n",
    "    for predicate, obj in graph.predicate_objects(subject=node):\n",
    "        subgraph.add((node, predicate, obj))\n",
    "        # Traverse further if the object is a URIRef or Blank Node.\n",
    "        if isinstance(obj, (rdflib.URIRef, rdflib.BNode)):\n",
    "            extract_related_triples(graph, obj, visited, subgraph)\n",
    "\n",
    "def extract_entity_subgraph(file_path, entity_uri):\n",
    "    \"\"\"\n",
    "    - Loads a Turtle file.\n",
    "    - Extracts the subgraph related to the given 'entity_uri' (following outgoing triples).\n",
    "    - Returns the extracted subgraph serialized in Turtle format.\n",
    "    \"\"\"\n",
    "    g = rdflib.Graph()\n",
    "    g.parse(file_path, format=\"turtle\")\n",
    "    \n",
    "    target_node = rdflib.URIRef(entity_uri)\n",
    "    if target_node not in set(g.subjects()):\n",
    "        print(f\"Entity {entity_uri} not found as a subject in the graph.\")\n",
    "        return None\n",
    "    \n",
    "    # Create a new graph for the subgraph and use the original's namespace manager.\n",
    "    subgraph = rdflib.Graph()\n",
    "    subgraph.namespace_manager = g.namespace_manager\n",
    "    \n",
    "    visited = set()\n",
    "    extract_related_triples(g, target_node, visited, subgraph)\n",
    "    \n",
    "    turtle_output = subgraph.serialize(format=\"turtle\")\n",
    "    # Decode if the output is in bytes.\n",
    "    if isinstance(turtle_output, bytes):\n",
    "        turtle_output = turtle_output.decode(\"utf-8\")\n",
    "    return turtle_output\n",
    "\n",
    "# Example usage in Jupyter Notebook:\n",
    "entity_turtle = extract_entity_subgraph(\n",
    "    'data/rdf/epd_rdf_instance_datastore_canonical_skos_din.ttl',\n",
    "    'https://example.org/ilcd/0643c04910464d518ddc760f32642a72'\n",
    ")\n",
    "print(entity_turtle)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
