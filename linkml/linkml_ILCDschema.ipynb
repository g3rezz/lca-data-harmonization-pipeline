{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate only the `processInformation` part of the Schema and JSON Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "sd_yaml_schema = \"../linkml/data/yaml/linkml_shared_definitions.yaml\"\n",
    "sd_python_file = \"../linkml/data/py/linkml_shared_definitions.py\"\n",
    "\n",
    "generator = PythonGenerator(schema=sd_yaml_schema)\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "\n",
    "with open(sd_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {sd_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "\n",
    "pi_yaml_schema = \"../linkml/data/yaml/linkml_processInformation_schema.yaml\"\n",
    "pi_python_file = \"../linkml/data/py/linkml_processInformation_schema.py\"\n",
    "\n",
    "\n",
    "generator = PythonGenerator(\n",
    "    schema=pi_yaml_schema,\n",
    "    gen_slots=True,\n",
    "    gen_classvars=True,\n",
    "    mergeimports=True,\n",
    "    metadata=True,\n",
    "    genmeta=False,\n",
    ")\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "with open(pi_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {pi_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance Generation and Loading processInformation EPD YAML\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "from data.py.linkml_processInformation_schema import ProcessInformation\n",
    "from linkml.validator import Validator\n",
    "from linkml_runtime.loaders.yaml_loader import YAMLLoader\n",
    "\n",
    "\n",
    "def odict_to_dict(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert an OrderedDict (or list/dict containing OrderedDicts) to a standard dict.\n",
    "\n",
    "    Parameters:\n",
    "        obj: An OrderedDict, list, or dict potentially containing OrderedDicts.\n",
    "\n",
    "    Returns:\n",
    "        A new structure where all OrderedDict instances are replaced with standard dicts.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, OrderedDict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [odict_to_dict(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def build_minimal_epd_structure(data):\n",
    "    \"\"\"\n",
    "    Build a minimal nested dictionary according to our minimal schema.\n",
    "    This function assumes that the JSON instance already contains a \"processInformation\" key.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): A dictionary loaded from a JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary structured with a top-level 'processInformation' key.\n",
    "    \"\"\"\n",
    "    instance_dict = {\"processInformation\": data.get(\"processInformation\", {})}\n",
    "    return instance_dict\n",
    "\n",
    "\n",
    "def process_instance(input_path, schema_path, output_yaml_path):\n",
    "    \"\"\"\n",
    "    Process an instance by loading a minimal JSON instance, converting it, dumping it to YAML,\n",
    "    validating it against a schema, and loading it with YAMLLoader.\n",
    "\n",
    "    Steps:\n",
    "      1. Load a minimal JSON instance containing only the processInformation section.\n",
    "      2. Convert OrderedDicts to dicts.\n",
    "      3. Wrap the data in a top-level ProcessInformation structure.\n",
    "      4. Dump the resulting instance to a YAML file.\n",
    "      5. Validate the instance using a schema validator.\n",
    "      6. Load the YAML instance with YAMLLoader.\n",
    "\n",
    "    Parameters:\n",
    "        input_path (str): Path to the input JSON file.\n",
    "        schema_path (str): Path to the YAML schema file.\n",
    "        output_yaml_path (str): Path where the output YAML file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during validation or instance loading.\n",
    "    \"\"\"\n",
    "    # Load the minimal JSON instance (containing only processInformation)\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    data = odict_to_dict(data)\n",
    "\n",
    "    # Build the nested instance structure.\n",
    "    instance_data = build_minimal_epd_structure(data)\n",
    "\n",
    "    # Dump to YAML\n",
    "    with open(output_yaml_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        yaml.dump(instance_data, out, sort_keys=False, allow_unicode=True)\n",
    "    print(f\"processInformation EPD YAML instance written to: {output_yaml_path}\")\n",
    "\n",
    "    # Validate using the Validator\n",
    "    try:\n",
    "        validator = Validator(schema_path, strict=False)\n",
    "        report = validator.validate(instance_data, \"ProcessInformation\")\n",
    "        if report.results:\n",
    "            print(\"Validation errors:\")\n",
    "            for result in report.results:\n",
    "                print(result.message)\n",
    "        else:\n",
    "            print(\"The ProcessInformation instance is valid!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during validation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Load the YAML instance using YAMLLoader\n",
    "    try:\n",
    "        loader = YAMLLoader()\n",
    "        # If the instance is wrapped in a \"processInformation\" key, unwrap it.\n",
    "        if \"processInformation\" in instance_data:\n",
    "            instance_data = instance_data[\"processInformation\"]\n",
    "        loader.load(instance_data, target_class=ProcessInformation)\n",
    "        print(\"Instance loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading via YAMLLoader:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Usage\n",
    "input_file = \"../linkml/data/json/instance_processInformation.json\"\n",
    "schema_file = \"../linkml/data/yaml/linkml_processInformation_schema.yaml\"\n",
    "output_yaml = \"../linkml/data/yaml/linkml_processInformation_instance.yaml\"\n",
    "\n",
    "process_instance(input_file, schema_file, output_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and RDF (JSON-LD, Turtle) Generation processInformation EPD YAML Instance\n",
    "\n",
    "import yaml\n",
    "from linkml_runtime.dumpers import RDFLibDumper\n",
    "from linkml_runtime.loaders import YAMLLoader\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "from data.py.linkml_processInformation_schema import ProcessInformation\n",
    "\n",
    "\n",
    "def generate_rdf(\n",
    "    schema_path: str,\n",
    "    instance_path: str,\n",
    "    output_jsonld_path: str,\n",
    "    output_turtle_path: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load a validated LinkML YAML instance, create a SchemaView, and dump the instance to RDF in JSON-LD and Turtle formats.\n",
    "\n",
    "    Steps:\n",
    "        1. Load the instance from a YAML file.\n",
    "        2. Create a SchemaView from the schema file.\n",
    "        3. Generate RDF in JSON-LD and Turtle formats.\n",
    "        4. Write the RDF to files.\n",
    "\n",
    "    Parameters:\n",
    "        schema_path (str): Path to the LinkML YAML schema file.\n",
    "        instance_path (str): Path to the validated LinkML YAML instance file.\n",
    "        output_jsonld_path (str): Path where the output JSON-LD file will be written.\n",
    "        output_turtle_path (str): Path where the output Turtle file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during instance loading, SchemaView creation, or RDF generation.\n",
    "    \"\"\"\n",
    "    # Load instance from file\n",
    "    try:\n",
    "        with open(instance_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            loaded_data = yaml.safe_load(f)  # This is now a Python dict\n",
    "        # Extract \"processInformation\"\n",
    "        if \"processInformation\" in loaded_data:\n",
    "            loaded_data = loaded_data[\"processInformation\"]\n",
    "        # Feed loaded_data dictionary into the loader\n",
    "        instance_obj = YAMLLoader().load(loaded_data, target_class=ProcessInformation)\n",
    "        print(f\"Instance loaded from: {instance_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Create SchemaView\n",
    "    try:\n",
    "        sv = SchemaView(schema_path)\n",
    "        print(\"SchemaView created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during SchemaView creation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Generate and write RDF (JSON-LD, Turtle)\n",
    "    try:\n",
    "        dumper = RDFLibDumper()\n",
    "        # JSON‑LD format\n",
    "        # rdf_jsonld = dumper.dumps(instance_obj, schemaview=sv, fmt=\"json-ld\")\n",
    "        # with open(output_jsonld_path, \"w\", encoding=\"utf-8\") as rdf_file:\n",
    "        #     rdf_file.write(rdf_jsonld)\n",
    "        # print(f\"RDF (JSON‑LD) successfully written to: {output_jsonld_path}\")\n",
    "        # Turtle format\n",
    "        rdf_turtle = dumper.dumps(instance_obj, schemaview=sv, fmt=\"turtle\")\n",
    "        with open(output_turtle_path, \"w\", encoding=\"utf-8\") as ttl_file:\n",
    "            ttl_file.write(rdf_turtle)\n",
    "        print(f\"RDF (Turtle) successfully written to: {output_turtle_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during RDF generation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "schema_path = \"../linkml/data/yaml/linkml_processInformation_schema.yaml\"\n",
    "instance_path = \"../linkml/data/yaml/linkml_processInformation_instance.yaml\"\n",
    "output_jsonld_path = \"../linkml/data/rdf/linkml_processInformation_instance.jsonld\"\n",
    "output_turtle_path = \"../linkml/data/rdf/linkml_processInformation_instance.ttl\"\n",
    "\n",
    "generate_rdf(schema_path, instance_path, output_jsonld_path, output_turtle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate only the `modellingAndValidation` part of the Schema and JSON Instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "sd_yaml_schema = \"../linkml/data/yaml/linkml_shared_definitions.yaml\"\n",
    "sd_python_file = \"../linkml/data/py/linkml_shared_definitions.py\"\n",
    "\n",
    "generator = PythonGenerator(schema=sd_yaml_schema)\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "\n",
    "with open(sd_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {sd_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "mav_yaml_schema = \"../linkml/data/yaml/linkml_modellingAndValidation_schema.yaml\"\n",
    "mav_python_file = \"../linkml/data/py/linkml_modellingAndValidation_schema.py\"\n",
    "\n",
    "\n",
    "generator = PythonGenerator(schema=mav_yaml_schema)\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "with open(mav_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {mav_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance Generation and Loading modellingAndValidation EPD YAML\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "from data.py.linkml_modellingAndValidation_schema import ModellingAndValidation\n",
    "from linkml.validator import Validator\n",
    "from linkml_runtime.loaders.yaml_loader import YAMLLoader\n",
    "\n",
    "\n",
    "def odict_to_dict(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert an OrderedDict (or list/dict containing OrderedDicts) to a standard dict.\n",
    "\n",
    "    Parameters:\n",
    "        obj: An OrderedDict, list, or dict potentially containing OrderedDicts.\n",
    "\n",
    "    Returns:\n",
    "        A new structure where all OrderedDict instances are replaced with standard dicts.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, OrderedDict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [odict_to_dict(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def build_minimal_epd_structure(data):\n",
    "    \"\"\"\n",
    "    Build a minimal nested dictionary according to our minimal schema.\n",
    "    This function assumes that the JSON instance already contains a \"modellingAndValidation\" key.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): A dictionary loaded from a JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary structured with a top-level 'modellingAndValidation' key.\n",
    "    \"\"\"\n",
    "    instance_dict = {\"modellingAndValidation\": data.get(\"modellingAndValidation\", {})}\n",
    "    return instance_dict\n",
    "\n",
    "\n",
    "def process_instance(input_path, schema_path, output_yaml_path):\n",
    "    \"\"\"\n",
    "    Process an instance by loading a minimal JSON instance, converting it, dumping it to YAML,\n",
    "    validating it against a schema, and loading it with YAMLLoader.\n",
    "\n",
    "    Steps:\n",
    "      1. Load a minimal JSON instance containing only the modellingAndValidation section.\n",
    "      2. Convert OrderedDicts to dicts.\n",
    "      3. Wrap the data in a top-level ModellingAndValidation structure.\n",
    "      4. Dump the resulting instance to a YAML file.\n",
    "      5. Validate the instance using a schema validator.\n",
    "      6. Load the YAML instance with YAMLLoader.\n",
    "\n",
    "    Parameters:\n",
    "        input_path (str): Path to the input JSON file.\n",
    "        schema_path (str): Path to the YAML schema file.\n",
    "        output_yaml_path (str): Path where the output YAML file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during validation or instance loading.\n",
    "    \"\"\"\n",
    "    # Load the minimal JSON instance (containing only modellingAndValidation)\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    data = odict_to_dict(data)\n",
    "\n",
    "    # Build the nested instance structure.\n",
    "    instance_data = build_minimal_epd_structure(data)\n",
    "\n",
    "    # Dump to YAML\n",
    "    with open(output_yaml_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        yaml.dump(instance_data, out, sort_keys=False, allow_unicode=True)\n",
    "    print(f\"modellingAndValidation EPD YAML instance written to: {output_yaml_path}\")\n",
    "\n",
    "    # Validate using the Validator\n",
    "    try:\n",
    "        validator = Validator(schema_path, strict=False)\n",
    "        report = validator.validate(instance_data, \"ModellingAndValidation\")\n",
    "        if report.results:\n",
    "            print(\"Validation errors:\")\n",
    "            for result in report.results:\n",
    "                print(result.message)\n",
    "        else:\n",
    "            print(\"The ModellingAndValidation instance is valid!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during validation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Load the YAML instance using YAMLLoader\n",
    "    try:\n",
    "        loader = YAMLLoader()\n",
    "        # If the instance is wrapped in a \"modellingAndValidation\" key, unwrap it.\n",
    "        if \"modellingAndValidation\" in instance_data:\n",
    "            instance_data = instance_data[\"modellingAndValidation\"]\n",
    "        loader.load(instance_data, target_class=ModellingAndValidation)\n",
    "        print(\"Instance loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading via YAMLLoader:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Usage\n",
    "input_file = \"../linkml/data/json/instance_modellingAndValidation.json\"\n",
    "schema_file = \"../linkml/data/yaml/linkml_modellingAndValidation_schema.yaml\"\n",
    "output_yaml = \"../linkml/data/yaml/linkml_modellingAndValidation_instance.yaml\"\n",
    "\n",
    "process_instance(input_file, schema_file, output_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and RDF (JSON-LD, Turtle) Generation modellingAndValidation EPD YAML Instance\n",
    "\n",
    "import yaml\n",
    "from linkml_runtime.dumpers import RDFLibDumper\n",
    "from linkml_runtime.loaders import YAMLLoader\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "from data.py.linkml_modellingAndValidation_schema import ModellingAndValidation\n",
    "\n",
    "\n",
    "def generate_rdf(\n",
    "    schema_path: str,\n",
    "    instance_path: str,\n",
    "    output_jsonld_path: str,\n",
    "    output_turtle_path: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load a validated LinkML YAML instance, create a SchemaView, and dump the instance to RDF in JSON-LD and Turtle formats.\n",
    "\n",
    "    Steps:\n",
    "        1. Load the instance from a YAML file.\n",
    "        2. Create a SchemaView from the schema file.\n",
    "        3. Generate RDF in JSON-LD and Turtle formats.\n",
    "        4. Write the RDF to files.\n",
    "\n",
    "    Parameters:\n",
    "        schema_path (str): Path to the LinkML YAML schema file.\n",
    "        instance_path (str): Path to the validated LinkML YAML instance file.\n",
    "        output_jsonld_path (str): Path where the output JSON-LD file will be written.\n",
    "        output_turtle_path (str): Path where the output Turtle file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during instance loading, SchemaView creation, or RDF generation.\n",
    "    \"\"\"\n",
    "    # Load instance from file\n",
    "    try:\n",
    "        with open(instance_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            loaded_data = yaml.safe_load(f)  # This is now a Python dict\n",
    "        # Extract \"modellingAndValidation\"\n",
    "        if \"modellingAndValidation\" in loaded_data:\n",
    "            loaded_data = loaded_data[\"modellingAndValidation\"]\n",
    "        # Feed loaded_data dictionary into the loader\n",
    "        instance_obj = YAMLLoader().load(\n",
    "            loaded_data, target_class=ModellingAndValidation\n",
    "        )\n",
    "        print(f\"Instance loaded from: {instance_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Create SchemaView\n",
    "    try:\n",
    "        sv = SchemaView(schema_path)\n",
    "        print(\"SchemaView created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during SchemaView creation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Generate and write RDF (JSON-LD, Turtle)\n",
    "    try:\n",
    "        dumper = RDFLibDumper()\n",
    "        # # JSON‑LD format\n",
    "        # rdf_jsonld = dumper.dumps(instance_obj, schemaview=sv, fmt=\"json-ld\")\n",
    "        # with open(output_jsonld_path, \"w\", encoding=\"utf-8\") as rdf_file:\n",
    "        #     rdf_file.write(rdf_jsonld)\n",
    "        # print(f\"RDF (JSON‑LD) successfully written to: {output_jsonld_path}\")\n",
    "        # Turtle format\n",
    "        rdf_turtle = dumper.dumps(instance_obj, schemaview=sv, fmt=\"turtle\")\n",
    "        with open(output_turtle_path, \"w\", encoding=\"utf-8\") as ttl_file:\n",
    "            ttl_file.write(rdf_turtle)\n",
    "        print(f\"RDF (Turtle) successfully written to: {output_turtle_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during RDF generation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "schema_path = \"../linkml/data/yaml/linkml_modellingAndValidation_schema.yaml\"\n",
    "instance_path = \"../linkml/data/yaml/linkml_modellingAndValidation_instance.yaml\"\n",
    "output_jsonld_path = \"../linkml/data/rdf/linkml_modellingAndValidation_instance.jsonld\"\n",
    "output_turtle_path = \"../linkml/data/rdf/linkml_modellingAndValidation_instance.ttl\"\n",
    "\n",
    "generate_rdf(schema_path, instance_path, output_jsonld_path, output_turtle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate only the `administrativeInformation` part of the Schema and JSON Instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "sd_yaml_schema = \"../linkml/data/yaml/linkml_shared_definitions.yaml\"\n",
    "sd_python_file = \"../linkml/data/py/linkml_shared_definitions.py\"\n",
    "\n",
    "generator = PythonGenerator(schema=sd_yaml_schema)\n",
    "python_code = generator.serialize()\n",
    "\n",
    "with open(sd_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {sd_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "ai_yaml_schema = \"../linkml/data/yaml/linkml_administrativeInformation_schema.yaml\"\n",
    "ai_python_file = \"../linkml/data/py/linkml_administrativeInformation_schema.py\"\n",
    "\n",
    "generator = PythonGenerator(schema=ai_yaml_schema)\n",
    "python_code = generator.serialize()\n",
    "\n",
    "with open(ai_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {ai_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance Generation and Loading administrativeInformation EPD YAML\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "from data.py.linkml_administrativeInformation_schema import AdministrativeInformation\n",
    "from linkml.validator import Validator\n",
    "from linkml_runtime.loaders.yaml_loader import YAMLLoader\n",
    "\n",
    "\n",
    "def odict_to_dict(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert an OrderedDict (or list/dict containing OrderedDicts) to a standard dict.\n",
    "\n",
    "    Parameters:\n",
    "        obj: An OrderedDict, list, or dict potentially containing OrderedDicts.\n",
    "\n",
    "    Returns:\n",
    "        A new structure where all OrderedDict instances are replaced with standard dicts.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, OrderedDict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [odict_to_dict(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def build_minimal_epd_structure(data):\n",
    "    \"\"\"\n",
    "    Build a minimal nested dictionary according to our minimal schema.\n",
    "    This function assumes that the JSON instance already contains a \"administrativeInformation\" key.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): A dictionary loaded from a JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary structured with a top-level 'administrativeInformation' key.\n",
    "    \"\"\"\n",
    "    instance_dict = {\n",
    "        \"administrativeInformation\": data.get(\"administrativeInformation\", {})\n",
    "    }\n",
    "    return instance_dict\n",
    "\n",
    "\n",
    "def process_instance(input_path, schema_path, output_yaml_path):\n",
    "    \"\"\"\n",
    "    Process an instance by loading a minimal JSON instance, converting it, dumping it to YAML,\n",
    "    validating it against a schema, and loading it with YAMLLoader.\n",
    "\n",
    "    Steps:\n",
    "      1. Load a minimal JSON instance containing only the administrativeInformation section.\n",
    "      2. Convert OrderedDicts to dicts.\n",
    "      3. Wrap the data in a top-level AdministrativeInformation structure.\n",
    "      4. Dump the resulting instance to a YAML file.\n",
    "      5. Validate the instance using a schema validator.\n",
    "      6. Load the YAML instance with YAMLLoader.\n",
    "\n",
    "    Parameters:\n",
    "        input_path (str): Path to the input JSON file.\n",
    "        schema_path (str): Path to the YAML schema file.\n",
    "        output_yaml_path (str): Path where the output YAML file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during validation or instance loading.\n",
    "    \"\"\"\n",
    "    # Load the minimal JSON instance (containing only administrativeInformation)\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    data = odict_to_dict(data)\n",
    "\n",
    "    # Build the nested instance structure.\n",
    "    instance_data = build_minimal_epd_structure(data)\n",
    "\n",
    "    # Dump to YAML\n",
    "    with open(output_yaml_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        yaml.dump(instance_data, out, sort_keys=False, allow_unicode=True)\n",
    "    print(f\"administrativeInformation EPD YAML instance written to: {output_yaml_path}\")\n",
    "\n",
    "    # Validate using the Validator\n",
    "    try:\n",
    "        validator = Validator(schema_path, strict=False)\n",
    "        report = validator.validate(instance_data, \"AdministrativeInformation\")\n",
    "        if report.results:\n",
    "            print(\"Validation errors:\")\n",
    "            for result in report.results:\n",
    "                print(result.message)\n",
    "        else:\n",
    "            print(\"The AdministrativeInformation instance is valid!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during validation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Load the YAML instance using YAMLLoader\n",
    "    try:\n",
    "        loader = YAMLLoader()\n",
    "        # If the instance is wrapped in a \"administrativeInformation\" key, unwrap it.\n",
    "        if \"administrativeInformation\" in instance_data:\n",
    "            instance_data = instance_data[\"administrativeInformation\"]\n",
    "        loader.load(instance_data, target_class=AdministrativeInformation)\n",
    "        print(\"Instance loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading via YAMLLoader:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Usage\n",
    "input_file = \"../linkml/data/json/instance_administrativeInformation.json\"\n",
    "schema_file = \"../linkml/data/yaml/linkml_administrativeInformation_schema.yaml\"\n",
    "output_yaml = \"../linkml/data/yaml/linkml_administrativeInformation_instance.yaml\"\n",
    "\n",
    "process_instance(input_file, schema_file, output_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and RDF (JSON-LD, Turtle) Generation administrativeInformation EPD YAML Instance\n",
    "\n",
    "import yaml\n",
    "from linkml_runtime.dumpers import RDFLibDumper\n",
    "from linkml_runtime.loaders import YAMLLoader\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "from data.py.linkml_administrativeInformation_schema import AdministrativeInformation\n",
    "\n",
    "\n",
    "def generate_rdf(\n",
    "    schema_path: str,\n",
    "    instance_path: str,\n",
    "    output_jsonld_path: str,\n",
    "    output_turtle_path: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load a validated LinkML YAML instance, create a SchemaView, and dump the instance to RDF in JSON-LD and Turtle formats.\n",
    "\n",
    "    Steps:\n",
    "        1. Load the instance from a YAML file.\n",
    "        2. Create a SchemaView from the schema file.\n",
    "        3. Generate RDF in JSON-LD and Turtle formats.\n",
    "        4. Write the RDF to files.\n",
    "\n",
    "    Parameters:\n",
    "        schema_path (str): Path to the LinkML YAML schema file.\n",
    "        instance_path (str): Path to the validated LinkML YAML instance file.\n",
    "        output_jsonld_path (str): Path where the output JSON-LD file will be written.\n",
    "        output_turtle_path (str): Path where the output Turtle file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during instance loading, SchemaView creation, or RDF generation.\n",
    "    \"\"\"\n",
    "    # Load instance from file\n",
    "    try:\n",
    "        with open(instance_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            loaded_data = yaml.safe_load(f)  # This is now a Python dict\n",
    "        # Extract \"administrativeInformation\"\n",
    "        if \"administrativeInformation\" in loaded_data:\n",
    "            loaded_data = loaded_data[\"administrativeInformation\"]\n",
    "        # Feed loaded_data dictionary into the loader\n",
    "        instance_obj = YAMLLoader().load(\n",
    "            loaded_data, target_class=AdministrativeInformation\n",
    "        )\n",
    "        print(f\"Instance loaded from: {instance_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Create SchemaView\n",
    "    try:\n",
    "        sv = SchemaView(schema_path)\n",
    "        print(\"SchemaView created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during SchemaView creation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Generate and write RDF (JSON-LD, Turtle)\n",
    "    try:\n",
    "        dumper = RDFLibDumper()\n",
    "        # JSON‑LD format\n",
    "        # rdf_jsonld = dumper.dumps(instance_obj, schemaview=sv, fmt=\"json-ld\")\n",
    "        # with open(output_jsonld_path, \"w\", encoding=\"utf-8\") as rdf_file:\n",
    "        #     rdf_file.write(rdf_jsonld)\n",
    "        # print(f\"RDF (JSON‑LD) successfully written to: {output_jsonld_path}\")\n",
    "        # Turtle format\n",
    "        rdf_turtle = dumper.dumps(instance_obj, schemaview=sv, fmt=\"turtle\")\n",
    "        with open(output_turtle_path, \"w\", encoding=\"utf-8\") as ttl_file:\n",
    "            ttl_file.write(rdf_turtle)\n",
    "        print(f\"RDF (Turtle) successfully written to: {output_turtle_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during RDF generation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "schema_path = \"../linkml/data/yaml/linkml_administrativeInformation_schema.yaml\"\n",
    "instance_path = \"../linkml/data/yaml/linkml_administrativeInformation_instance.yaml\"\n",
    "output_jsonld_path = (\n",
    "    \"../linkml/data/rdf/linkml_administrativeInformation_instance.jsonld\"\n",
    ")\n",
    "output_turtle_path = \"../linkml/data/rdf/linkml_administrativeInformation_instance.ttl\"\n",
    "\n",
    "generate_rdf(schema_path, instance_path, output_jsonld_path, output_turtle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate only the `exchanges` part of the Schema and JSON Instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "sd_yaml_schema = \"../linkml/data/yaml/linkml_shared_definitions.yaml\"\n",
    "sd_python_file = \"../linkml/data/py/linkml_shared_definitions.py\"\n",
    "\n",
    "generator = PythonGenerator(schema=sd_yaml_schema)\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "\n",
    "with open(sd_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {sd_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "ex_yaml_schema = \"../linkml/data/yaml/linkml_exchanges_schema.yaml\"\n",
    "ex_python_file = \"../linkml/data/py/linkml_exchanges_schema.py\"\n",
    "\n",
    "generator = PythonGenerator(schema=ex_yaml_schema)\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "with open(ex_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {ex_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance Generation and Loading exchanges EPD YAML\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "from data.py.linkml_exchanges_schema import Exchanges\n",
    "from linkml.validator import Validator\n",
    "from linkml_runtime.loaders.yaml_loader import YAMLLoader\n",
    "\n",
    "\n",
    "def odict_to_dict(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert an OrderedDict (or list/dict containing OrderedDicts) to a standard dict.\n",
    "\n",
    "    Parameters:\n",
    "        obj: An OrderedDict, list, or dict potentially containing OrderedDicts.\n",
    "\n",
    "    Returns:\n",
    "        A new structure where all OrderedDict instances are replaced with standard dicts.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, OrderedDict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [odict_to_dict(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def build_minimal_epd_structure(data):\n",
    "    \"\"\"\n",
    "    Build a minimal nested dictionary according to our minimal schema.\n",
    "    This function assumes that the JSON instance already contains a \"exchanges\" key.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): A dictionary loaded from a JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary structured with a top-level 'exchanges' key.\n",
    "    \"\"\"\n",
    "    instance_dict = {\"exchanges\": data.get(\"exchanges\", {})}\n",
    "    return instance_dict\n",
    "\n",
    "\n",
    "def process_instance(input_path, schema_path, output_yaml_path):\n",
    "    \"\"\"\n",
    "    Process an instance by loading a minimal JSON instance, converting it, dumping it to YAML,\n",
    "    validating it against a schema, and loading it with YAMLLoader.\n",
    "\n",
    "    Steps:\n",
    "      1. Load a minimal JSON instance containing only the exchanges section.\n",
    "      2. Convert OrderedDicts to dicts.\n",
    "      3. Wrap the data in a top-level Exchanges structure.\n",
    "      4. Dump the resulting instance to a YAML file.\n",
    "      5. Validate the instance using a schema validator.\n",
    "      6. Load the YAML instance with YAMLLoader.\n",
    "\n",
    "    Parameters:\n",
    "        input_path (str): Path to the input JSON file.\n",
    "        schema_path (str): Path to the YAML schema file.\n",
    "        output_yaml_path (str): Path where the output YAML file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during validation or instance loading.\n",
    "    \"\"\"\n",
    "    # Load the minimal JSON instance (containing only exchanges)\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    data = odict_to_dict(data)\n",
    "\n",
    "    # Build the nested instance structure.\n",
    "    instance_data = build_minimal_epd_structure(data)\n",
    "\n",
    "    # Dump to YAML\n",
    "    with open(output_yaml_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        yaml.dump(instance_data, out, sort_keys=False, allow_unicode=True)\n",
    "    print(f\"exchanges EPD YAML instance written to: {output_yaml_path}\")\n",
    "\n",
    "    # Validate using the Validator\n",
    "    try:\n",
    "        validator = Validator(schema_path, strict=False)\n",
    "        report = validator.validate(instance_data, \"Exchanges\")\n",
    "        if report.results:\n",
    "            print(\"Validation errors:\")\n",
    "            for result in report.results:\n",
    "                print(result.message)\n",
    "        else:\n",
    "            print(\"The Exchanges instance is valid!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during validation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Load the YAML instance using YAMLLoader\n",
    "    try:\n",
    "        loader = YAMLLoader()\n",
    "        # If the instance is wrapped in a \"exchanges\" key, unwrap it.\n",
    "        if \"exchanges\" in instance_data:\n",
    "            instance_data = instance_data[\"exchanges\"]\n",
    "        loader.load(instance_data, target_class=Exchanges)\n",
    "        print(\"Instance loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading via YAMLLoader:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Usage\n",
    "input_file = \"../linkml/data/json/instance_exchanges.json\"\n",
    "schema_file = \"../linkml/data/yaml/linkml_exchanges_schema.yaml\"\n",
    "output_yaml = \"../linkml/data/yaml/linkml_exchanges_instance.yaml\"\n",
    "\n",
    "process_instance(input_file, schema_file, output_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and RDF (JSON-LD, Turtle) Generation exchanges EPD YAML Instance\n",
    "\n",
    "import yaml\n",
    "from linkml_runtime.dumpers import RDFLibDumper\n",
    "from linkml_runtime.loaders import YAMLLoader\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "from data.py.linkml_exchanges_schema import Exchanges\n",
    "\n",
    "\n",
    "def generate_rdf(\n",
    "    schema_path: str,\n",
    "    instance_path: str,\n",
    "    output_jsonld_path: str,\n",
    "    output_turtle_path: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load a validated LinkML YAML instance, create a SchemaView, and dump the instance to RDF in JSON-LD and Turtle formats.\n",
    "\n",
    "    Steps:\n",
    "        1. Load the instance from a YAML file.\n",
    "        2. Create a SchemaView from the schema file.\n",
    "        3. Generate RDF in JSON-LD and Turtle formats.\n",
    "        4. Write the RDF to files.\n",
    "\n",
    "    Parameters:\n",
    "        schema_path (str): Path to the LinkML YAML schema file.\n",
    "        instance_path (str): Path to the validated LinkML YAML instance file.\n",
    "        output_jsonld_path (str): Path where the output JSON-LD file will be written.\n",
    "        output_turtle_path (str): Path where the output Turtle file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during instance loading, SchemaView creation, or RDF generation.\n",
    "    \"\"\"\n",
    "    # Load instance from file\n",
    "    try:\n",
    "        with open(instance_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            loaded_data = yaml.safe_load(f)  # This is now a Python dict\n",
    "        # Extract \"exchanges\"\n",
    "        if \"exchanges\" in loaded_data:\n",
    "            loaded_data = loaded_data[\"exchanges\"]\n",
    "        # Feed loaded_data dictionary into the loader\n",
    "        instance_obj = YAMLLoader().load(loaded_data, target_class=Exchanges)\n",
    "        print(f\"Instance loaded from: {instance_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Create SchemaView\n",
    "    try:\n",
    "        sv = SchemaView(schema_path)\n",
    "        print(\"SchemaView created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during SchemaView creation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Generate and write RDF (JSON-LD, Turtle)\n",
    "    try:\n",
    "        dumper = RDFLibDumper()\n",
    "        # JSON‑LD format\n",
    "        # rdf_jsonld = dumper.dumps(instance_obj, schemaview=sv, fmt=\"json-ld\")\n",
    "        # with open(output_jsonld_path, \"w\", encoding=\"utf-8\") as rdf_file:\n",
    "        #     rdf_file.write(rdf_jsonld)\n",
    "        # print(f\"RDF (JSON‑LD) successfully written to: {output_jsonld_path}\")\n",
    "        # Turtle format\n",
    "        rdf_turtle = dumper.dumps(instance_obj, schemaview=sv, fmt=\"turtle\")\n",
    "        with open(output_turtle_path, \"w\", encoding=\"utf-8\") as ttl_file:\n",
    "            ttl_file.write(rdf_turtle)\n",
    "        print(f\"RDF (Turtle) successfully written to: {output_turtle_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during RDF generation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "schema_path = \"../linkml/data/yaml/linkml_exchanges_schema.yaml\"\n",
    "instance_path = \"../linkml/data/yaml/linkml_exchanges_instance.yaml\"\n",
    "output_jsonld_path = \"../linkml/data/rdf/linkml_exchanges_instance.jsonld\"\n",
    "output_turtle_path = \"../linkml/data/rdf/linkml_exchanges_instance.ttl\"\n",
    "\n",
    "generate_rdf(schema_path, instance_path, output_jsonld_path, output_turtle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate only the `LCIAResults` part of the Schema and JSON Instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "sd_yaml_schema = \"../linkml/data/yaml/linkml_shared_definitions.yaml\"\n",
    "sd_python_file = \"../linkml/data/py/linkml_shared_definitions.py\"\n",
    "\n",
    "# Define an import map so that types like \"String\", \"Boolean\", etc.\n",
    "# are resolved to their LinkML runtime equivalents.\n",
    "import_map = {\n",
    "    \"Boolean\": \"linkml_runtime.linkml_model.types.Boolean\",\n",
    "    \"String\": \"linkml_runtime.linkml_model.types.String\",\n",
    "    \"Integer\": \"linkml_runtime.linkml_model.types.Integer\",\n",
    "    \"Float\": \"linkml_runtime.linkml_model.types.Float\",\n",
    "    \"dateTime\": \"linkml_runtime.linkml_model.types.dateTime\",\n",
    "    # \"Version\": \"linkml_runtime.linkml_model.types.Version\", # Not in LinkML runtime\n",
    "    # \"anyURI\": \"linkml_runtime.linkml_model.types.anyURI\"  # Not in LinkML runtime\n",
    "}\n",
    "\n",
    "generator = PythonGenerator(\n",
    "    schema=sd_yaml_schema,\n",
    "    gen_slots=True,\n",
    "    gen_classvars=True,\n",
    "    mergeimports=True,\n",
    "    metadata=True,\n",
    "    genmeta=False,\n",
    "    importmap=import_map,\n",
    ")\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "\n",
    "with open(sd_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {sd_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "ai_yaml_schema = \"../linkml/data/yaml/linkml_lciaResults_schema.yaml\"\n",
    "ai_python_file = \"../linkml/data/py/linkml_lciaResults_schema.py\"\n",
    "\n",
    "# Define an import map so that types like \"String\", \"Boolean\", etc.\n",
    "# are resolved to their LinkML runtime equivalents.\n",
    "import_map = {\n",
    "    \"Boolean\": \"linkml_runtime.linkml_model.types.Boolean\",\n",
    "    \"String\": \"linkml_runtime.linkml_model.types.String\",\n",
    "    \"Integer\": \"linkml_runtime.linkml_model.types.Integer\",\n",
    "    \"Float\": \"linkml_runtime.linkml_model.types.Float\",\n",
    "    \"dateTime\": \"linkml_runtime.linkml_model.types.dateTime\",\n",
    "    # \"Version\": \"linkml_runtime.linkml_model.types.Version\", # Not in LinkML runtime\n",
    "    # \"anyURI\": \"linkml_runtime.linkml_model.types.anyURI\"  # Not in LinkML runtime\n",
    "}\n",
    "\n",
    "generator = PythonGenerator(\n",
    "    schema=ai_yaml_schema,\n",
    "    gen_slots=True,\n",
    "    gen_classvars=True,\n",
    "    mergeimports=True,\n",
    "    metadata=True,\n",
    "    genmeta=False,\n",
    "    importmap=import_map,\n",
    ")\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "with open(ai_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {ai_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance Generation and Loading LCIAResults EPD YAML\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "from data.py.linkml_lciaResults_schema import LCIAResults\n",
    "from linkml.validator import Validator\n",
    "from linkml_runtime.loaders.yaml_loader import YAMLLoader\n",
    "\n",
    "\n",
    "def odict_to_dict(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert an OrderedDict (or list/dict containing OrderedDicts) to a standard dict.\n",
    "\n",
    "    Parameters:\n",
    "        obj: An OrderedDict, list, or dict potentially containing OrderedDicts.\n",
    "\n",
    "    Returns:\n",
    "        A new structure where all OrderedDict instances are replaced with standard dicts.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, OrderedDict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [odict_to_dict(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def build_minimal_epd_structure(data):\n",
    "    \"\"\"\n",
    "    Build a minimal nested dictionary according to our minimal schema.\n",
    "    This function assumes that the JSON instance already contains a \"lciaResults\" key.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): A dictionary loaded from a JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary structured with a top-level 'lciaResults' key.\n",
    "    \"\"\"\n",
    "    instance_dict = {\"lciaResults\": data.get(\"lciaResults\", {})}\n",
    "    return instance_dict\n",
    "\n",
    "\n",
    "def process_instance(input_path, schema_path, output_yaml_path):\n",
    "    \"\"\"\n",
    "    Process an instance by loading a minimal JSON instance, converting it, dumping it to YAML,\n",
    "    validating it against a schema, and loading it with YAMLLoader.\n",
    "\n",
    "    Steps:\n",
    "      1. Load a minimal JSON instance containing only the LCIAResults section.\n",
    "      2. Convert OrderedDicts to dicts.\n",
    "      3. Wrap the data in a top-level LCIAResults structure.\n",
    "      4. Dump the resulting instance to a YAML file.\n",
    "      5. Validate the instance using a schema validator.\n",
    "      6. Load the YAML instance with YAMLLoader.\n",
    "\n",
    "    Parameters:\n",
    "        input_path (str): Path to the input JSON file.\n",
    "        schema_path (str): Path to the YAML schema file.\n",
    "        output_yaml_path (str): Path where the output YAML file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during validation or instance loading.\n",
    "    \"\"\"\n",
    "    # Load the minimal JSON instance (containing only LCIAResults)\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    data = odict_to_dict(data)\n",
    "\n",
    "    # Build the nested instance structure.\n",
    "    instance_data = build_minimal_epd_structure(data)\n",
    "\n",
    "    # Dump to YAML\n",
    "    with open(output_yaml_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        yaml.dump(instance_data, out, sort_keys=False, allow_unicode=True)\n",
    "    print(f\"LCIAResults EPD YAML instance written to: {output_yaml_path}\")\n",
    "\n",
    "    # Validate using the Validator\n",
    "    try:\n",
    "        validator = Validator(schema_path, strict=False)\n",
    "        report = validator.validate(instance_data, \"lciaResults\")\n",
    "        if report.results:\n",
    "            print(\"Validation errors:\")\n",
    "            for result in report.results:\n",
    "                print(result.message)\n",
    "        else:\n",
    "            print(\"The LCIAResults instance is valid!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during validation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Load the YAML instance using YAMLLoader\n",
    "    try:\n",
    "        loader = YAMLLoader()\n",
    "        # If the instance is wrapped in a \"LCIAResults\" key, unwrap it.\n",
    "        if \"lciaResults\" in instance_data:\n",
    "            instance_data = instance_data[\"lciaResults\"]\n",
    "        loader.load(instance_data, target_class=LCIAResults)\n",
    "        print(\"Instance loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading via YAMLLoader:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Usage\n",
    "input_file = \"../linkml/data/json/instance_LCIAResults.json\"\n",
    "schema_file = \"../linkml/data/yaml/linkml_LCIAResults_schema.yaml\"\n",
    "output_yaml = \"../linkml/data/yaml/linkml_LCIAResults_instance.yaml\"\n",
    "\n",
    "process_instance(input_file, schema_file, output_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and RDF (JSON-LD, Turtle) Generation LCIAResults EPD YAML Instance\n",
    "\n",
    "import yaml\n",
    "from linkml_runtime.dumpers import RDFLibDumper\n",
    "from linkml_runtime.loaders import YAMLLoader\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "from data.py.linkml_lciaResults_schema import LCIAResults\n",
    "\n",
    "\n",
    "def generate_rdf(\n",
    "    schema_path: str,\n",
    "    instance_path: str,\n",
    "    output_jsonld_path: str,\n",
    "    output_turtle_path: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load a validated LinkML YAML instance, create a SchemaView, and dump the instance to RDF in JSON-LD and Turtle formats.\n",
    "\n",
    "    Steps:\n",
    "        1. Load the instance from a YAML file.\n",
    "        2. Create a SchemaView from the schema file.\n",
    "        3. Generate RDF in JSON-LD and Turtle formats.\n",
    "        4. Write the RDF to files.\n",
    "\n",
    "    Parameters:\n",
    "        schema_path (str): Path to the LinkML YAML schema file.\n",
    "        instance_path (str): Path to the validated LinkML YAML instance file.\n",
    "        output_jsonld_path (str): Path where the output JSON-LD file will be written.\n",
    "        output_turtle_path (str): Path where the output Turtle file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during instance loading, SchemaView creation, or RDF generation.\n",
    "    \"\"\"\n",
    "    # Load instance from file\n",
    "    try:\n",
    "        with open(instance_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            loaded_data = yaml.safe_load(f)  # This is now a Python dict\n",
    "        # Extract \"LCIAResults\"\n",
    "        if \"lciaResults\" in loaded_data:\n",
    "            loaded_data = loaded_data[\"lciaResults\"]\n",
    "        # Feed loaded_data dictionary into the loader\n",
    "        instance_obj = YAMLLoader().load(loaded_data, target_class=LCIAResults)\n",
    "        print(f\"Instance loaded from: {instance_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Create SchemaView\n",
    "    try:\n",
    "        sv = SchemaView(schema_path)\n",
    "        print(\"SchemaView created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during SchemaView creation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Generate and write RDF (JSON-LD, Turtle)\n",
    "    try:\n",
    "        dumper = RDFLibDumper()\n",
    "        # # JSON‑LD format\n",
    "        # rdf_jsonld = dumper.dumps(instance_obj, schemaview=sv, fmt=\"json-ld\")\n",
    "        # with open(output_jsonld_path, \"w\", encoding=\"utf-8\") as rdf_file:\n",
    "        #     rdf_file.write(rdf_jsonld)\n",
    "        # print(f\"RDF (JSON‑LD) successfully written to: {output_jsonld_path}\")\n",
    "        # Turtle format\n",
    "        rdf_turtle = dumper.dumps(instance_obj, schemaview=sv, fmt=\"turtle\")\n",
    "        with open(output_turtle_path, \"w\", encoding=\"utf-8\") as ttl_file:\n",
    "            ttl_file.write(rdf_turtle)\n",
    "        print(f\"RDF (Turtle) successfully written to: {output_turtle_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during RDF generation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "# Example usage:\n",
    "schema_path = \"../linkml/data/yaml/linkml_LCIAResults_schema.yaml\"\n",
    "instance_path = \"../linkml/data/yaml/linkml_LCIAResults_instance.yaml\"\n",
    "output_jsonld_path = \"../linkml/data/rdf/linkml_LCIAResults_instance.jsonld\"\n",
    "output_turtle_path = \"../linkml/data/rdf/linkml_LCIAResults_instance.ttl\"\n",
    "\n",
    "generate_rdf(schema_path, instance_path, output_jsonld_path, output_turtle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the whole schema and JSON Instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "sd_yaml_schema = \"../linkml/data/yaml/linkml_shared_definitions.yaml\"\n",
    "sd_python_file = \"../linkml/data/py/linkml_shared_definitions.py\"\n",
    "\n",
    "generator = PythonGenerator(schema=sd_yaml_schema)\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "\n",
    "with open(sd_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {sd_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "ai_yaml_schema = \"../linkml/data/yaml/linkml_processDataSet_schema.yaml\"\n",
    "ai_python_file = \"../linkml/data/py/linkml_processDataSet_schema.py\"\n",
    "\n",
    "generator = PythonGenerator(schema=ai_yaml_schema)\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "with open(ai_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {ai_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance Generation and Loading processDataSet EPD YAML\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "from data.py.linkml_processDataSet_schema import ProcessDataSet\n",
    "from linkml.validator import Validator\n",
    "from linkml_runtime.loaders.yaml_loader import YAMLLoader\n",
    "\n",
    "def odict_to_dict(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert an OrderedDict (or list/dict containing OrderedDicts) to a standard dict.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, OrderedDict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [odict_to_dict(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def process_instance(input_path, schema_path, output_yaml_path):\n",
    "    \"\"\"\n",
    "    Process an instance by loading a JSON file that is already structured\n",
    "    at the top level for the ProcessDataSet class. Example top-level JSON keys:\n",
    "        {\n",
    "          \"processInformation\": { ... },\n",
    "          \"modellingAndValidation\": { ... },\n",
    "          ...\n",
    "          \"version\": \"1.1\"\n",
    "        }\n",
    "\n",
    "    Steps:\n",
    "      1. Load and parse the JSON directly into a dict.\n",
    "      2. Dump the dict to a YAML file for inspection (optional).\n",
    "      3. Validate the dict against the ProcessDataSet class in your schema.\n",
    "      4. Load the dict as a ProcessDataSet object using YAMLLoader.\n",
    "    \"\"\"\n",
    "    # 1. Load the JSON instance\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # (Optional) Convert OrderedDicts to normal dicts if needed\n",
    "    data = odict_to_dict(data)\n",
    "\n",
    "    # 2. Dump the data to a YAML file (for debugging or pipeline usage)\n",
    "    with open(output_yaml_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        yaml.dump(data, out, sort_keys=False, allow_unicode=True)\n",
    "    print(f\"ProcessDataSet EPD YAML instance written to: {output_yaml_path}\")\n",
    "\n",
    "    # 3. Validate the dict using the LinkML Validator\n",
    "    try:\n",
    "        validator = Validator(schema_path, strict=False)\n",
    "        report = validator.validate(data, \"ProcessDataSet\")\n",
    "        if report.results:\n",
    "            print(\"Validation errors:\")\n",
    "            for result in report.results:\n",
    "                print(result.message)\n",
    "        else:\n",
    "            print(\"The ProcessDataSet instance is valid!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during validation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # 4. Load the dict as a ProcessDataSet object\n",
    "    try:\n",
    "        loader = YAMLLoader()\n",
    "        obj = loader.load(data, target_class=ProcessDataSet)\n",
    "        print(\"Instance loaded successfully as ProcessDataSet object!\")\n",
    "        # You can now use `obj` in your code, e.g. print(obj)\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading via YAMLLoader:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Usage example\n",
    "input_file = \"../data/pipeline2/json/epds/0db12903-1403-4c9a-817e-b48299d17aba_RN_ID.json\"\n",
    "# input_file = \"../linkml/data/json/5b6b44e0-f5e4-451f-54a3-08dcec2f0f89_renamedScript_newID.json\"\n",
    "schema_file = \"../linkml/data/yaml/linkml_processDataSet_schema.yaml\"\n",
    "output_yaml = \"../linkml/data/yaml/linkml_processDataSet_instance.yaml\"\n",
    "\n",
    "process_instance(input_file, schema_file, output_yaml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and RDF (JSON-LD, Turtle) Generation processDataSet EPD YAML Instance\n",
    "\n",
    "import yaml\n",
    "from linkml_runtime.dumpers import RDFLibDumper\n",
    "from linkml_runtime.loaders import YAMLLoader\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "from data.py.linkml_processDataSet_schema import ProcessDataSet\n",
    "\n",
    "\n",
    "def generate_rdf(\n",
    "    schema_path: str,\n",
    "    instance_path: str,\n",
    "    output_jsonld_path: str,\n",
    "    output_turtle_path: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load a validated LinkML YAML instance, create a SchemaView, and dump the instance to RDF in JSON-LD and Turtle formats.\n",
    "\n",
    "    Steps:\n",
    "        1. Load the instance from a YAML file.\n",
    "        2. Create a SchemaView from the schema file.\n",
    "        3. Generate RDF in JSON-LD and Turtle formats.\n",
    "        4. Write the RDF to files.\n",
    "\n",
    "    Parameters:\n",
    "        schema_path (str): Path to the LinkML YAML schema file.\n",
    "        instance_path (str): Path to the validated LinkML YAML instance file.\n",
    "        output_jsonld_path (str): Path where the output JSON-LD file will be written.\n",
    "        output_turtle_path (str): Path where the output Turtle file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during instance loading, SchemaView creation, or RDF generation.\n",
    "    \"\"\"\n",
    "    # Load instance from file\n",
    "    try:\n",
    "        with open(instance_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            loaded_data = yaml.safe_load(f)  # This is now a Python dict\n",
    "        # Extract \"processDataSet\"\n",
    "        if \"processDataSet\" in loaded_data:\n",
    "            loaded_data = loaded_data[\"processDataSet\"]\n",
    "        # Feed loaded_data dictionary into the loader\n",
    "        instance_obj = YAMLLoader().load(loaded_data, target_class=ProcessDataSet)\n",
    "        print(f\"Instance loaded from: {instance_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Create SchemaView\n",
    "    try:\n",
    "        sv = SchemaView(schema_path)\n",
    "        print(\"SchemaView created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during SchemaView creation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Generate and write RDF (JSON-LD, Turtle)\n",
    "    try:\n",
    "        dumper = RDFLibDumper()\n",
    "        # JSON‑LD format\n",
    "        # rdf_jsonld = dumper.dumps(instance_obj, schemaview=sv, fmt=\"json-ld\")\n",
    "        # with open(output_jsonld_path, \"w\", encoding=\"utf-8\") as rdf_file:\n",
    "        #     rdf_file.write(rdf_jsonld)\n",
    "        # print(f\"RDF (JSON‑LD) successfully written to: {output_jsonld_path}\")\n",
    "        # Turtle format\n",
    "        rdf_turtle = dumper.dumps(instance_obj, schemaview=sv, fmt=\"turtle\")\n",
    "        with open(output_turtle_path, \"w\", encoding=\"utf-8\") as ttl_file:\n",
    "            ttl_file.write(rdf_turtle)\n",
    "        print(f\"RDF (Turtle) successfully written to: {output_turtle_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during RDF generation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "schema_path = \"../linkml/data/yaml/linkml_processDataSet_schema.yaml\"\n",
    "instance_path = \"../linkml/data/yaml/linkml_processDataSet_instance.yaml\"\n",
    "output_jsonld_path = \"../linkml/data/rdf/linkml_processDataSet_instance.jsonld\"\n",
    "output_turtle_path = \"../linkml/data/rdf/linkml_processDataSet_instance.ttl\"\n",
    "\n",
    "generate_rdf(schema_path, instance_path, output_jsonld_path, output_turtle_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
