{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate only the `processInformation` part of the Schema and JSON Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python model generated and written to ../linkml/data/py/linkml_shared_definitions.py.\n"
     ]
    }
   ],
   "source": [
    "# Produce Python module from SharedDefinitions using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "sd_yaml_schema = \"../linkml/data/yaml/linkml_shared_definitions.yaml\"\n",
    "sd_python_file = \"../linkml/data/py/linkml_shared_definitions.py\"\n",
    "\n",
    "# Define an import map so that types like \"String\", \"Boolean\", etc.\n",
    "# are resolved to their LinkML runtime equivalents.\n",
    "import_map = {\n",
    "    \"Boolean\": \"linkml_runtime.linkml_model.types.Boolean\",\n",
    "    \"String\": \"linkml_runtime.linkml_model.types.String\",\n",
    "    \"Integer\": \"linkml_runtime.linkml_model.types.Integer\",\n",
    "    \"Float\": \"linkml_runtime.linkml_model.types.Float\",\n",
    "    \"dateTime\": \"linkml_runtime.linkml_model.types.dateTime\",\n",
    "    # \"Version\": \"linkml_runtime.linkml_model.types.Version\", # Not in LinkML runtime\n",
    "    # \"anyURI\": \"linkml_runtime.linkml_model.types.anyURI\"  # Not in LinkML runtime\n",
    "}\n",
    "\n",
    "generator = PythonGenerator(\n",
    "    schema=sd_yaml_schema,\n",
    "    gen_slots=True,\n",
    "    gen_classvars=True,\n",
    "    mergeimports=True,\n",
    "    metadata=True,\n",
    "    genmeta=False,\n",
    "    importmap=import_map,\n",
    ")\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "\n",
    "with open(sd_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {sd_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python model generated and written to ../linkml/data/py/linkml_processInformation_schema.py.\n"
     ]
    }
   ],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "\n",
    "pi_yaml_schema = \"../linkml/data/yaml/linkml_processInformation_schema.yaml\"\n",
    "pi_python_file = \"../linkml/data/py/linkml_processInformation_schema.py\"\n",
    "\n",
    "\n",
    "# Define an import map so that types like \"String\", \"Boolean\", etc.\n",
    "# are resolved to their LinkML runtime equivalents.\n",
    "import_map = {\n",
    "    \"Boolean\": \"linkml_runtime.linkml_model.types.Boolean\",\n",
    "    \"String\": \"linkml_runtime.linkml_model.types.String\",\n",
    "    \"Integer\": \"linkml_runtime.linkml_model.types.Integer\",\n",
    "    \"Float\": \"linkml_runtime.linkml_model.types.Float\",\n",
    "    \"dateTime\": \"linkml_runtime.linkml_model.types.dateTime\",\n",
    "    # \"Version\": \"linkml_runtime.linkml_model.types.Version\", # Not in LinkML runtime\n",
    "    # \"anyURI\": \"linkml_runtime.linkml_model.types.anyURI\"  # Not in LinkML runtime\n",
    "}\n",
    "\n",
    "generator = PythonGenerator(\n",
    "    schema=pi_yaml_schema,\n",
    "    gen_slots=True,\n",
    "    gen_classvars=True,\n",
    "    mergeimports=True,\n",
    "    metadata=True,\n",
    "    genmeta=False,\n",
    "    importmap=import_map,\n",
    ")\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "with open(pi_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {pi_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processInformation EPD YAML instance written to: ../linkml/data/yaml/linkml_processInformation_instance.yaml\n",
      "The ProcessInformation instance is valid!\n",
      "Instance loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Instance Generation and Loading processInformation EPD YAML\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "from data.py.linkml_processInformation_schema import ProcessInformation\n",
    "from linkml.validator import Validator\n",
    "from linkml_runtime.loaders.yaml_loader import YAMLLoader\n",
    "\n",
    "\n",
    "def odict_to_dict(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert an OrderedDict (or list/dict containing OrderedDicts) to a standard dict.\n",
    "\n",
    "    Parameters:\n",
    "        obj: An OrderedDict, list, or dict potentially containing OrderedDicts.\n",
    "\n",
    "    Returns:\n",
    "        A new structure where all OrderedDict instances are replaced with standard dicts.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, OrderedDict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [odict_to_dict(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def build_minimal_epd_structure(data):\n",
    "    \"\"\"\n",
    "    Build a minimal nested dictionary according to our minimal schema.\n",
    "    This function assumes that the JSON instance already contains a \"processInformation\" key.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): A dictionary loaded from a JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary structured with a top-level 'processInformation' key.\n",
    "    \"\"\"\n",
    "    instance_dict = {\"processInformation\": data.get(\"processInformation\", {})}\n",
    "    return instance_dict\n",
    "\n",
    "\n",
    "def process_instance(input_path, schema_path, output_yaml_path):\n",
    "    \"\"\"\n",
    "    Process an instance by loading a minimal JSON instance, converting it, dumping it to YAML,\n",
    "    validating it against a schema, and loading it with YAMLLoader.\n",
    "\n",
    "    Steps:\n",
    "      1. Load a minimal JSON instance containing only the processInformation section.\n",
    "      2. Convert OrderedDicts to dicts.\n",
    "      3. Wrap the data in a top-level ProcessInformation structure.\n",
    "      4. Dump the resulting instance to a YAML file.\n",
    "      5. Validate the instance using a schema validator.\n",
    "      6. Load the YAML instance with YAMLLoader.\n",
    "\n",
    "    Parameters:\n",
    "        input_path (str): Path to the input JSON file.\n",
    "        schema_path (str): Path to the YAML schema file.\n",
    "        output_yaml_path (str): Path where the output YAML file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during validation or instance loading.\n",
    "    \"\"\"\n",
    "    # Load the minimal JSON instance (containing only processInformation)\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    data = odict_to_dict(data)\n",
    "\n",
    "    # Build the nested instance structure.\n",
    "    instance_data = build_minimal_epd_structure(data)\n",
    "\n",
    "    # Dump to YAML\n",
    "    with open(output_yaml_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        yaml.dump(instance_data, out, sort_keys=False, allow_unicode=True)\n",
    "    print(f\"processInformation EPD YAML instance written to: {output_yaml_path}\")\n",
    "\n",
    "    # Validate using the Validator\n",
    "    try:\n",
    "        validator = Validator(schema_path, strict=False)\n",
    "        report = validator.validate(instance_data, \"ProcessInformation\")\n",
    "        if report.results:\n",
    "            print(\"Validation errors:\")\n",
    "            for result in report.results:\n",
    "                print(result.message)\n",
    "        else:\n",
    "            print(\"The ProcessInformation instance is valid!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during validation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Load the YAML instance using YAMLLoader\n",
    "    try:\n",
    "        loader = YAMLLoader()\n",
    "        # If the instance is wrapped in a \"processInformation\" key, unwrap it.\n",
    "        if \"processInformation\" in instance_data:\n",
    "            instance_data = instance_data[\"processInformation\"]\n",
    "        loader.load(instance_data, target_class=ProcessInformation)\n",
    "        print(\"Instance loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading via YAMLLoader:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Usage\n",
    "input_file = \"../linkml/data/json/instance_processInformation.json\"\n",
    "schema_file = \"../linkml/data/yaml/linkml_processInformation_schema.yaml\"\n",
    "output_yaml = \"../linkml/data/yaml/linkml_processInformation_instance.yaml\"\n",
    "\n",
    "process_instance(input_file, schema_file, output_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance loaded from: ../linkml/data/yaml/linkml_processInformation_instance.yaml\n",
      "SchemaView created successfully.\n",
      "RDF (Turtle) successfully written to: ../linkml/data/rdf/linkml_processInformation_instance.ttl\n"
     ]
    }
   ],
   "source": [
    "# Loading and RDF (JSON-LD, Turtle) Generation processInformation EPD YAML Instance\n",
    "\n",
    "import yaml\n",
    "from linkml_runtime.dumpers import RDFLibDumper\n",
    "from linkml_runtime.loaders import YAMLLoader\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "from data.py.linkml_processInformation_schema import ProcessInformation\n",
    "\n",
    "\n",
    "def generate_rdf(\n",
    "    schema_path: str,\n",
    "    instance_path: str,\n",
    "    output_jsonld_path: str,\n",
    "    output_turtle_path: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load a validated LinkML YAML instance, create a SchemaView, and dump the instance to RDF in JSON-LD and Turtle formats.\n",
    "\n",
    "    Steps:\n",
    "        1. Load the instance from a YAML file.\n",
    "        2. Create a SchemaView from the schema file.\n",
    "        3. Generate RDF in JSON-LD and Turtle formats.\n",
    "        4. Write the RDF to files.\n",
    "\n",
    "    Parameters:\n",
    "        schema_path (str): Path to the LinkML YAML schema file.\n",
    "        instance_path (str): Path to the validated LinkML YAML instance file.\n",
    "        output_jsonld_path (str): Path where the output JSON-LD file will be written.\n",
    "        output_turtle_path (str): Path where the output Turtle file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during instance loading, SchemaView creation, or RDF generation.\n",
    "    \"\"\"\n",
    "    # Load instance from file\n",
    "    try:\n",
    "        with open(instance_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            loaded_data = yaml.safe_load(f)  # This is now a Python dict\n",
    "        # Extract \"processInformation\"\n",
    "        if \"processInformation\" in loaded_data:\n",
    "            loaded_data = loaded_data[\"processInformation\"]\n",
    "        # Feed loaded_data dictionary into the loader\n",
    "        instance_obj = YAMLLoader().load(loaded_data, target_class=ProcessInformation)\n",
    "        print(f\"Instance loaded from: {instance_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Create SchemaView\n",
    "    try:\n",
    "        sv = SchemaView(schema_path)\n",
    "        print(\"SchemaView created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during SchemaView creation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Generate and write RDF (JSON-LD, Turtle)\n",
    "    try:\n",
    "        dumper = RDFLibDumper()\n",
    "        # JSON‑LD format\n",
    "        # rdf_jsonld = dumper.dumps(instance_obj, schemaview=sv, fmt=\"json-ld\")\n",
    "        # with open(output_jsonld_path, \"w\", encoding=\"utf-8\") as rdf_file:\n",
    "        #     rdf_file.write(rdf_jsonld)\n",
    "        # print(f\"RDF (JSON‑LD) successfully written to: {output_jsonld_path}\")\n",
    "        # Turtle format\n",
    "        rdf_turtle = dumper.dumps(instance_obj, schemaview=sv, fmt=\"turtle\")\n",
    "        with open(output_turtle_path, \"w\", encoding=\"utf-8\") as ttl_file:\n",
    "            ttl_file.write(rdf_turtle)\n",
    "        print(f\"RDF (Turtle) successfully written to: {output_turtle_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during RDF generation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "schema_path = \"../linkml/data/yaml/linkml_processInformation_schema.yaml\"\n",
    "instance_path = \"../linkml/data/yaml/linkml_processInformation_instance.yaml\"\n",
    "output_jsonld_path = \"../linkml/data/rdf/linkml_processInformation_instance.jsonld\"\n",
    "output_turtle_path = \"../linkml/data/rdf/linkml_processInformation_instance.ttl\"\n",
    "\n",
    "generate_rdf(schema_path, instance_path, output_jsonld_path, output_turtle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate only the `modellingAndValidation` part of the Schema and JSON Instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python model generated and written to ../linkml/data/py/linkml_shared_definitions.py.\n"
     ]
    }
   ],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "sd_yaml_schema = \"../linkml/data/yaml/linkml_shared_definitions.yaml\"\n",
    "sd_python_file = \"../linkml/data/py/linkml_shared_definitions.py\"\n",
    "\n",
    "# Define an import map so that types like \"String\", \"Boolean\", etc.\n",
    "# are resolved to their LinkML runtime equivalents.\n",
    "import_map = {\n",
    "    \"Boolean\": \"linkml_runtime.linkml_model.types.Boolean\",\n",
    "    \"String\": \"linkml_runtime.linkml_model.types.String\",\n",
    "    \"Integer\": \"linkml_runtime.linkml_model.types.Integer\",\n",
    "    \"Float\": \"linkml_runtime.linkml_model.types.Float\",\n",
    "    \"dateTime\": \"linkml_runtime.linkml_model.types.dateTime\",\n",
    "    # \"Version\": \"linkml_runtime.linkml_model.types.Version\", # Not in LinkML runtime\n",
    "    # \"anyURI\": \"linkml_runtime.linkml_model.types.anyURI\"  # Not in LinkML runtime\n",
    "}\n",
    "\n",
    "generator = PythonGenerator(\n",
    "    schema=sd_yaml_schema,\n",
    "    gen_slots=True,\n",
    "    gen_classvars=True,\n",
    "    mergeimports=True,\n",
    "    metadata=True,\n",
    "    genmeta=False,\n",
    "    importmap=import_map,\n",
    ")\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "\n",
    "with open(sd_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {sd_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python model generated and written to ../linkml/data/py/linkml_modellingAndValidation_schema.py.\n"
     ]
    }
   ],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "mav_yaml_schema = \"../linkml/data/yaml/linkml_modellingAndValidation_schema.yaml\"\n",
    "mav_python_file = \"../linkml/data/py/linkml_modellingAndValidation_schema.py\"\n",
    "\n",
    "# Define an import map so that types like \"String\", \"Boolean\", etc.\n",
    "# are resolved to their LinkML runtime equivalents.\n",
    "import_map = {\n",
    "    \"Boolean\": \"linkml_runtime.linkml_model.types.Boolean\",\n",
    "    \"String\": \"linkml_runtime.linkml_model.types.String\",\n",
    "    \"Integer\": \"linkml_runtime.linkml_model.types.Integer\",\n",
    "    \"Float\": \"linkml_runtime.linkml_model.types.Float\",\n",
    "    \"dateTime\": \"linkml_runtime.linkml_model.types.dateTime\",\n",
    "    # \"Version\": \"linkml_runtime.linkml_model.types.Version\", # Not in LinkML runtime\n",
    "    # \"anyURI\": \"linkml_runtime.linkml_model.types.anyURI\"  # Not in LinkML runtime\n",
    "}\n",
    "\n",
    "generator = PythonGenerator(\n",
    "    schema=mav_yaml_schema,\n",
    "    gen_slots=True,\n",
    "    gen_classvars=True,\n",
    "    mergeimports=True,\n",
    "    metadata=True,\n",
    "    genmeta=False,\n",
    "    importmap=import_map,\n",
    ")\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "with open(mav_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {mav_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modellingAndValidation EPD YAML instance written to: ../linkml/data/yaml/linkml_modellingAndValidation_instance.yaml\n",
      "The ModellingAndValidation instance is valid!\n",
      "Instance loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Instance Generation and Loading modellingAndValidation EPD YAML\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "from data.py.linkml_modellingAndValidation_schema import ModellingAndValidation\n",
    "from linkml.validator import Validator\n",
    "from linkml_runtime.loaders.yaml_loader import YAMLLoader\n",
    "\n",
    "\n",
    "def odict_to_dict(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert an OrderedDict (or list/dict containing OrderedDicts) to a standard dict.\n",
    "\n",
    "    Parameters:\n",
    "        obj: An OrderedDict, list, or dict potentially containing OrderedDicts.\n",
    "\n",
    "    Returns:\n",
    "        A new structure where all OrderedDict instances are replaced with standard dicts.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, OrderedDict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [odict_to_dict(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def build_minimal_epd_structure(data):\n",
    "    \"\"\"\n",
    "    Build a minimal nested dictionary according to our minimal schema.\n",
    "    This function assumes that the JSON instance already contains a \"modellingAndValidation\" key.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): A dictionary loaded from a JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary structured with a top-level 'modellingAndValidation' key.\n",
    "    \"\"\"\n",
    "    instance_dict = {\"modellingAndValidation\": data.get(\"modellingAndValidation\", {})}\n",
    "    return instance_dict\n",
    "\n",
    "\n",
    "def process_instance(input_path, schema_path, output_yaml_path):\n",
    "    \"\"\"\n",
    "    Process an instance by loading a minimal JSON instance, converting it, dumping it to YAML,\n",
    "    validating it against a schema, and loading it with YAMLLoader.\n",
    "\n",
    "    Steps:\n",
    "      1. Load a minimal JSON instance containing only the modellingAndValidation section.\n",
    "      2. Convert OrderedDicts to dicts.\n",
    "      3. Wrap the data in a top-level ModellingAndValidation structure.\n",
    "      4. Dump the resulting instance to a YAML file.\n",
    "      5. Validate the instance using a schema validator.\n",
    "      6. Load the YAML instance with YAMLLoader.\n",
    "\n",
    "    Parameters:\n",
    "        input_path (str): Path to the input JSON file.\n",
    "        schema_path (str): Path to the YAML schema file.\n",
    "        output_yaml_path (str): Path where the output YAML file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during validation or instance loading.\n",
    "    \"\"\"\n",
    "    # Load the minimal JSON instance (containing only modellingAndValidation)\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    data = odict_to_dict(data)\n",
    "\n",
    "    # Build the nested instance structure.\n",
    "    instance_data = build_minimal_epd_structure(data)\n",
    "\n",
    "    # Dump to YAML\n",
    "    with open(output_yaml_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        yaml.dump(instance_data, out, sort_keys=False, allow_unicode=True)\n",
    "    print(f\"modellingAndValidation EPD YAML instance written to: {output_yaml_path}\")\n",
    "\n",
    "    # Validate using the Validator\n",
    "    try:\n",
    "        validator = Validator(schema_path, strict=False)\n",
    "        report = validator.validate(instance_data, \"ModellingAndValidation\")\n",
    "        if report.results:\n",
    "            print(\"Validation errors:\")\n",
    "            for result in report.results:\n",
    "                print(result.message)\n",
    "        else:\n",
    "            print(\"The ModellingAndValidation instance is valid!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during validation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Load the YAML instance using YAMLLoader\n",
    "    try:\n",
    "        loader = YAMLLoader()\n",
    "        # If the instance is wrapped in a \"modellingAndValidation\" key, unwrap it.\n",
    "        if \"modellingAndValidation\" in instance_data:\n",
    "            instance_data = instance_data[\"modellingAndValidation\"]\n",
    "        loader.load(instance_data, target_class=ModellingAndValidation)\n",
    "        print(\"Instance loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading via YAMLLoader:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Usage\n",
    "input_file = \"../linkml/data/json/instance_modellingAndValidation.json\"\n",
    "schema_file = \"../linkml/data/yaml/linkml_modellingAndValidation_schema.yaml\"\n",
    "output_yaml = \"../linkml/data/yaml/linkml_modellingAndValidation_instance.yaml\"\n",
    "\n",
    "process_instance(input_file, schema_file, output_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance loaded from: ../linkml/data/yaml/linkml_modellingAndValidation_instance.yaml\n",
      "SchemaView created successfully.\n",
      "RDF (Turtle) successfully written to: ../linkml/data/rdf/linkml_modellingAndValidation_instance.ttl\n"
     ]
    }
   ],
   "source": [
    "# Loading and RDF (JSON-LD, Turtle) Generation modellingAndValidation EPD YAML Instance\n",
    "\n",
    "import yaml\n",
    "from linkml_runtime.dumpers import RDFLibDumper\n",
    "from linkml_runtime.loaders import YAMLLoader\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "from data.py.linkml_modellingAndValidation_schema import ModellingAndValidation\n",
    "\n",
    "\n",
    "def generate_rdf(\n",
    "    schema_path: str,\n",
    "    instance_path: str,\n",
    "    output_jsonld_path: str,\n",
    "    output_turtle_path: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load a validated LinkML YAML instance, create a SchemaView, and dump the instance to RDF in JSON-LD and Turtle formats.\n",
    "\n",
    "    Steps:\n",
    "        1. Load the instance from a YAML file.\n",
    "        2. Create a SchemaView from the schema file.\n",
    "        3. Generate RDF in JSON-LD and Turtle formats.\n",
    "        4. Write the RDF to files.\n",
    "\n",
    "    Parameters:\n",
    "        schema_path (str): Path to the LinkML YAML schema file.\n",
    "        instance_path (str): Path to the validated LinkML YAML instance file.\n",
    "        output_jsonld_path (str): Path where the output JSON-LD file will be written.\n",
    "        output_turtle_path (str): Path where the output Turtle file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during instance loading, SchemaView creation, or RDF generation.\n",
    "    \"\"\"\n",
    "    # Load instance from file\n",
    "    try:\n",
    "        with open(instance_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            loaded_data = yaml.safe_load(f)  # This is now a Python dict\n",
    "        # Extract \"modellingAndValidation\"\n",
    "        if \"modellingAndValidation\" in loaded_data:\n",
    "            loaded_data = loaded_data[\"modellingAndValidation\"]\n",
    "        # Feed loaded_data dictionary into the loader\n",
    "        instance_obj = YAMLLoader().load(\n",
    "            loaded_data, target_class=ModellingAndValidation\n",
    "        )\n",
    "        print(f\"Instance loaded from: {instance_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Create SchemaView\n",
    "    try:\n",
    "        sv = SchemaView(schema_path)\n",
    "        print(\"SchemaView created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during SchemaView creation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Generate and write RDF (JSON-LD, Turtle)\n",
    "    try:\n",
    "        dumper = RDFLibDumper()\n",
    "        # # JSON‑LD format\n",
    "        # rdf_jsonld = dumper.dumps(instance_obj, schemaview=sv, fmt=\"json-ld\")\n",
    "        # with open(output_jsonld_path, \"w\", encoding=\"utf-8\") as rdf_file:\n",
    "        #     rdf_file.write(rdf_jsonld)\n",
    "        # print(f\"RDF (JSON‑LD) successfully written to: {output_jsonld_path}\")\n",
    "        # Turtle format\n",
    "        rdf_turtle = dumper.dumps(instance_obj, schemaview=sv, fmt=\"turtle\")\n",
    "        with open(output_turtle_path, \"w\", encoding=\"utf-8\") as ttl_file:\n",
    "            ttl_file.write(rdf_turtle)\n",
    "        print(f\"RDF (Turtle) successfully written to: {output_turtle_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during RDF generation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "schema_path = \"../linkml/data/yaml/linkml_modellingAndValidation_schema.yaml\"\n",
    "instance_path = \"../linkml/data/yaml/linkml_modellingAndValidation_instance.yaml\"\n",
    "output_jsonld_path = \"../linkml/data/rdf/linkml_modellingAndValidation_instance.jsonld\"\n",
    "output_turtle_path = \"../linkml/data/rdf/linkml_modellingAndValidation_instance.ttl\"\n",
    "\n",
    "generate_rdf(schema_path, instance_path, output_jsonld_path, output_turtle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate only the `administrativeInformation` part of the Schema and JSON Instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python model generated and written to ../linkml/data/py/linkml_shared_definitions.py.\n"
     ]
    }
   ],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "sd_yaml_schema = \"../linkml/data/yaml/linkml_shared_definitions.yaml\"\n",
    "sd_python_file = \"../linkml/data/py/linkml_shared_definitions.py\"\n",
    "\n",
    "generator = PythonGenerator(schema=sd_yaml_schema)\n",
    "python_code = generator.serialize()\n",
    "\n",
    "with open(sd_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {sd_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python model generated and written to ../linkml/data/py/linkml_administrativeInformation_schema.py.\n"
     ]
    }
   ],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "ai_yaml_schema = \"../linkml/data/yaml/linkml_administrativeInformation_schema.yaml\"\n",
    "ai_python_file = \"../linkml/data/py/linkml_administrativeInformation_schema.py\"\n",
    "\n",
    "generator = PythonGenerator(schema=ai_yaml_schema)\n",
    "python_code = generator.serialize()\n",
    "\n",
    "with open(ai_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {ai_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "administrativeInformation EPD YAML instance written to: ../linkml/data/yaml/linkml_administrativeInformation_instance.yaml\n",
      "The AdministrativeInformation instance is valid!\n",
      "Instance loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Instance Generation and Loading administrativeInformation EPD YAML\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "from data.py.linkml_administrativeInformation_schema import AdministrativeInformation\n",
    "from linkml.validator import Validator\n",
    "from linkml_runtime.loaders.yaml_loader import YAMLLoader\n",
    "\n",
    "\n",
    "def odict_to_dict(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert an OrderedDict (or list/dict containing OrderedDicts) to a standard dict.\n",
    "\n",
    "    Parameters:\n",
    "        obj: An OrderedDict, list, or dict potentially containing OrderedDicts.\n",
    "\n",
    "    Returns:\n",
    "        A new structure where all OrderedDict instances are replaced with standard dicts.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, OrderedDict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [odict_to_dict(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def build_minimal_epd_structure(data):\n",
    "    \"\"\"\n",
    "    Build a minimal nested dictionary according to our minimal schema.\n",
    "    This function assumes that the JSON instance already contains a \"administrativeInformation\" key.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): A dictionary loaded from a JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary structured with a top-level 'administrativeInformation' key.\n",
    "    \"\"\"\n",
    "    instance_dict = {\n",
    "        \"administrativeInformation\": data.get(\"administrativeInformation\", {})\n",
    "    }\n",
    "    return instance_dict\n",
    "\n",
    "\n",
    "def process_instance(input_path, schema_path, output_yaml_path):\n",
    "    \"\"\"\n",
    "    Process an instance by loading a minimal JSON instance, converting it, dumping it to YAML,\n",
    "    validating it against a schema, and loading it with YAMLLoader.\n",
    "\n",
    "    Steps:\n",
    "      1. Load a minimal JSON instance containing only the administrativeInformation section.\n",
    "      2. Convert OrderedDicts to dicts.\n",
    "      3. Wrap the data in a top-level AdministrativeInformation structure.\n",
    "      4. Dump the resulting instance to a YAML file.\n",
    "      5. Validate the instance using a schema validator.\n",
    "      6. Load the YAML instance with YAMLLoader.\n",
    "\n",
    "    Parameters:\n",
    "        input_path (str): Path to the input JSON file.\n",
    "        schema_path (str): Path to the YAML schema file.\n",
    "        output_yaml_path (str): Path where the output YAML file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during validation or instance loading.\n",
    "    \"\"\"\n",
    "    # Load the minimal JSON instance (containing only administrativeInformation)\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    data = odict_to_dict(data)\n",
    "\n",
    "    # Build the nested instance structure.\n",
    "    instance_data = build_minimal_epd_structure(data)\n",
    "\n",
    "    # Dump to YAML\n",
    "    with open(output_yaml_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        yaml.dump(instance_data, out, sort_keys=False, allow_unicode=True)\n",
    "    print(f\"administrativeInformation EPD YAML instance written to: {output_yaml_path}\")\n",
    "\n",
    "    # Validate using the Validator\n",
    "    try:\n",
    "        validator = Validator(schema_path, strict=False)\n",
    "        report = validator.validate(instance_data, \"AdministrativeInformation\")\n",
    "        if report.results:\n",
    "            print(\"Validation errors:\")\n",
    "            for result in report.results:\n",
    "                print(result.message)\n",
    "        else:\n",
    "            print(\"The AdministrativeInformation instance is valid!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during validation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Load the YAML instance using YAMLLoader\n",
    "    try:\n",
    "        loader = YAMLLoader()\n",
    "        # If the instance is wrapped in a \"administrativeInformation\" key, unwrap it.\n",
    "        if \"administrativeInformation\" in instance_data:\n",
    "            instance_data = instance_data[\"administrativeInformation\"]\n",
    "        loader.load(instance_data, target_class=AdministrativeInformation)\n",
    "        print(\"Instance loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading via YAMLLoader:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Usage\n",
    "input_file = \"../linkml/data/json/instance_administrativeInformation.json\"\n",
    "schema_file = \"../linkml/data/yaml/linkml_administrativeInformation_schema.yaml\"\n",
    "output_yaml = \"../linkml/data/yaml/linkml_administrativeInformation_instance.yaml\"\n",
    "\n",
    "process_instance(input_file, schema_file, output_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance loaded from: ../linkml/data/yaml/linkml_administrativeInformation_instance.yaml\n",
      "SchemaView created successfully.\n",
      "RDF (Turtle) successfully written to: ../linkml/data/rdf/linkml_administrativeInformation_instance.ttl\n"
     ]
    }
   ],
   "source": [
    "# Loading and RDF (JSON-LD, Turtle) Generation administrativeInformation EPD YAML Instance\n",
    "\n",
    "import yaml\n",
    "from linkml_runtime.dumpers import RDFLibDumper\n",
    "from linkml_runtime.loaders import YAMLLoader\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "from data.py.linkml_administrativeInformation_schema import AdministrativeInformation\n",
    "\n",
    "\n",
    "def generate_rdf(\n",
    "    schema_path: str,\n",
    "    instance_path: str,\n",
    "    output_jsonld_path: str,\n",
    "    output_turtle_path: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load a validated LinkML YAML instance, create a SchemaView, and dump the instance to RDF in JSON-LD and Turtle formats.\n",
    "\n",
    "    Steps:\n",
    "        1. Load the instance from a YAML file.\n",
    "        2. Create a SchemaView from the schema file.\n",
    "        3. Generate RDF in JSON-LD and Turtle formats.\n",
    "        4. Write the RDF to files.\n",
    "\n",
    "    Parameters:\n",
    "        schema_path (str): Path to the LinkML YAML schema file.\n",
    "        instance_path (str): Path to the validated LinkML YAML instance file.\n",
    "        output_jsonld_path (str): Path where the output JSON-LD file will be written.\n",
    "        output_turtle_path (str): Path where the output Turtle file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during instance loading, SchemaView creation, or RDF generation.\n",
    "    \"\"\"\n",
    "    # Load instance from file\n",
    "    try:\n",
    "        with open(instance_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            loaded_data = yaml.safe_load(f)  # This is now a Python dict\n",
    "        # Extract \"administrativeInformation\"\n",
    "        if \"administrativeInformation\" in loaded_data:\n",
    "            loaded_data = loaded_data[\"administrativeInformation\"]\n",
    "        # Feed loaded_data dictionary into the loader\n",
    "        instance_obj = YAMLLoader().load(\n",
    "            loaded_data, target_class=AdministrativeInformation\n",
    "        )\n",
    "        print(f\"Instance loaded from: {instance_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Create SchemaView\n",
    "    try:\n",
    "        sv = SchemaView(schema_path)\n",
    "        print(\"SchemaView created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during SchemaView creation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Generate and write RDF (JSON-LD, Turtle)\n",
    "    try:\n",
    "        dumper = RDFLibDumper()\n",
    "        # JSON‑LD format\n",
    "        # rdf_jsonld = dumper.dumps(instance_obj, schemaview=sv, fmt=\"json-ld\")\n",
    "        # with open(output_jsonld_path, \"w\", encoding=\"utf-8\") as rdf_file:\n",
    "        #     rdf_file.write(rdf_jsonld)\n",
    "        # print(f\"RDF (JSON‑LD) successfully written to: {output_jsonld_path}\")\n",
    "        # Turtle format\n",
    "        rdf_turtle = dumper.dumps(instance_obj, schemaview=sv, fmt=\"turtle\")\n",
    "        with open(output_turtle_path, \"w\", encoding=\"utf-8\") as ttl_file:\n",
    "            ttl_file.write(rdf_turtle)\n",
    "        print(f\"RDF (Turtle) successfully written to: {output_turtle_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during RDF generation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "schema_path = \"../linkml/data/yaml/linkml_administrativeInformation_schema.yaml\"\n",
    "instance_path = \"../linkml/data/yaml/linkml_administrativeInformation_instance.yaml\"\n",
    "output_jsonld_path = (\n",
    "    \"../linkml/data/rdf/linkml_administrativeInformation_instance.jsonld\"\n",
    ")\n",
    "output_turtle_path = \"../linkml/data/rdf/linkml_administrativeInformation_instance.ttl\"\n",
    "\n",
    "generate_rdf(schema_path, instance_path, output_jsonld_path, output_turtle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate only the `exchanges` part of the Schema and JSON Instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python model generated and written to ../linkml/data/py/linkml_shared_definitions.py.\n"
     ]
    }
   ],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "sd_yaml_schema = \"../linkml/data/yaml/linkml_shared_definitions.yaml\"\n",
    "sd_python_file = \"../linkml/data/py/linkml_shared_definitions.py\"\n",
    "\n",
    "# Define an import map so that types like \"String\", \"Boolean\", etc.\n",
    "# are resolved to their LinkML runtime equivalents.\n",
    "import_map = {\n",
    "    \"Boolean\": \"linkml_runtime.linkml_model.types.Boolean\",\n",
    "    \"String\": \"linkml_runtime.linkml_model.types.String\",\n",
    "    \"Integer\": \"linkml_runtime.linkml_model.types.Integer\",\n",
    "    \"Float\": \"linkml_runtime.linkml_model.types.Float\",\n",
    "    \"dateTime\": \"linkml_runtime.linkml_model.types.dateTime\",\n",
    "    # \"Version\": \"linkml_runtime.linkml_model.types.Version\", # Not in LinkML runtime\n",
    "    # \"anyURI\": \"linkml_runtime.linkml_model.types.anyURI\"  # Not in LinkML runtime\n",
    "}\n",
    "\n",
    "generator = PythonGenerator(\n",
    "    schema=sd_yaml_schema,\n",
    "    gen_slots=True,\n",
    "    gen_classvars=True,\n",
    "    mergeimports=True,\n",
    "    metadata=True,\n",
    "    genmeta=False,\n",
    "    importmap=import_map,\n",
    ")\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "\n",
    "with open(sd_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {sd_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python model generated and written to ../linkml/data/py/linkml_exchanges_schema.py.\n"
     ]
    }
   ],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "ex_yaml_schema = \"../linkml/data/yaml/linkml_exchanges_schema.yaml\"\n",
    "ex_python_file = \"../linkml/data/py/linkml_exchanges_schema.py\"\n",
    "\n",
    "# Define an import map so that types like \"String\", \"Boolean\", etc.\n",
    "# are resolved to their LinkML runtime equivalents.\n",
    "import_map = {\n",
    "    \"Boolean\": \"linkml_runtime.linkml_model.types.Boolean\",\n",
    "    \"String\": \"linkml_runtime.linkml_model.types.String\",\n",
    "    \"Integer\": \"linkml_runtime.linkml_model.types.Integer\",\n",
    "    \"Float\": \"linkml_runtime.linkml_model.types.Float\",\n",
    "    \"dateTime\": \"linkml_runtime.linkml_model.types.dateTime\",\n",
    "    # \"Version\": \"linkml_runtime.linkml_model.types.Version\", # Not in LinkML runtime\n",
    "    # \"anyURI\": \"linkml_runtime.linkml_model.types.anyURI\"  # Not in LinkML runtime\n",
    "}\n",
    "\n",
    "generator = PythonGenerator(\n",
    "    schema=ex_yaml_schema,\n",
    "    gen_slots=True,\n",
    "    gen_classvars=True,\n",
    "    mergeimports=True,\n",
    "    metadata=True,\n",
    "    genmeta=False,\n",
    "    importmap=import_map,\n",
    ")\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "with open(ex_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {ex_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exchanges EPD YAML instance written to: ../linkml/data/yaml/linkml_exchanges_instance.yaml\n",
      "The Exchanges instance is valid!\n",
      "Instance loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Instance Generation and Loading exchanges EPD YAML\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "from data.py.linkml_exchanges_schema import Exchanges\n",
    "from linkml.validator import Validator\n",
    "from linkml_runtime.loaders.yaml_loader import YAMLLoader\n",
    "\n",
    "\n",
    "def odict_to_dict(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert an OrderedDict (or list/dict containing OrderedDicts) to a standard dict.\n",
    "\n",
    "    Parameters:\n",
    "        obj: An OrderedDict, list, or dict potentially containing OrderedDicts.\n",
    "\n",
    "    Returns:\n",
    "        A new structure where all OrderedDict instances are replaced with standard dicts.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, OrderedDict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [odict_to_dict(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def build_minimal_epd_structure(data):\n",
    "    \"\"\"\n",
    "    Build a minimal nested dictionary according to our minimal schema.\n",
    "    This function assumes that the JSON instance already contains a \"exchanges\" key.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): A dictionary loaded from a JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary structured with a top-level 'exchanges' key.\n",
    "    \"\"\"\n",
    "    instance_dict = {\"exchanges\": data.get(\"exchanges\", {})}\n",
    "    return instance_dict\n",
    "\n",
    "\n",
    "def process_instance(input_path, schema_path, output_yaml_path):\n",
    "    \"\"\"\n",
    "    Process an instance by loading a minimal JSON instance, converting it, dumping it to YAML,\n",
    "    validating it against a schema, and loading it with YAMLLoader.\n",
    "\n",
    "    Steps:\n",
    "      1. Load a minimal JSON instance containing only the exchanges section.\n",
    "      2. Convert OrderedDicts to dicts.\n",
    "      3. Wrap the data in a top-level Exchanges structure.\n",
    "      4. Dump the resulting instance to a YAML file.\n",
    "      5. Validate the instance using a schema validator.\n",
    "      6. Load the YAML instance with YAMLLoader.\n",
    "\n",
    "    Parameters:\n",
    "        input_path (str): Path to the input JSON file.\n",
    "        schema_path (str): Path to the YAML schema file.\n",
    "        output_yaml_path (str): Path where the output YAML file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during validation or instance loading.\n",
    "    \"\"\"\n",
    "    # Load the minimal JSON instance (containing only exchanges)\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    data = odict_to_dict(data)\n",
    "\n",
    "    # Build the nested instance structure.\n",
    "    instance_data = build_minimal_epd_structure(data)\n",
    "\n",
    "    # Dump to YAML\n",
    "    with open(output_yaml_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        yaml.dump(instance_data, out, sort_keys=False, allow_unicode=True)\n",
    "    print(f\"exchanges EPD YAML instance written to: {output_yaml_path}\")\n",
    "\n",
    "    # Validate using the Validator\n",
    "    try:\n",
    "        validator = Validator(schema_path, strict=False)\n",
    "        report = validator.validate(instance_data, \"Exchanges\")\n",
    "        if report.results:\n",
    "            print(\"Validation errors:\")\n",
    "            for result in report.results:\n",
    "                print(result.message)\n",
    "        else:\n",
    "            print(\"The Exchanges instance is valid!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during validation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Load the YAML instance using YAMLLoader\n",
    "    try:\n",
    "        loader = YAMLLoader()\n",
    "        # If the instance is wrapped in a \"exchanges\" key, unwrap it.\n",
    "        if \"exchanges\" in instance_data:\n",
    "            instance_data = instance_data[\"exchanges\"]\n",
    "        loader.load(instance_data, target_class=Exchanges)\n",
    "        print(\"Instance loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading via YAMLLoader:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Usage\n",
    "input_file = \"../linkml/data/json/instance_exchanges.json\"\n",
    "schema_file = \"../linkml/data/yaml/linkml_exchanges_schema.yaml\"\n",
    "output_yaml = \"../linkml/data/yaml/linkml_exchanges_instance.yaml\"\n",
    "\n",
    "process_instance(input_file, schema_file, output_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance loaded from: ../linkml/data/yaml/linkml_exchanges_instance.yaml\n",
      "SchemaView created successfully.\n",
      "RDF (Turtle) successfully written to: ../linkml/data/rdf/linkml_exchanges_instance.ttl\n"
     ]
    }
   ],
   "source": [
    "# Loading and RDF (JSON-LD, Turtle) Generation exchanges EPD YAML Instance\n",
    "\n",
    "import yaml\n",
    "from linkml_runtime.dumpers import RDFLibDumper\n",
    "from linkml_runtime.loaders import YAMLLoader\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "from data.py.linkml_exchanges_schema import Exchanges\n",
    "\n",
    "\n",
    "def generate_rdf(\n",
    "    schema_path: str,\n",
    "    instance_path: str,\n",
    "    output_jsonld_path: str,\n",
    "    output_turtle_path: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load a validated LinkML YAML instance, create a SchemaView, and dump the instance to RDF in JSON-LD and Turtle formats.\n",
    "\n",
    "    Steps:\n",
    "        1. Load the instance from a YAML file.\n",
    "        2. Create a SchemaView from the schema file.\n",
    "        3. Generate RDF in JSON-LD and Turtle formats.\n",
    "        4. Write the RDF to files.\n",
    "\n",
    "    Parameters:\n",
    "        schema_path (str): Path to the LinkML YAML schema file.\n",
    "        instance_path (str): Path to the validated LinkML YAML instance file.\n",
    "        output_jsonld_path (str): Path where the output JSON-LD file will be written.\n",
    "        output_turtle_path (str): Path where the output Turtle file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during instance loading, SchemaView creation, or RDF generation.\n",
    "    \"\"\"\n",
    "    # Load instance from file\n",
    "    try:\n",
    "        with open(instance_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            loaded_data = yaml.safe_load(f)  # This is now a Python dict\n",
    "        # Extract \"exchanges\"\n",
    "        if \"exchanges\" in loaded_data:\n",
    "            loaded_data = loaded_data[\"exchanges\"]\n",
    "        # Feed loaded_data dictionary into the loader\n",
    "        instance_obj = YAMLLoader().load(loaded_data, target_class=Exchanges)\n",
    "        print(f\"Instance loaded from: {instance_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Create SchemaView\n",
    "    try:\n",
    "        sv = SchemaView(schema_path)\n",
    "        print(\"SchemaView created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during SchemaView creation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Generate and write RDF (JSON-LD, Turtle)\n",
    "    try:\n",
    "        dumper = RDFLibDumper()\n",
    "        # JSON‑LD format\n",
    "        # rdf_jsonld = dumper.dumps(instance_obj, schemaview=sv, fmt=\"json-ld\")\n",
    "        # with open(output_jsonld_path, \"w\", encoding=\"utf-8\") as rdf_file:\n",
    "        #     rdf_file.write(rdf_jsonld)\n",
    "        # print(f\"RDF (JSON‑LD) successfully written to: {output_jsonld_path}\")\n",
    "        # Turtle format\n",
    "        rdf_turtle = dumper.dumps(instance_obj, schemaview=sv, fmt=\"turtle\")\n",
    "        with open(output_turtle_path, \"w\", encoding=\"utf-8\") as ttl_file:\n",
    "            ttl_file.write(rdf_turtle)\n",
    "        print(f\"RDF (Turtle) successfully written to: {output_turtle_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during RDF generation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "schema_path = \"../linkml/data/yaml/linkml_exchanges_schema.yaml\"\n",
    "instance_path = \"../linkml/data/yaml/linkml_exchanges_instance.yaml\"\n",
    "output_jsonld_path = \"../linkml/data/rdf/linkml_exchanges_instance.jsonld\"\n",
    "output_turtle_path = \"../linkml/data/rdf/linkml_exchanges_instance.ttl\"\n",
    "\n",
    "generate_rdf(schema_path, instance_path, output_jsonld_path, output_turtle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate only the `LCIAResults` part of the Schema and JSON Instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python model generated and written to ../linkml/data/py/linkml_shared_definitions.py.\n"
     ]
    }
   ],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "sd_yaml_schema = \"../linkml/data/yaml/linkml_shared_definitions.yaml\"\n",
    "sd_python_file = \"../linkml/data/py/linkml_shared_definitions.py\"\n",
    "\n",
    "# Define an import map so that types like \"String\", \"Boolean\", etc.\n",
    "# are resolved to their LinkML runtime equivalents.\n",
    "import_map = {\n",
    "    \"Boolean\": \"linkml_runtime.linkml_model.types.Boolean\",\n",
    "    \"String\": \"linkml_runtime.linkml_model.types.String\",\n",
    "    \"Integer\": \"linkml_runtime.linkml_model.types.Integer\",\n",
    "    \"Float\": \"linkml_runtime.linkml_model.types.Float\",\n",
    "    \"dateTime\": \"linkml_runtime.linkml_model.types.dateTime\",\n",
    "    # \"Version\": \"linkml_runtime.linkml_model.types.Version\", # Not in LinkML runtime\n",
    "    # \"anyURI\": \"linkml_runtime.linkml_model.types.anyURI\"  # Not in LinkML runtime\n",
    "}\n",
    "\n",
    "generator = PythonGenerator(\n",
    "    schema=sd_yaml_schema,\n",
    "    gen_slots=True,\n",
    "    gen_classvars=True,\n",
    "    mergeimports=True,\n",
    "    metadata=True,\n",
    "    genmeta=False,\n",
    "    importmap=import_map,\n",
    ")\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "\n",
    "with open(sd_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {sd_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python model generated and written to ../linkml/data/py/linkml_lciaResults_schema.py.\n"
     ]
    }
   ],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "ai_yaml_schema = \"../linkml/data/yaml/linkml_lciaResults_schema.yaml\"\n",
    "ai_python_file = \"../linkml/data/py/linkml_lciaResults_schema.py\"\n",
    "\n",
    "# Define an import map so that types like \"String\", \"Boolean\", etc.\n",
    "# are resolved to their LinkML runtime equivalents.\n",
    "import_map = {\n",
    "    \"Boolean\": \"linkml_runtime.linkml_model.types.Boolean\",\n",
    "    \"String\": \"linkml_runtime.linkml_model.types.String\",\n",
    "    \"Integer\": \"linkml_runtime.linkml_model.types.Integer\",\n",
    "    \"Float\": \"linkml_runtime.linkml_model.types.Float\",\n",
    "    \"dateTime\": \"linkml_runtime.linkml_model.types.dateTime\",\n",
    "    # \"Version\": \"linkml_runtime.linkml_model.types.Version\", # Not in LinkML runtime\n",
    "    # \"anyURI\": \"linkml_runtime.linkml_model.types.anyURI\"  # Not in LinkML runtime\n",
    "}\n",
    "\n",
    "generator = PythonGenerator(\n",
    "    schema=ai_yaml_schema,\n",
    "    gen_slots=True,\n",
    "    gen_classvars=True,\n",
    "    mergeimports=True,\n",
    "    metadata=True,\n",
    "    genmeta=False,\n",
    "    importmap=import_map,\n",
    ")\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "with open(ai_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {ai_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCIAResults EPD YAML instance written to: ../linkml/data/yaml/linkml_LCIAResults_instance.yaml\n",
      "The LCIAResults instance is valid!\n",
      "Instance loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Instance Generation and Loading LCIAResults EPD YAML\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "from data.py.linkml_lciaResults_schema import LCIAResults\n",
    "from linkml.validator import Validator\n",
    "from linkml_runtime.loaders.yaml_loader import YAMLLoader\n",
    "\n",
    "\n",
    "def odict_to_dict(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert an OrderedDict (or list/dict containing OrderedDicts) to a standard dict.\n",
    "\n",
    "    Parameters:\n",
    "        obj: An OrderedDict, list, or dict potentially containing OrderedDicts.\n",
    "\n",
    "    Returns:\n",
    "        A new structure where all OrderedDict instances are replaced with standard dicts.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, OrderedDict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [odict_to_dict(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def build_minimal_epd_structure(data):\n",
    "    \"\"\"\n",
    "    Build a minimal nested dictionary according to our minimal schema.\n",
    "    This function assumes that the JSON instance already contains a \"LCIAResults\" key.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): A dictionary loaded from a JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary structured with a top-level 'LCIAResults' key.\n",
    "    \"\"\"\n",
    "    instance_dict = {\"LCIAResults\": data.get(\"LCIAResults\", {})}\n",
    "    return instance_dict\n",
    "\n",
    "\n",
    "def process_instance(input_path, schema_path, output_yaml_path):\n",
    "    \"\"\"\n",
    "    Process an instance by loading a minimal JSON instance, converting it, dumping it to YAML,\n",
    "    validating it against a schema, and loading it with YAMLLoader.\n",
    "\n",
    "    Steps:\n",
    "      1. Load a minimal JSON instance containing only the LCIAResults section.\n",
    "      2. Convert OrderedDicts to dicts.\n",
    "      3. Wrap the data in a top-level LCIAResults structure.\n",
    "      4. Dump the resulting instance to a YAML file.\n",
    "      5. Validate the instance using a schema validator.\n",
    "      6. Load the YAML instance with YAMLLoader.\n",
    "\n",
    "    Parameters:\n",
    "        input_path (str): Path to the input JSON file.\n",
    "        schema_path (str): Path to the YAML schema file.\n",
    "        output_yaml_path (str): Path where the output YAML file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during validation or instance loading.\n",
    "    \"\"\"\n",
    "    # Load the minimal JSON instance (containing only LCIAResults)\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    data = odict_to_dict(data)\n",
    "\n",
    "    # Build the nested instance structure.\n",
    "    instance_data = build_minimal_epd_structure(data)\n",
    "\n",
    "    # Dump to YAML\n",
    "    with open(output_yaml_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        yaml.dump(instance_data, out, sort_keys=False, allow_unicode=True)\n",
    "    print(f\"LCIAResults EPD YAML instance written to: {output_yaml_path}\")\n",
    "\n",
    "    # Validate using the Validator\n",
    "    try:\n",
    "        validator = Validator(schema_path, strict=False)\n",
    "        report = validator.validate(instance_data, \"LCIAResults\")\n",
    "        if report.results:\n",
    "            print(\"Validation errors:\")\n",
    "            for result in report.results:\n",
    "                print(result.message)\n",
    "        else:\n",
    "            print(\"The LCIAResults instance is valid!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during validation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Load the YAML instance using YAMLLoader\n",
    "    try:\n",
    "        loader = YAMLLoader()\n",
    "        # If the instance is wrapped in a \"LCIAResults\" key, unwrap it.\n",
    "        if \"LCIAResults\" in instance_data:\n",
    "            instance_data = instance_data[\"LCIAResults\"]\n",
    "        loader.load(instance_data, target_class=LCIAResults)\n",
    "        print(\"Instance loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading via YAMLLoader:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Usage\n",
    "input_file = \"../linkml/data/json/instance_LCIAResults.json\"\n",
    "schema_file = \"../linkml/data/yaml/linkml_LCIAResults_schema.yaml\"\n",
    "output_yaml = \"../linkml/data/yaml/linkml_LCIAResults_instance.yaml\"\n",
    "\n",
    "process_instance(input_file, schema_file, output_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance loaded from: ../linkml/data/yaml/linkml_LCIAResults_instance.yaml\n",
      "SchemaView created successfully.\n",
      "RDF (Turtle) successfully written to: ../linkml/data/rdf/linkml_LCIAResults_instance.ttl\n"
     ]
    }
   ],
   "source": [
    "# Loading and RDF (JSON-LD, Turtle) Generation LCIAResults EPD YAML Instance\n",
    "\n",
    "import yaml\n",
    "from linkml_runtime.dumpers import RDFLibDumper\n",
    "from linkml_runtime.loaders import YAMLLoader\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "from data.py.linkml_lciaResults_schema import LCIAResults\n",
    "\n",
    "\n",
    "def generate_rdf(\n",
    "    schema_path: str,\n",
    "    instance_path: str,\n",
    "    output_jsonld_path: str,\n",
    "    output_turtle_path: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load a validated LinkML YAML instance, create a SchemaView, and dump the instance to RDF in JSON-LD and Turtle formats.\n",
    "\n",
    "    Steps:\n",
    "        1. Load the instance from a YAML file.\n",
    "        2. Create a SchemaView from the schema file.\n",
    "        3. Generate RDF in JSON-LD and Turtle formats.\n",
    "        4. Write the RDF to files.\n",
    "\n",
    "    Parameters:\n",
    "        schema_path (str): Path to the LinkML YAML schema file.\n",
    "        instance_path (str): Path to the validated LinkML YAML instance file.\n",
    "        output_jsonld_path (str): Path where the output JSON-LD file will be written.\n",
    "        output_turtle_path (str): Path where the output Turtle file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during instance loading, SchemaView creation, or RDF generation.\n",
    "    \"\"\"\n",
    "    # Load instance from file\n",
    "    try:\n",
    "        with open(instance_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            loaded_data = yaml.safe_load(f)  # This is now a Python dict\n",
    "        # Extract \"LCIAResults\"\n",
    "        if \"LCIAResults\" in loaded_data:\n",
    "            loaded_data = loaded_data[\"LCIAResults\"]\n",
    "        # Feed loaded_data dictionary into the loader\n",
    "        instance_obj = YAMLLoader().load(loaded_data, target_class=LCIAResults)\n",
    "        print(f\"Instance loaded from: {instance_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Create SchemaView\n",
    "    try:\n",
    "        sv = SchemaView(schema_path)\n",
    "        print(\"SchemaView created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during SchemaView creation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Generate and write RDF (JSON-LD, Turtle)\n",
    "    try:\n",
    "        dumper = RDFLibDumper()\n",
    "        # # JSON‑LD format\n",
    "        # rdf_jsonld = dumper.dumps(instance_obj, schemaview=sv, fmt=\"json-ld\")\n",
    "        # with open(output_jsonld_path, \"w\", encoding=\"utf-8\") as rdf_file:\n",
    "        #     rdf_file.write(rdf_jsonld)\n",
    "        # print(f\"RDF (JSON‑LD) successfully written to: {output_jsonld_path}\")\n",
    "        # Turtle format\n",
    "        rdf_turtle = dumper.dumps(instance_obj, schemaview=sv, fmt=\"turtle\")\n",
    "        with open(output_turtle_path, \"w\", encoding=\"utf-8\") as ttl_file:\n",
    "            ttl_file.write(rdf_turtle)\n",
    "        print(f\"RDF (Turtle) successfully written to: {output_turtle_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during RDF generation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "# Example usage:\n",
    "schema_path = \"../linkml/data/yaml/linkml_LCIAResults_schema.yaml\"\n",
    "instance_path = \"../linkml/data/yaml/linkml_LCIAResults_instance.yaml\"\n",
    "output_jsonld_path = \"../linkml/data/rdf/linkml_LCIAResults_instance.jsonld\"\n",
    "output_turtle_path = \"../linkml/data/rdf/linkml_LCIAResults_instance.ttl\"\n",
    "\n",
    "generate_rdf(schema_path, instance_path, output_jsonld_path, output_turtle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate only the `otherAttributes` part of the Schema and JSON Instance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "sd_yaml_schema = \"../linkml/data/yaml/linkml_shared_definitions.yaml\"\n",
    "sd_python_file = \"../linkml/data/py/linkml_shared_definitions.py\"\n",
    "\n",
    "# Define an import map so that types like \"String\", \"Boolean\", etc.\n",
    "# are resolved to their LinkML runtime equivalents.\n",
    "import_map = {\n",
    "    \"Boolean\": \"linkml_runtime.linkml_model.types.Boolean\",\n",
    "    \"String\": \"linkml_runtime.linkml_model.types.String\",\n",
    "    \"Integer\": \"linkml_runtime.linkml_model.types.Integer\",\n",
    "    \"Float\": \"linkml_runtime.linkml_model.types.Float\",\n",
    "    \"dateTime\": \"linkml_runtime.linkml_model.types.dateTime\",\n",
    "    # \"Version\": \"linkml_runtime.linkml_model.types.Version\", # Not in LinkML runtime\n",
    "    # \"anyURI\": \"linkml_runtime.linkml_model.types.anyURI\"  # Not in LinkML runtime\n",
    "}\n",
    "\n",
    "generator = PythonGenerator(\n",
    "    schema=sd_yaml_schema,\n",
    "    gen_slots=True,\n",
    "    gen_classvars=True,\n",
    "    mergeimports=True,\n",
    "    metadata=True,\n",
    "    genmeta=False,\n",
    "    importmap=import_map,\n",
    ")\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "\n",
    "with open(sd_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {sd_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m ai_python_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../linkml/data/py/linkml_otherAttributes_schema.py\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m generator \u001b[38;5;241m=\u001b[39m PythonGenerator(schema\u001b[38;5;241m=\u001b[39mai_yaml_schema,)\n\u001b[1;32m---> 10\u001b[0m python_code \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(ai_python_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     13\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(python_code)\n",
      "File \u001b[1;32mc:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml\\utils\\generator.py:334\u001b[0m, in \u001b[0;36mGenerator.serialize\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sub_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m             out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sub_out\n\u001b[1;32m--> 334\u001b[0m sub_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sub_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    336\u001b[0m     out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sub_out\n",
      "File \u001b[1;32mc:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml\\generators\\pythongen.py:289\u001b[0m, in \u001b[0;36mPythonGenerator.end_schema\u001b[1;34m(self, **_)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mend_schema\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m +\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml\\generators\\pythongen.py:280\u001b[0m, in \u001b[0;36mPythonGenerator.gen_schema\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m             split_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m#   \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mdescription\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    248\u001b[0m         head \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m# Auto generated from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39msource_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneratorname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneratorversion\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124m# Generation date: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mgeneration_date\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m         )\n\u001b[0;32m    257\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhead\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124m# id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;124m# description: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_description\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124m# license: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mlicense)\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \n\u001b[0;32m    262\u001b[0m \u001b[38;5;132;01m{\u001b[39;00mall_imports\u001b[38;5;241m.\u001b[39mrender()\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_imports()\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \n\u001b[0;32m    265\u001b[0m \u001b[38;5;124mmetamodel_version = \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mmetamodel_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;124mversion = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    267\u001b[0m \n\u001b[0;32m    268\u001b[0m \u001b[38;5;124m# Overwrite dataclasses _init_fn to add **kwargs in __init__\u001b[39m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;124mdataclasses._init_fn = dataclasses_init_fn_with_kwargs\u001b[39m\n\u001b[0;32m    270\u001b[0m \n\u001b[0;32m    271\u001b[0m \u001b[38;5;124m# Namespaces\u001b[39m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_namespaces()\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \n\u001b[0;32m    274\u001b[0m \n\u001b[0;32m    275\u001b[0m \u001b[38;5;124m# Types\u001b[39m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_typedefs()\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124m# Class references\u001b[39m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_references()\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \n\u001b[1;32m--> 280\u001b[0m \u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_classdefs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \n\u001b[0;32m    282\u001b[0m \u001b[38;5;124m# Enumerations\u001b[39m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_enumerations()\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \n\u001b[0;32m    285\u001b[0m \u001b[38;5;124m# Slots\u001b[39m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_slotdefs()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml\\generators\\pythongen.py:480\u001b[0m, in \u001b[0;36mPythonGenerator.gen_classdefs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create class definitions for all non-mixin classes in the model\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;124;03mNote that apply_to classes are transformed to mixins\u001b[39;00m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    479\u001b[0m clist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_classes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m.\u001b[39mclasses\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m--> 480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_classdef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m clist \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mimported_from])\n",
      "File \u001b[1;32mc:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml\\generators\\pythongen.py:487\u001b[0m, in \u001b[0;36mPythonGenerator.gen_classdef\u001b[1;34m(self, cls)\u001b[0m\n\u001b[0;32m    485\u001b[0m parentref \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformatted_element_name(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mis_a,\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mis_a\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYAMLRoot\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    486\u001b[0m slotdefs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_class_variables(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m--> 487\u001b[0m postinits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_postinits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m constructor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen_constructor(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    490\u001b[0m wrapped_description \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mwrapped_annotation(be(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdescription))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m be(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdescription) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    492\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml\\generators\\pythongen.py:753\u001b[0m, in \u001b[0;36mPythonGenerator.gen_postinits\u001b[1;34m(self, cls)\u001b[0m\n\u001b[0;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m slot\u001b[38;5;241m.\u001b[39mrequired:\n\u001b[0;32m    751\u001b[0m         \u001b[38;5;66;03m# TODO: Remove the bypass whenever we get default_range fixed\u001b[39;00m\n\u001b[0;32m    752\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m slot\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pkeys \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m slot\u001b[38;5;241m.\u001b[39mifabsent \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 753\u001b[0m             post_inits\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_postinit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslot\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    754\u001b[0m post_inits_designators \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    756\u001b[0m domain_slot_names \u001b[38;5;241m=\u001b[39m [s\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain_slots(\u001b[38;5;28mcls\u001b[39m)]\n",
      "File \u001b[1;32mc:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml\\generators\\pythongen.py:950\u001b[0m, in \u001b[0;36mPythonGenerator.gen_postinit\u001b[1;34m(self, cls, slot)\u001b[0m\n\u001b[0;32m    948\u001b[0m             rlines\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mself.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maliased_slot_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_type_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(**as_dict(self.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maliased_slot_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m))\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slot\u001b[38;5;241m.\u001b[39minlined:\n\u001b[1;32m--> 950\u001b[0m     slot_range_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m[\u001b[49m\u001b[43mslot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrange\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    951\u001b[0m     identifier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_identifier(slot_range_cls)\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;66;03m# If we don't have an identifier, and we are expecting to be inlined first class elements\u001b[39;00m\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;66;03m# (inlined_as_list is not True), we will use the first required field as the key.\u001b[39;00m\n\u001b[0;32m    954\u001b[0m     \u001b[38;5;66;03m#  Note that this may not always work, but the workaround is straight forward -- set inlined_as_list to\u001b[39;00m\n\u001b[0;32m    955\u001b[0m     \u001b[38;5;66;03m#  True\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'string'"
     ]
    }
   ],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "ai_yaml_schema = \"../linkml/data/yaml/linkml_otherAttributes_schema.yaml\"\n",
    "ai_python_file = \"../linkml/data/py/linkml_otherAttributes_schema.py\"\n",
    "\n",
    "generator = PythonGenerator(schema=ai_yaml_schema,)\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "with open(ai_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {ai_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otherAttributes EPD YAML instance written to: ../linkml/data/yaml/linkml_otherAttributes_instance.yaml\n",
      "Exception during validation:\n",
      " Unknown argument: key_name = True\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": " Unknown argument: key_name = True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 117\u001b[0m\n\u001b[0;32m    114\u001b[0m schema_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../linkml/data/yaml/linkml_otherAttributes_schema.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    115\u001b[0m output_yaml \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../linkml/data/yaml/linkml_otherAttributes_instance.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 117\u001b[0m \u001b[43mprocess_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_yaml\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 85\u001b[0m, in \u001b[0;36mprocess_instance\u001b[1;34m(input_path, schema_path, output_yaml_path)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Validate using the Validator\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     validator \u001b[38;5;241m=\u001b[39m \u001b[43mValidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     report \u001b[38;5;241m=\u001b[39m validator\u001b[38;5;241m.\u001b[39mvalidate(instance_data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherAttributes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m report\u001b[38;5;241m.\u001b[39mresults:\n",
      "File \u001b[1;32mc:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml\\validator\\validator.py:39\u001b[0m, in \u001b[0;36mValidator.__init__\u001b[1;34m(self, schema, validation_plugins, strict)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema \u001b[38;5;241m=\u001b[39m schema\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema: SchemaDefinition \u001b[38;5;241m=\u001b[39m \u001b[43myaml_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSchemaDefinition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m schema:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema\u001b[38;5;241m.\u001b[39msource_file \u001b[38;5;241m=\u001b[39m schema\n",
      "File \u001b[1;32mc:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\loaders\\loader_root.py:76\u001b[0m, in \u001b[0;36mLoader.load\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[BaseModel, YAMLRoot]:\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    Load source as an instance of target_class\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m    :return: instance of target_class\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_any\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results, BaseModel) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results, YAMLRoot):\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32mc:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\loaders\\yaml_loader.py:42\u001b[0m, in \u001b[0;36mYAMLLoader.load_any\u001b[1;34m(self, source, target_class, base_dir, metadata, **_)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_any\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     37\u001b[0m              source: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m, TextIO],\n\u001b[0;32m     38\u001b[0m              target_class: Union[Type[YAMLRoot], Type[BaseModel]],\n\u001b[0;32m     39\u001b[0m              \u001b[38;5;241m*\u001b[39m, base_dir: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     40\u001b[0m              metadata: Optional[FileInfo] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[YAMLRoot, List[YAMLRoot]]:\n\u001b[0;32m     41\u001b[0m     data_as_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_as_dict(source, base_dir\u001b[38;5;241m=\u001b[39mbase_dir, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_target_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_as_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\loaders\\loader_root.py:137\u001b[0m, in \u001b[0;36mLoader._construct_target_class\u001b[1;34m(self, data_as_dict, target_class)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m target_class\u001b[38;5;241m.\u001b[39mparse_obj(data_as_dict)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata_as_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_as_dict, JsonObj):\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [target_class(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mas_dict(x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data_as_dict]\n",
      "File \u001b[1;32m<string>:68\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, name, id_prefixes, id_prefixes_are_closed, definition_uri, local_names, conforms_to, implements, instantiates, extensions, annotations, description, alt_descriptions, title, deprecated, todos, notes, comments, examples, in_subset, from_schema, imported_from, source, in_language, see_also, deprecated_element_has_exact_replacement, deprecated_element_has_possible_replacement, aliases, structured_aliases, mappings, exact_mappings, close_mappings, related_mappings, narrow_mappings, broad_mappings, created_by, contributors, created_on, last_updated_on, modified_by, status, rank, categories, keywords, id, version, imports, license, prefixes, emit_prefixes, default_curi_maps, default_prefix, default_range, subsets, types, enums, slots, classes, metamodel_version, source_file, source_file_date, source_file_size, generation_date, slot_names_unique, settings, bindings, **_kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\meta.py:600\u001b[0m, in \u001b[0;36mSchemaDefinition.__post_init__\u001b[1;34m(self, *_, **kwargs)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_inlined_as_dict(slot_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtypes\u001b[39m\u001b[38;5;124m\"\u001b[39m, slot_type\u001b[38;5;241m=\u001b[39mTypeDefinition, key_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, keyed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_inlined_as_dict(slot_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menums\u001b[39m\u001b[38;5;124m\"\u001b[39m, slot_type\u001b[38;5;241m=\u001b[39mEnumDefinition, key_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, keyed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 600\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_inlined_as_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslot_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslots\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslot_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSlotDefinition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_inlined_as_dict(slot_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m\"\u001b[39m, slot_type\u001b[38;5;241m=\u001b[39mClassDefinition, key_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, keyed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetamodel_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetamodel_version, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\utils\\yamlutils.py:108\u001b[0m, in \u001b[0;36mYAMLRoot._normalize_inlined_as_dict\u001b[1;34m(self, slot_name, slot_type, key_name, keyed)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_normalize_inlined_as_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, slot_name: \u001b[38;5;28mstr\u001b[39m, slot_type: Type, key_name: \u001b[38;5;28mstr\u001b[39m, keyed: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_inlined\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslot_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslot_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\utils\\yamlutils.py:213\u001b[0m, in \u001b[0;36mYAMLRoot._normalize_inlined\u001b[1;34m(self, slot_name, slot_type, key_name, keyed, is_list)\u001b[0m\n\u001b[0;32m    211\u001b[0m     order_up(k, v)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, (\u001b[38;5;28mdict\u001b[39m, JsonObj)):\n\u001b[1;32m--> 213\u001b[0m     \u001b[43mform_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    215\u001b[0m     order_up(k, slot_type(\u001b[38;5;241m*\u001b[39m[k, v]))\n",
      "File \u001b[1;32mc:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\utils\\yamlutils.py:161\u001b[0m, in \u001b[0;36mYAMLRoot._normalize_inlined.<locals>.form_1\u001b[1;34m(entries)\u001b[0m\n\u001b[0;32m    159\u001b[0m     raw_obj[key_name] \u001b[38;5;241m=\u001b[39m key\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mtype\u001b[39m(raw_obj), slot_type):\n\u001b[1;32m--> 161\u001b[0m     order_up(key, \u001b[43mslot_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mas_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_obj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     order_up(key, raw_obj)\n",
      "File \u001b[1;32m<string>:120\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, name, id_prefixes, id_prefixes_are_closed, definition_uri, local_names, conforms_to, implements, instantiates, extensions, annotations, description, alt_descriptions, title, deprecated, todos, notes, comments, examples, in_subset, from_schema, imported_from, source, in_language, see_also, deprecated_element_has_exact_replacement, deprecated_element_has_possible_replacement, aliases, structured_aliases, mappings, exact_mappings, close_mappings, related_mappings, narrow_mappings, broad_mappings, created_by, contributors, created_on, last_updated_on, modified_by, status, rank, categories, keywords, is_a, abstract, mixin, mixins, apply_to, values_from, string_serialization, singular_name, domain, slot_uri, array, inherited, readonly, ifabsent, list_elements_unique, list_elements_ordered, shared, key, identifier, designates_type, alias, owner, domain_of, subproperty_of, symmetric, reflexive, locally_reflexive, irreflexive, asymmetric, transitive, inverse, is_class_field, transitive_form_of, reflexive_transitive_form_of, role, is_usage_slot, usage_slot_name, relational_role, slot_group, is_grouping_slot, path_rule, disjoint_with, children_are_mutually_disjoint, union_of, type_mappings, range, range_expression, enum_range, bindings, required, recommended, multivalued, inlined, inlined_as_list, minimum_value, maximum_value, pattern, structured_pattern, unit, implicit_prefix, value_presence, equals_string, equals_string_in, equals_number, equals_expression, exact_cardinality, minimum_cardinality, maximum_cardinality, has_member, all_members, none_of, exactly_one_of, any_of, all_of, **_kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\meta.py:2531\u001b[0m, in \u001b[0;36mSlotDefinition.__post_init__\u001b[1;34m(self, *_, **kwargs)\u001b[0m\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_of \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_of] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_of \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m   2529\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_of \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, AnonymousSlotExpression) \u001b[38;5;28;01melse\u001b[39;00m AnonymousSlotExpression(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mas_dict(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_of]\n\u001b[1;32m-> 2531\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__post_init__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\meta.py:863\u001b[0m, in \u001b[0;36mDefinition.__post_init__\u001b[1;34m(self, *_, **kwargs)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstring_serialization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstring_serialization, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstring_serialization \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstring_serialization)\n\u001b[1;32m--> 863\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__post_init__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\meta.py:516\u001b[0m, in \u001b[0;36mElement.__post_init__\u001b[1;34m(self, *_, **kwargs)\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeywords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeywords] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeywords \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeywords \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeywords]\n\u001b[1;32m--> 516\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__post_init__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\utils\\yamlutils.py:56\u001b[0m, in \u001b[0;36mYAMLRoot.__post_init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     54\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrepr\u001b[39m(kwargs[k])[:\u001b[38;5;241m40\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     55\u001b[0m     messages\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTypedNode\u001b[38;5;241m.\u001b[39myaml_loc(k)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Unknown argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(messages))\n",
      "\u001b[1;31mValueError\u001b[0m:  Unknown argument: key_name = True"
     ]
    }
   ],
   "source": [
    "# Instance Generation and Loading otherAttributes EPD YAML\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "from data.py.linkml_otherAttributes_schema import OtherAttributes\n",
    "from linkml.validator import Validator\n",
    "from linkml_runtime.loaders.yaml_loader import YAMLLoader\n",
    "\n",
    "\n",
    "def odict_to_dict(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert an OrderedDict (or list/dict containing OrderedDicts) to a standard dict.\n",
    "\n",
    "    Parameters:\n",
    "        obj: An OrderedDict, list, or dict potentially containing OrderedDicts.\n",
    "\n",
    "    Returns:\n",
    "        A new structure where all OrderedDict instances are replaced with standard dicts.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, OrderedDict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [odict_to_dict(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def build_minimal_epd_structure(data):\n",
    "    \"\"\"\n",
    "    Build a minimal nested dictionary according to our minimal schema.\n",
    "    This function assumes that the JSON instance already contains a \"otherAttributes\" key.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): A dictionary loaded from a JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary structured with a top-level 'otherAttributes' key.\n",
    "    \"\"\"\n",
    "    instance_dict = {\"otherAttributes\": data.get(\"otherAttributes\", {})}\n",
    "    return instance_dict\n",
    "\n",
    "\n",
    "def process_instance(input_path, schema_path, output_yaml_path):\n",
    "    \"\"\"\n",
    "    Process an instance by loading a minimal JSON instance, converting it, dumping it to YAML,\n",
    "    validating it against a schema, and loading it with YAMLLoader.\n",
    "\n",
    "    Steps:\n",
    "      1. Load a minimal JSON instance containing only the otherAttributes section.\n",
    "      2. Convert OrderedDicts to dicts.\n",
    "      3. Wrap the data in a top-level OtherAttributes structure.\n",
    "      4. Dump the resulting instance to a YAML file.\n",
    "      5. Validate the instance using a schema validator.\n",
    "      6. Load the YAML instance with YAMLLoader.\n",
    "\n",
    "    Parameters:\n",
    "        input_path (str): Path to the input JSON file.\n",
    "        schema_path (str): Path to the YAML schema file.\n",
    "        output_yaml_path (str): Path where the output YAML file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during validation or instance loading.\n",
    "    \"\"\"\n",
    "    # Load the minimal JSON instance (containing only otherAttributes)\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    data = odict_to_dict(data)\n",
    "\n",
    "    # Build the nested instance structure.\n",
    "    instance_data = build_minimal_epd_structure(data)\n",
    "\n",
    "    # Dump to YAML\n",
    "    with open(output_yaml_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        yaml.dump(instance_data, out, sort_keys=False, allow_unicode=True)\n",
    "    print(f\"otherAttributes EPD YAML instance written to: {output_yaml_path}\")\n",
    "\n",
    "    # Validate using the Validator\n",
    "    try:\n",
    "        validator = Validator(schema_path, strict=False)\n",
    "        report = validator.validate(instance_data, \"OtherAttributes\")\n",
    "        if report.results:\n",
    "            print(\"Validation errors:\")\n",
    "            for result in report.results:\n",
    "                print(result.message)\n",
    "        else:\n",
    "            print(\"The OtherAttributes instance is valid!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during validation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Load the YAML instance using YAMLLoader\n",
    "    try:\n",
    "        loader = YAMLLoader()\n",
    "        # If the instance is wrapped in a \"otherAttributes\" key, unwrap it.\n",
    "        if \"otherAttributes\" in instance_data:\n",
    "            instance_data = instance_data[\"otherAttributes\"]\n",
    "        loader.load(instance_data, target_class=OtherAttributes)\n",
    "        print(\"Instance loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading via YAMLLoader:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Usage\n",
    "input_file = \"../linkml/data/json/instance_otherAttributes.json\"\n",
    "schema_file = \"../linkml/data/yaml/linkml_otherAttributes_schema.yaml\"\n",
    "output_yaml = \"../linkml/data/yaml/linkml_otherAttributes_instance.yaml\"\n",
    "\n",
    "process_instance(input_file, schema_file, output_yaml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance loaded from: ../linkml/data/yaml/linkml_LCIAResults_instance.yaml\n",
      "SchemaView created successfully.\n",
      "RDF (Turtle) successfully written to: ../linkml/data/rdf/linkml_LCIAResults_instance.ttl\n"
     ]
    }
   ],
   "source": [
    "# Loading and RDF (JSON-LD, Turtle) Generation LCIAResults EPD YAML Instance\n",
    "\n",
    "import yaml\n",
    "from linkml_runtime.dumpers import RDFLibDumper\n",
    "from linkml_runtime.loaders import YAMLLoader\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "from data.py.linkml_LCIAResults_schema import LCIAResults\n",
    "\n",
    "\n",
    "def generate_rdf(\n",
    "    schema_path: str,\n",
    "    instance_path: str,\n",
    "    output_jsonld_path: str,\n",
    "    output_turtle_path: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load a validated LinkML YAML instance, create a SchemaView, and dump the instance to RDF in JSON-LD and Turtle formats.\n",
    "\n",
    "    Steps:\n",
    "        1. Load the instance from a YAML file.\n",
    "        2. Create a SchemaView from the schema file.\n",
    "        3. Generate RDF in JSON-LD and Turtle formats.\n",
    "        4. Write the RDF to files.\n",
    "\n",
    "    Parameters:\n",
    "        schema_path (str): Path to the LinkML YAML schema file.\n",
    "        instance_path (str): Path to the validated LinkML YAML instance file.\n",
    "        output_jsonld_path (str): Path where the output JSON-LD file will be written.\n",
    "        output_turtle_path (str): Path where the output Turtle file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during instance loading, SchemaView creation, or RDF generation.\n",
    "    \"\"\"\n",
    "    # Load instance from file\n",
    "    try:\n",
    "        with open(instance_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            loaded_data = yaml.safe_load(f)  # This is now a Python dict\n",
    "        # Extract \"LCIAResults\"\n",
    "        if \"LCIAResults\" in loaded_data:\n",
    "            loaded_data = loaded_data[\"LCIAResults\"]\n",
    "        # Feed loaded_data dictionary into the loader\n",
    "        instance_obj = YAMLLoader().load(loaded_data, target_class=LCIAResults)\n",
    "        print(f\"Instance loaded from: {instance_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Create SchemaView\n",
    "    try:\n",
    "        sv = SchemaView(schema_path)\n",
    "        print(\"SchemaView created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during SchemaView creation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Generate and write RDF (JSON-LD, Turtle)\n",
    "    try:\n",
    "        dumper = RDFLibDumper()\n",
    "        # # JSON‑LD format\n",
    "        # rdf_jsonld = dumper.dumps(instance_obj, schemaview=sv, fmt=\"json-ld\")\n",
    "        # with open(output_jsonld_path, \"w\", encoding=\"utf-8\") as rdf_file:\n",
    "        #     rdf_file.write(rdf_jsonld)\n",
    "        # print(f\"RDF (JSON‑LD) successfully written to: {output_jsonld_path}\")\n",
    "        # Turtle format\n",
    "        rdf_turtle = dumper.dumps(instance_obj, schemaview=sv, fmt=\"turtle\")\n",
    "        with open(output_turtle_path, \"w\", encoding=\"utf-8\") as ttl_file:\n",
    "            ttl_file.write(rdf_turtle)\n",
    "        print(f\"RDF (Turtle) successfully written to: {output_turtle_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during RDF generation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "# Example usage:\n",
    "schema_path = \"../linkml/data/yaml/linkml_LCIAResults_schema.yaml\"\n",
    "instance_path = \"../linkml/data/yaml/linkml_LCIAResults_instance.yaml\"\n",
    "output_jsonld_path = \"../linkml/data/rdf/linkml_LCIAResults_instance.jsonld\"\n",
    "output_turtle_path = \"../linkml/data/rdf/linkml_LCIAResults_instance.ttl\"\n",
    "\n",
    "generate_rdf(schema_path, instance_path, output_jsonld_path, output_turtle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the whole schema and JSON Instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python model generated and written to ../linkml/data/py/linkml_shared_definitions.py.\n"
     ]
    }
   ],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "sd_yaml_schema = \"../linkml/data/yaml/linkml_shared_definitions.yaml\"\n",
    "sd_python_file = \"../linkml/data/py/linkml_shared_definitions.py\"\n",
    "\n",
    "# Define an import map so that types like \"String\", \"Boolean\", etc.\n",
    "# are resolved to their LinkML runtime equivalents.\n",
    "import_map = {\n",
    "    \"Boolean\": \"linkml_runtime.linkml_model.types.Boolean\",\n",
    "    \"String\": \"linkml_runtime.linkml_model.types.String\",\n",
    "    \"Integer\": \"linkml_runtime.linkml_model.types.Integer\",\n",
    "    \"Float\": \"linkml_runtime.linkml_model.types.Float\",\n",
    "    \"dateTime\": \"linkml_runtime.linkml_model.types.dateTime\",\n",
    "    # \"Version\": \"linkml_runtime.linkml_model.types.Version\", # Not in LinkML runtime\n",
    "    # \"anyURI\": \"linkml_runtime.linkml_model.types.anyURI\"  # Not in LinkML runtime\n",
    "}\n",
    "\n",
    "generator = PythonGenerator(\n",
    "    schema=sd_yaml_schema,\n",
    "    gen_slots=True,\n",
    "    gen_classvars=True,\n",
    "    mergeimports=True,\n",
    "    metadata=True,\n",
    "    genmeta=False,\n",
    "    importmap=import_map,\n",
    ")\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "\n",
    "with open(sd_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {sd_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python model generated and written to ../linkml/data/py/linkml_processDataSet_schema.py.\n"
     ]
    }
   ],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "ai_yaml_schema = \"../linkml/data/yaml/linkml_processDataSet_schema.yaml\"\n",
    "ai_python_file = \"../linkml/data/py/linkml_processDataSet_schema.py\"\n",
    "\n",
    "generator = PythonGenerator(schema=ai_yaml_schema)\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "with open(ai_python_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Python model generated and written to {ai_python_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessDataSet EPD YAML instance written to: ../linkml/data/yaml/linkml_processDataSet_instance.yaml\n",
      "The ProcessDataSet instance is valid!\n",
      "Instance loaded successfully as ProcessDataSet object!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "from data.py.linkml_processDataSet_schema import ProcessDataSet\n",
    "from linkml.validator import Validator\n",
    "from linkml_runtime.loaders.yaml_loader import YAMLLoader\n",
    "\n",
    "def odict_to_dict(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert an OrderedDict (or list/dict containing OrderedDicts) to a standard dict.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, OrderedDict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [odict_to_dict(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def process_instance(input_path, schema_path, output_yaml_path):\n",
    "    \"\"\"\n",
    "    Process an instance by loading a JSON file that is already structured\n",
    "    at the top level for the ProcessDataSet class. Example top-level JSON keys:\n",
    "        {\n",
    "          \"processInformation\": { ... },\n",
    "          \"modellingAndValidation\": { ... },\n",
    "          ...\n",
    "          \"version\": \"1.1\"\n",
    "        }\n",
    "\n",
    "    Steps:\n",
    "      1. Load and parse the JSON directly into a dict.\n",
    "      2. Dump the dict to a YAML file for inspection (optional).\n",
    "      3. Validate the dict against the ProcessDataSet class in your schema.\n",
    "      4. Load the dict as a ProcessDataSet object using YAMLLoader.\n",
    "    \"\"\"\n",
    "    # 1. Load the JSON instance\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # (Optional) Convert OrderedDicts to normal dicts if needed\n",
    "    data = odict_to_dict(data)\n",
    "\n",
    "    # 2. Dump the data to a YAML file (for debugging or pipeline usage)\n",
    "    with open(output_yaml_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        yaml.dump(data, out, sort_keys=False, allow_unicode=True)\n",
    "    print(f\"ProcessDataSet EPD YAML instance written to: {output_yaml_path}\")\n",
    "\n",
    "    # 3. Validate the dict using the LinkML Validator\n",
    "    try:\n",
    "        validator = Validator(schema_path, strict=False)\n",
    "        report = validator.validate(data, \"ProcessDataSet\")\n",
    "        if report.results:\n",
    "            print(\"Validation errors:\")\n",
    "            for result in report.results:\n",
    "                print(result.message)\n",
    "        else:\n",
    "            print(\"The ProcessDataSet instance is valid!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during validation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # 4. Load the dict as a ProcessDataSet object\n",
    "    try:\n",
    "        loader = YAMLLoader()\n",
    "        obj = loader.load(data, target_class=ProcessDataSet)\n",
    "        print(\"Instance loaded successfully as ProcessDataSet object!\")\n",
    "        # You can now use `obj` in your code, e.g. print(obj)\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading via YAMLLoader:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Usage example\n",
    "input_file = \"../data/pipeline2/json/epds/0db12903-1403-4c9a-817e-b48299d17aba_RN_ID.json\"\n",
    "# input_file = \"../linkml/data/json/5b6b44e0-f5e4-451f-54a3-08dcec2f0f89_renamedScript_newID.json\"\n",
    "schema_file = \"../linkml/data/yaml/linkml_processDataSet_schema.yaml\"\n",
    "output_yaml = \"../linkml/data/yaml/linkml_processDataSet_instance.yaml\"\n",
    "\n",
    "process_instance(input_file, schema_file, output_yaml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance loaded from: ../linkml/data/yaml/linkml_processDataSet_instance.yaml\n",
      "SchemaView created successfully.\n",
      "RDF (Turtle) successfully written to: ../linkml/data/rdf/linkml_processDataSet_instance.ttl\n"
     ]
    }
   ],
   "source": [
    "# Loading and RDF (JSON-LD, Turtle) Generation processDataSet EPD YAML Instance\n",
    "\n",
    "import yaml\n",
    "from linkml_runtime.dumpers import RDFLibDumper\n",
    "from linkml_runtime.loaders import YAMLLoader\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "from data.py.linkml_processDataSet_schema import ProcessDataSet\n",
    "\n",
    "\n",
    "def generate_rdf(\n",
    "    schema_path: str,\n",
    "    instance_path: str,\n",
    "    output_jsonld_path: str,\n",
    "    output_turtle_path: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load a validated LinkML YAML instance, create a SchemaView, and dump the instance to RDF in JSON-LD and Turtle formats.\n",
    "\n",
    "    Steps:\n",
    "        1. Load the instance from a YAML file.\n",
    "        2. Create a SchemaView from the schema file.\n",
    "        3. Generate RDF in JSON-LD and Turtle formats.\n",
    "        4. Write the RDF to files.\n",
    "\n",
    "    Parameters:\n",
    "        schema_path (str): Path to the LinkML YAML schema file.\n",
    "        instance_path (str): Path to the validated LinkML YAML instance file.\n",
    "        output_jsonld_path (str): Path where the output JSON-LD file will be written.\n",
    "        output_turtle_path (str): Path where the output Turtle file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during instance loading, SchemaView creation, or RDF generation.\n",
    "    \"\"\"\n",
    "    # Load instance from file\n",
    "    try:\n",
    "        with open(instance_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            loaded_data = yaml.safe_load(f)  # This is now a Python dict\n",
    "        # Extract \"processDataSet\"\n",
    "        if \"processDataSet\" in loaded_data:\n",
    "            loaded_data = loaded_data[\"processDataSet\"]\n",
    "        # Feed loaded_data dictionary into the loader\n",
    "        instance_obj = YAMLLoader().load(loaded_data, target_class=ProcessDataSet)\n",
    "        print(f\"Instance loaded from: {instance_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Create SchemaView\n",
    "    try:\n",
    "        sv = SchemaView(schema_path)\n",
    "        print(\"SchemaView created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during SchemaView creation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Generate and write RDF (JSON-LD, Turtle)\n",
    "    try:\n",
    "        dumper = RDFLibDumper()\n",
    "        # JSON‑LD format\n",
    "        # rdf_jsonld = dumper.dumps(instance_obj, schemaview=sv, fmt=\"json-ld\")\n",
    "        # with open(output_jsonld_path, \"w\", encoding=\"utf-8\") as rdf_file:\n",
    "        #     rdf_file.write(rdf_jsonld)\n",
    "        # print(f\"RDF (JSON‑LD) successfully written to: {output_jsonld_path}\")\n",
    "        # Turtle format\n",
    "        rdf_turtle = dumper.dumps(instance_obj, schemaview=sv, fmt=\"turtle\")\n",
    "        with open(output_turtle_path, \"w\", encoding=\"utf-8\") as ttl_file:\n",
    "            ttl_file.write(rdf_turtle)\n",
    "        print(f\"RDF (Turtle) successfully written to: {output_turtle_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during RDF generation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "schema_path = \"../linkml/data/yaml/linkml_processDataSet_schema.yaml\"\n",
    "instance_path = \"../linkml/data/yaml/linkml_processDataSet_instance.yaml\"\n",
    "output_jsonld_path = \"../linkml/data/rdf/linkml_processDataSet_instance.jsonld\"\n",
    "output_turtle_path = \"../linkml/data/rdf/linkml_processDataSet_instance.ttl\"\n",
    "\n",
    "generate_rdf(schema_path, instance_path, output_jsonld_path, output_turtle_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
