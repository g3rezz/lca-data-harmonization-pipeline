{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Walk upward to find the project-root folder \"simple_rag\"\n",
    "p = Path.cwd().resolve()\n",
    "while p.name != \"simple_rag\" and p.parent != p:\n",
    "    p = p.parent\n",
    "\n",
    "PROJECT_ROOT = p                      # .../simple_rag\n",
    "if str(PROJECT_ROOT) not in sys.path: # avoid duplicates\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "del p, Path, sys                      # keep namespace tidy\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# Do sanity-check\n",
    "print(\"✓ project root on sys.path →\", PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epd_count=10 # Number of instances to convert from every ILCDx Dataset (use 10 for prototype, 1000 for full run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add The International EPD System\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import jsonlines\n",
    "import re\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. PARSE THE XML TO BUILD A CATEGORY LOOKUP STRUCTURE\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_category_dict(xml_element):\n",
    "    category_id = xml_element.get('id')\n",
    "    category_name = xml_element.get('name')\n",
    "    \n",
    "    children_dict = {}\n",
    "    for child_elem in xml_element.findall('{*}category'):\n",
    "        child_data = build_category_dict(child_elem)\n",
    "        children_dict[child_data['name']] = child_data\n",
    "\n",
    "    return {\n",
    "        'id': category_id,\n",
    "        'name': category_name,\n",
    "        'children': children_dict\n",
    "    }\n",
    "\n",
    "def parse_category_xml(xml_path):\n",
    "    tree = ET.parse(xml_path, parser=ET.XMLParser(encoding='utf-8'))\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    category_system = root.find('.//{*}categories[@dataType=\"Process\"]')\n",
    "    top_level_dict = {}\n",
    "    if category_system is not None:\n",
    "        for cat_elem in category_system.findall('{*}category'):\n",
    "            cat_data = build_category_dict(cat_elem)\n",
    "            top_level_dict[cat_data['name']] = cat_data\n",
    "    else:\n",
    "        print(\"Warning: Could not find <categories dataType='Process'> in XML.\")\n",
    "    \n",
    "    return top_level_dict\n",
    "\n",
    "def get_category_path_info(category_hierarchy, categories_dict):\n",
    "    path_info = []\n",
    "    if not category_hierarchy:\n",
    "        return path_info\n",
    "\n",
    "    current_dict = categories_dict\n",
    "    for i, cat_name in enumerate(category_hierarchy):\n",
    "        if cat_name not in current_dict:\n",
    "            raise ValueError(f\"Category '{cat_name}' not found at level {i}.\")\n",
    "        this_cat = current_dict[cat_name]\n",
    "        path_info.append((this_cat['name'], this_cat['id']))\n",
    "        current_dict = this_cat['children']\n",
    "    return path_info\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# STEP 1.1: SELECT + CHANGE CATEGORY, PRIORITIZING \"compressive\" OR \"density\"\n",
    "#           Then rewrite materialProperties to a standard form and output to JSONLines\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "xml_path = \"../data/pipeline2/xml/OEKOBAU.DAT_Categories_EN_API.xml\"\n",
    "category_dict = parse_category_xml(xml_path)\n",
    "\n",
    "db_path = \"../data/pipeline2/sql/epd_database.sqlite\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "csv_path = \"../data/pipeline2/sql/regex_classified/filtered_epd_data02_classified_concrete05.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "num_epd_files = epd_count\n",
    "df_limited = df  \n",
    "\n",
    "modified_json_docs = {}\n",
    "TARGET_CLASSIFICATION = \"Mineral building products > Mortar and Concrete > Ready mixed concrete\"\n",
    "selected_uuids = []\n",
    "\n",
    "# Filter df down to only those rows whose RegEx Classification matches your target\n",
    "filtered_results = df[df[\"RegEx Classification\"] == TARGET_CLASSIFICATION]\n",
    "print(f\"Found {len(filtered_results)} EPD(s) after category filtering.\\n\")\n",
    "\n",
    "# Helper function to unify materialProperties\n",
    "def unify_material_property(mp):\n",
    "    \"\"\"\n",
    "    If mp[\"name\"] contains 'compressive', rename to 'compressive strength',\n",
    "       unit->'MPa', unitDescription->'megapascals'\n",
    "    If mp[\"name\"] contains 'density', rename to 'gross density',\n",
    "       unit->'kg/m^3', unitDescription->'kilograms per cubic metre'\n",
    "    Remove trailing parentheses from original name if it helps match.\n",
    "\n",
    "    Note: We keep mp[\"value\"] as is. Adjust if you want different numeric behavior.\n",
    "    \"\"\"\n",
    "    name_lower = mp[\"name\"].lower()\n",
    "\n",
    "    # Remove any trailing parentheses chunk (e.g. ' (kg/m^3)')\n",
    "    # This is optional if you want to remove that from the property name.\n",
    "    \n",
    "    mp[\"name\"] = re.sub(r\"\\(.*?\\)\", \"\", mp[\"name\"]).strip()\n",
    "\n",
    "    if \"compressive\" in name_lower:\n",
    "        mp[\"name\"] = \"compressive strength\"\n",
    "        mp[\"unit\"] = \"MPa\"\n",
    "        mp[\"unitDescription\"] = \"megapascals\"\n",
    "    elif \"density\" in name_lower:\n",
    "        mp[\"name\"] = \"gross density\"\n",
    "        mp[\"unit\"] = \"kg/m^3\"\n",
    "        mp[\"unitDescription\"] = \"kilograms per cubic meter\"\n",
    "\n",
    "for index, row in df_limited.iterrows():\n",
    "    if len(selected_uuids) >= num_epd_files:\n",
    "        break\n",
    "\n",
    "    uuid_val = row[\"UUID\"]\n",
    "    classification_str = row[\"RegEx Classification\"]\n",
    "\n",
    "    # Only process rows with the target classification\n",
    "    if classification_str == TARGET_CLASSIFICATION:\n",
    "        # 1) Fetch JSON from epd_documents\n",
    "        cursor.execute(\"SELECT document FROM epd_documents WHERE uuid = ?\", (uuid_val,))\n",
    "        result = cursor.fetchone()\n",
    "        if not result:\n",
    "            continue\n",
    "\n",
    "        json_text = result[0]\n",
    "        try:\n",
    "            json_data = json.loads(json_text)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        # 2) Check if there's a \"compressive\" and \"density\" entry in materialProperties\n",
    "        found_property_of_interest = False\n",
    "        compressive_found = False\n",
    "        density_found = False\n",
    "\n",
    "        exchanges = json_data.get(\"exchanges\", {}).get(\"exchange\", [])\n",
    "        for exch in exchanges:\n",
    "            mat_props = exch.get(\"materialProperties\", [])\n",
    "            for mp in mat_props:\n",
    "                name_lower = mp.get(\"name\", \"\").lower()\n",
    "                # Check for \"compressive\" only in this property\n",
    "                if \"compressive\" in name_lower and \"density\" not in name_lower:\n",
    "                    compressive_found = True\n",
    "                # Check for \"density\" only in this property\n",
    "                if \"density\" in name_lower and \"compressive\" not in name_lower:\n",
    "                    density_found = True\n",
    "                # If both have been found in two separate properties, we can stop searching\n",
    "                if compressive_found and density_found:\n",
    "                    found_property_of_interest = True\n",
    "                    break\n",
    "            if found_property_of_interest:\n",
    "                break\n",
    "\n",
    "        if not found_property_of_interest:\n",
    "            continue  # skip EPD if no two separate relevant property names\n",
    "\n",
    "        # 3) Build new classification array from the CSV \"RegEx Classification\"\n",
    "        category_names = [x.strip() for x in classification_str.split(\">\")]\n",
    "        try:\n",
    "            path_info = get_category_path_info(category_names, category_dict)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        class_array = []\n",
    "        for level, (cat_name, cat_id) in enumerate(path_info):\n",
    "            class_array.append({\n",
    "                \"value\": cat_name,\n",
    "                \"level\": level,\n",
    "                \"classId\": cat_id\n",
    "            })\n",
    "\n",
    "        new_classification_item = {\n",
    "            \"class\": class_array,\n",
    "            \"name\": \"OEKOBAU.DAT\"\n",
    "        }\n",
    "        \n",
    "        # 4) Inject new classification while preserving existing\n",
    "        process_info = json_data.setdefault(\"processInformation\", {})\n",
    "        data_set_info = process_info.setdefault(\"dataSetInformation\", {})\n",
    "        classification_info = data_set_info.setdefault(\"classificationInformation\", {})\n",
    "        existing_classifications = classification_info.get(\"classification\")\n",
    "        if not isinstance(existing_classifications, list):\n",
    "            existing_classifications = []\n",
    "        existing_classifications.append(new_classification_item)\n",
    "        classification_info[\"classification\"] = existing_classifications\n",
    "\n",
    "        # 5) Now unify each relevant material property\n",
    "        #    We'll loop through all exchanges, all mat_props\n",
    "        #    rewriting as needed\n",
    "        for exch in exchanges:\n",
    "            mat_props = exch.get(\"materialProperties\", [])\n",
    "            for mp in mat_props:\n",
    "                if \"compressive\" in mp.get(\"name\", \"\").lower() or \"density\" in mp.get(\"name\", \"\").lower():\n",
    "                    unify_material_property(mp)\n",
    "\n",
    "        # Remove unwanted exchanges entries\n",
    "        # For each exchange, filter out any anies item with module \"A1\", \"A2\", or \"A3\"\n",
    "            other = exch.get(\"other\", {})\n",
    "            anies = other.get(\"anies\", [])\n",
    "            filtered_anies = [item for item in anies if item.get(\"module\") not in [\"A1\", \"A2\", \"A3\"]]\n",
    "            other[\"anies\"] = filtered_anies\n",
    "\n",
    "        # 6) Remove unwanted LCIAResults entries\n",
    "        #    For each LCIAResult, filter out any anies item with module \"A1\", \"A2\", or \"A3\"\n",
    "        lcia_results = json_data.get(\"LCIAResults\", {}).get(\"LCIAResult\", [])\n",
    "        for lcia in lcia_results:\n",
    "            other = lcia.get(\"other\", {})\n",
    "            anies = other.get(\"anies\", [])\n",
    "            filtered_anies = [item for item in anies if item.get(\"module\") not in [\"A1\", \"A2\", \"A3\"]]\n",
    "            other[\"anies\"] = filtered_anies\n",
    "\n",
    "        # 7) Store the updated JSON\n",
    "        modified_json_docs[uuid_val] = json_data\n",
    "        selected_uuids.append(uuid_val)\n",
    "        # print(f\"Selected EPD with UUID={uuid_val}: found 'compressive' or 'density'. Classification updated.\")\n",
    "        # print(\"--------------------------------------------------\")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Write out the results to JSONLines\n",
    "output_jsonl = \"../data/pipeline2/json/edited_epds.jsonl\"\n",
    "\n",
    "with jsonlines.open(output_jsonl, mode='w') as writer:\n",
    "    idx = 1\n",
    "    for epd_uuid in selected_uuids:\n",
    "        doc = modified_json_docs[epd_uuid]\n",
    "        # We can try to get the classification system from the doc:\n",
    "        classifications = (\n",
    "            doc.get(\"processInformation\", {})\n",
    "               .get(\"dataSetInformation\", {})\n",
    "               .get(\"classificationInformation\", {})\n",
    "               .get(\"classification\", [])\n",
    "        )\n",
    "        existing_class_sys = classifications[0].get(\"name\") if classifications else \"Unknown\"\n",
    "\n",
    "        # product name from baseName\n",
    "        base_names = (\n",
    "            doc.get(\"processInformation\", {})\n",
    "               .get(\"dataSetInformation\", {})\n",
    "               .get(\"name\", {})\n",
    "               .get(\"baseName\", [])\n",
    "        )\n",
    "        epd_name = base_names[0].get(\"value\", \"\") if base_names else \"\"\n",
    "\n",
    "        record = {\n",
    "            \"id\": idx,\n",
    "            \"uuid\": epd_uuid,\n",
    "            \"epd_name\": epd_name,\n",
    "            \"classificationSys\": existing_class_sys,\n",
    "            \"document\": doc\n",
    "        }\n",
    "        writer.write(record)\n",
    "        idx += 1\n",
    "\n",
    "print(f\"\\nWrote {len(selected_uuids)} EPD records to '{output_jsonl}' in JSONLines format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add OEKOBAU.DAT and IBUCategories\n",
    "\n",
    "import re\n",
    "import json\n",
    "import sqlite3\n",
    "import jsonlines\n",
    "\n",
    "db_path = \"../data/pipeline2/sql/epd_database.sqlite\"\n",
    "category_val = \"Beton\"\n",
    "max_items = epd_count\n",
    "\n",
    "def fetch_well_defined_epds(conn, classification_system, category_value):\n",
    "    \"\"\"\n",
    "    Fetch EPDs from epd_documents + epd_metadata where\n",
    "      m.classification_system (case-insensitive) = classification_system,\n",
    "      the JSON classification array has {\"value\": category_value},\n",
    "      and materialProperties[*].name includes 'compressive' or 'density'.\n",
    "\n",
    "    Returns: (classification_name_in_json, results_list)\n",
    "    where results_list is [(uuid, doc_data)].\n",
    "    \"\"\"\n",
    "    sql = \"\"\"\n",
    "    SELECT d.uuid, d.document\n",
    "    FROM epd_documents d\n",
    "    JOIN epd_metadata m ON d.uuid = m.uuid\n",
    "    WHERE m.classification_system COLLATE NOCASE = ?\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql, (classification_system,))\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    results = []\n",
    "    classification_system_in_json = None\n",
    "\n",
    "    for doc_uuid, doc_text in rows:\n",
    "        if not doc_text:\n",
    "            continue\n",
    "        try:\n",
    "            doc_data = json.loads(doc_text)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        # 1) Must have \"Beton\" in classification\n",
    "        classifications = (\n",
    "            doc_data.get(\"processInformation\", {})\n",
    "            .get(\"dataSetInformation\", {})\n",
    "            .get(\"classificationInformation\", {})\n",
    "            .get(\"classification\", [])\n",
    "        )\n",
    "        found_beton = False\n",
    "        classification_name_for_this_doc = None\n",
    "\n",
    "        for classification_item in classifications:\n",
    "            for cls_obj in classification_item.get(\"class\", []):\n",
    "                if cls_obj.get(\"value\") == category_value:\n",
    "                    found_beton = True\n",
    "                    classification_name_for_this_doc = classification_item.get(\"name\")\n",
    "                    break\n",
    "            if found_beton:\n",
    "                break\n",
    "        if not found_beton:\n",
    "            continue\n",
    "\n",
    "        # 2) Must have 'compressive' or 'density' in materialProperties\n",
    "        exchanges = doc_data.get(\"exchanges\", {}).get(\"exchange\", [])\n",
    "        found_property = False\n",
    "        for exch in exchanges:\n",
    "            mat_props = exch.get(\"materialProperties\", [])\n",
    "            for mp in mat_props:\n",
    "                name_lower = mp.get(\"name\", \"\").lower()\n",
    "                if \"compressive\" in name_lower or \"density\" in name_lower:\n",
    "                    found_property = True\n",
    "                    break\n",
    "            if found_property:\n",
    "                break\n",
    "        if not found_property:\n",
    "            continue\n",
    "\n",
    "        # Record doc\n",
    "        results.append((doc_uuid, doc_data))\n",
    "        if classification_name_for_this_doc:\n",
    "            classification_system_in_json = classification_name_for_this_doc\n",
    "\n",
    "    return classification_system_in_json, results\n",
    "\n",
    "\n",
    "def process_epds_and_limit(epds, classification_name_in_json, classification_label):\n",
    "    \"\"\"\n",
    "    Parse compressive strength from baseName, insert 'compressive strength' property,\n",
    "    exclude if missing pattern or no exchange, then slice to number of EPDs.\n",
    "    Return the final processed list of (uuid, doc, classification_label).\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    print(f\"\\n{classification_label}\")\n",
    "    print(f\"Found {len(epds)} matching EPD(s) overall. Now parsing compressive strength...\")\n",
    "\n",
    "    pattern = r\"C\\s*(\\d+)\\s*/\\s*(\\d+)\"\n",
    "    final_list = []\n",
    "\n",
    "    for epd_uuid, epd_doc in epds:\n",
    "        # 1) product_name from baseName[0].value\n",
    "        base_names = (\n",
    "            epd_doc.get(\"processInformation\", {})\n",
    "            .get(\"dataSetInformation\", {})\n",
    "            .get(\"name\", {})\n",
    "            .get(\"baseName\", [])\n",
    "        )\n",
    "        if not base_names or not isinstance(base_names, list):\n",
    "            continue\n",
    "        product_name = base_names[0].get(\"value\", \"\")\n",
    "        if not product_name:\n",
    "            continue\n",
    "\n",
    "        # 2) parse \"Cxx/yy\"\n",
    "        match = re.search(pattern, product_name, flags=re.IGNORECASE)\n",
    "        if not match:\n",
    "            continue\n",
    "        compressive_strength_value = match.group(2)\n",
    "\n",
    "        # 3) Insert property in first exchange\n",
    "        exchanges = epd_doc.get(\"exchanges\", {}).get(\"exchange\", [])\n",
    "        if not exchanges:\n",
    "            continue\n",
    "        first_exchange = exchanges[0]\n",
    "        mat_props = first_exchange.setdefault(\"materialProperties\", [])\n",
    "        cs_entry = {\n",
    "            \"name\": \"compressive strength\",\n",
    "            \"value\": compressive_strength_value,\n",
    "            \"unit\": \"MPa\",\n",
    "            \"unitDescription\": \"megapascals\",\n",
    "        }\n",
    "        mat_props.append(cs_entry)\n",
    "\n",
    "        # 4) classification => fallback if we don't have classification_name_in_json\n",
    "        classification_to_store = (\n",
    "            classification_name_in_json\n",
    "            if classification_name_in_json\n",
    "            else classification_label\n",
    "        )\n",
    "\n",
    "        final_list.append((epd_uuid, epd_doc, classification_to_store))\n",
    "\n",
    "    # slice to 50\n",
    "    print(f\"Total EPDs after compressive strength parse: {len(final_list)}\")\n",
    "    final_list = final_list[:max_items]\n",
    "    print(f\"Will output {len(final_list)} EPDs. (Needed at least {max_items}.)\")\n",
    "    return final_list\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# 1) For OEKOBAU.DAT\n",
    "classification_in_json_oeko, all_epds_oeko = fetch_well_defined_epds(\n",
    "    conn, classification_system=\"OEKOBAU.DAT\", category_value=category_val\n",
    ")\n",
    "final_epds_oeko = process_epds_and_limit(\n",
    "    all_epds_oeko, classification_in_json_oeko, classification_label=\"OEKOBAU.DAT\"\n",
    ")\n",
    "\n",
    "# 2) For IBUCategories\n",
    "classification_in_json_ibu, all_epds_ibu = fetch_well_defined_epds(\n",
    "    conn, classification_system=\"IBUCategories\", category_value=category_val\n",
    ")\n",
    "final_epds_ibu = process_epds_and_limit(\n",
    "    all_epds_ibu, classification_in_json_ibu, classification_label=\"IBUCategories\"\n",
    ")\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# Merge them. We'll store them all in a single JSONLines file\n",
    "combined_epds = final_epds_oeko + final_epds_ibu\n",
    "print(\n",
    "    f\"\\nTotal EPDs => OEKOBAU.DAT: {len(final_epds_oeko)}, IBUCategories: {len(final_epds_ibu)}, Combined: {len(combined_epds)}\"\n",
    ")\n",
    "\n",
    "# Ucomment to test\n",
    "# modified_json_docs = {}\n",
    "\n",
    "modified_json_docs_nr = len(modified_json_docs)\n",
    "\n",
    "# Write to JSONLines\n",
    "output_jsonl = \"../data/pipeline2/json/edited_epds.jsonl\"\n",
    "with jsonlines.open(output_jsonl, mode=\"a\") as writer:\n",
    "    idx = len(modified_json_docs) + 1\n",
    "    for epd_uuid, epd_doc, classification_str in combined_epds:\n",
    "        # product name\n",
    "        base_names = (\n",
    "            epd_doc.get(\"processInformation\", {})\n",
    "            .get(\"dataSetInformation\", {})\n",
    "            .get(\"name\", {})\n",
    "            .get(\"baseName\", [])\n",
    "        )\n",
    "        product_name = base_names[0].get(\"value\", \"\") if base_names else \"\"\n",
    "\n",
    "        # Add to modified dict\n",
    "        modified_json_docs[epd_uuid] = epd_doc\n",
    "        # print(f\"Added {epd_uuid} to modified_json_docs\")\n",
    "\n",
    "        record = {\n",
    "            \"id\": idx,\n",
    "            \"uuid\": epd_uuid,\n",
    "            \"epd_name\": product_name,\n",
    "            \"classificationSys\": classification_str,\n",
    "            \"document\": epd_doc,\n",
    "        }\n",
    "        writer.write(record)\n",
    "        idx += 1\n",
    "\n",
    "    print(f\"\\nAdded {len(modified_json_docs) - modified_json_docs_nr} to modified_json_docs\\n\")\n",
    "\n",
    "print(\n",
    "    f\"Final combined JSONLines => '{output_jsonl}' with {len(combined_epds)} records.\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add EPDNorge\n",
    "\n",
    "import json\n",
    "import jsonlines\n",
    "import sqlite3\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "# =============================================================================\n",
    "# PART 1: Load initial results from JSON lines and filter by best_category\n",
    "# =============================================================================\n",
    "\n",
    "context_path = \"../data/pipeline2/json/context_jinaai_jina-embeddings-v3_20250503134414.json\"\n",
    "batch_output_path = \"../data/pipeline2/json/openai/batch_output_EPDNorge_concrete_batch_6816185b6cc08190a28a2829a6c1f780.jsonl\"\n",
    "\n",
    "with open(context_path, 'r', encoding='utf-8') as f:\n",
    "    context_data = json.load(f)\n",
    "\n",
    "results = []\n",
    "with jsonlines.open(batch_output_path) as reader:\n",
    "    for idx, batch_record in enumerate(reader):\n",
    "        content_str = batch_record[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        content_json = json.loads(content_str)\n",
    "        best_category = content_json.get(\"best_category\", \"\")\n",
    "\n",
    "        new_entry = {\n",
    "            \"id\": idx,\n",
    "            \"Product\": context_data[idx][\"Product\"],\n",
    "            \"UUID\": context_data[idx][\"UUID\"],\n",
    "            \"best_category\": best_category\n",
    "        }\n",
    "        results.append(new_entry)\n",
    "\n",
    "TARGET_CLASSIFICATION = \"Mineral building products > Mortar and Concrete > Ready mixed concrete\"\n",
    "filtered_results = [r for r in results if r[\"best_category\"] == TARGET_CLASSIFICATION]\n",
    "\n",
    "print(f\"Found {len(filtered_results)} EPD(s) after category filtering.\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 2: Connect to DB and load EPDs. Insert compressive strength & density first.\n",
    "# =============================================================================\n",
    "\n",
    "db_path = \"../data/pipeline2/sql/epd_database.sqlite\"\n",
    "\n",
    "# Regex for BXX or CXX/YY in product name\n",
    "cs_pattern = re.compile(r'B\\s?(\\d{2})|C(\\d{2})/(\\d{2})', re.IGNORECASE)\n",
    "\n",
    "def insert_compressive_strength_from_name(doc_data):\n",
    "    \"\"\"\n",
    "    If product name has BXX or CXX/YY, insert 'compressive strength'\n",
    "    into the first exchange's materialProperties.\n",
    "    \"\"\"\n",
    "    base_name_list = (\n",
    "        doc_data\n",
    "        .get(\"processInformation\", {})\n",
    "        .get(\"dataSetInformation\", {})\n",
    "        .get(\"name\", {})\n",
    "        .get(\"baseName\", [])\n",
    "    )\n",
    "    if not base_name_list or not isinstance(base_name_list, list):\n",
    "        return\n",
    "\n",
    "    product_name = base_name_list[0].get(\"value\", \"\")\n",
    "    if not product_name:\n",
    "        return\n",
    "\n",
    "    match = cs_pattern.search(product_name)\n",
    "    if not match:\n",
    "        return\n",
    "\n",
    "    if match.group(1):\n",
    "        # BXX => group(1)\n",
    "        compressive_value = match.group(1)\n",
    "    else:\n",
    "        # CXX/YY => group(3)\n",
    "        compressive_value = match.group(3)\n",
    "\n",
    "    exchanges = doc_data.get(\"exchanges\", {}).get(\"exchange\", [])\n",
    "    if not exchanges:\n",
    "        return\n",
    "\n",
    "    first_exchange = exchanges[0]\n",
    "    mat_props = first_exchange.setdefault(\"materialProperties\", [])\n",
    "\n",
    "    # Insert new entry\n",
    "    mat_props.append({\n",
    "        \"name\": \"compressive strength\",\n",
    "        \"value\": compressive_value,\n",
    "        \"unit\": \"MPa\",\n",
    "        \"unitDescription\": \"megapascals\"\n",
    "    })\n",
    "\n",
    "def insert_gross_density_from_volume_mass(doc_data):\n",
    "    \"\"\"\n",
    "    If we find:\n",
    "      - 'Volume'/'Volum' with meanValue=1, referenceUnit='m3'\n",
    "      - 'Mass'/'Masse' with numeric meanValue\n",
    "    Then insert 'gross density' into the first exchange's materialProperties.\n",
    "    \"\"\"\n",
    "    exchanges = doc_data.get(\"exchanges\", {}).get(\"exchange\", [])\n",
    "    if not exchanges:\n",
    "        return\n",
    "\n",
    "    first_exchange = exchanges[0]\n",
    "    flow_props = first_exchange.get(\"flowProperties\", [])\n",
    "    if not flow_props:\n",
    "        return\n",
    "\n",
    "    found_volume = False\n",
    "    found_mass = False\n",
    "    mass_value = None\n",
    "\n",
    "    for fp in flow_props:\n",
    "        names = fp.get(\"name\", [])\n",
    "        mean_val = fp.get(\"meanValue\")\n",
    "        ref_unit = fp.get(\"referenceUnit\", \"\")\n",
    "\n",
    "        # Volume?\n",
    "        if any(\"volum\" in n.get(\"value\", \"\").lower() or \"volume\" in n.get(\"value\", \"\").lower() for n in names):\n",
    "            if mean_val == 1 and ref_unit == \"m3\":\n",
    "                found_volume = True\n",
    "\n",
    "        # Mass?\n",
    "        if any(\"mass\" in n.get(\"value\", \"\").lower() or \"masse\" in n.get(\"value\", \"\").lower() for n in names):\n",
    "            try:\n",
    "                mass_value = float(mean_val)\n",
    "                found_mass = True\n",
    "            except (TypeError, ValueError):\n",
    "                pass\n",
    "\n",
    "    if found_volume and found_mass and (mass_value is not None):\n",
    "        mat_props = first_exchange.setdefault(\"materialProperties\", [])\n",
    "        mat_props.append({\n",
    "            \"name\": \"gross density\",\n",
    "            \"value\": str(mass_value),\n",
    "            \"unit\": \"kg/m^3\",\n",
    "            \"unitDescription\": \"kilograms per cubic meter\"\n",
    "        })\n",
    "\n",
    "def get_location(doc_data):\n",
    "    return (\n",
    "        doc_data\n",
    "        .get(\"processInformation\", {})\n",
    "        .get(\"geography\", {})\n",
    "        .get(\"locationOfOperationSupplyOrProduction\", {})\n",
    "        .get(\"location\")\n",
    "    )\n",
    "\n",
    "def has_material_property(doc_data, prop_name):\n",
    "    \"\"\"\n",
    "    Returns True if the first exchange's materialProperties\n",
    "    has an entry with `name == prop_name`.\n",
    "    \"\"\"\n",
    "    exchanges = doc_data.get(\"exchanges\", {}).get(\"exchange\", [])\n",
    "    if not exchanges:\n",
    "        return False\n",
    "    first_exchange = exchanges[0]\n",
    "    mat_props = first_exchange.get(\"materialProperties\", [])\n",
    "    for mp in mat_props:\n",
    "        if mp.get(\"name\", \"\").lower() == prop_name.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Gather EPDs that end up having location, 'compressive strength', and 'gross density' after the inserts\n",
    "EPDs_with_all_data = []\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "for entry in filtered_results:\n",
    "    uuid_val = entry[\"UUID\"]\n",
    "    cursor.execute(\"SELECT document FROM epd_documents WHERE uuid = ?\", (uuid_val,))\n",
    "    row = cursor.fetchone()\n",
    "    if not row:\n",
    "        continue\n",
    "\n",
    "    doc_text = row[0]\n",
    "    try:\n",
    "        doc_data = json.loads(doc_text)\n",
    "    except json.JSONDecodeError:\n",
    "        continue\n",
    "\n",
    "    # 1) Check if we have a non-empty location\n",
    "    location_val = get_location(doc_data)\n",
    "    if not location_val:\n",
    "        # skip if there's no location\n",
    "        continue\n",
    "\n",
    "    # 2) Insert compressive strength from product name\n",
    "    insert_compressive_strength_from_name(doc_data)\n",
    "\n",
    "    # 3) Insert gross density from volume=1, mass\n",
    "    insert_gross_density_from_volume_mass(doc_data)\n",
    "\n",
    "    # Now see if we indeed have \"compressive strength\" and \"gross density\"\n",
    "    # after insertion\n",
    "    has_compressive = has_material_property(doc_data, \"compressive strength\")\n",
    "    has_density = has_material_property(doc_data, \"gross density\")\n",
    "\n",
    "    if has_compressive and has_density:\n",
    "        EPDs_with_all_data.append((uuid_val, doc_data))\n",
    "\n",
    "conn.close()\n",
    "\n",
    "print(f\"Total EPDs after insertion steps (location + compressive strength + gross density): {len(EPDs_with_all_data)}\\n\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# PART 3: Summation A1-A3, classification, limit to 100, output\n",
    "# =============================================================================\n",
    "\n",
    "def sum_a1_a2_a3(doc_data):\n",
    "    def handle_anies(anies_list):\n",
    "        if not anies_list:\n",
    "            return anies_list\n",
    "        sum_value = 0.0\n",
    "        sum_item_template = None\n",
    "        keepers = []\n",
    "\n",
    "        for item in anies_list:\n",
    "            mod = item.get(\"module\")\n",
    "            if mod in [\"A1\", \"A2\", \"A3\"]:\n",
    "                val_str = item.get(\"value\")\n",
    "                try:\n",
    "                    val = float(val_str)\n",
    "                except (ValueError, TypeError):\n",
    "                    val = 0.0\n",
    "                sum_value += val\n",
    "                if sum_item_template is None:\n",
    "                    sum_item_template = dict(item)\n",
    "            else:\n",
    "                keepers.append(item)\n",
    "\n",
    "        if sum_value != 0.0:\n",
    "            if sum_item_template is None:\n",
    "                sum_item_template = {}\n",
    "            sum_item_template[\"module\"] = \"A1-A3\"\n",
    "            sum_item_template[\"value\"] = str(sum_value)\n",
    "            keepers.insert(0, sum_item_template)\n",
    "        return keepers\n",
    "\n",
    "    exchanges = doc_data.get(\"exchanges\", {}).get(\"exchange\", [])\n",
    "    for exch in exchanges:\n",
    "        other = exch.get(\"other\", {})\n",
    "        anies = other.get(\"anies\", [])\n",
    "        other[\"anies\"] = handle_anies(anies)\n",
    "\n",
    "    lcia_results = doc_data.get(\"LCIAResults\", {}).get(\"LCIAResult\", [])\n",
    "    for lcia in lcia_results:\n",
    "        other = lcia.get(\"other\", {})\n",
    "        anies = other.get(\"anies\", [])\n",
    "        other[\"anies\"] = handle_anies(anies)\n",
    "\n",
    "def parse_category_xml(xml_path):\n",
    "    tree = ET.parse(xml_path, parser=ET.XMLParser(encoding='utf-8'))\n",
    "    root = tree.getroot()\n",
    "    category_system = root.find('.//{*}categories[@dataType=\"Process\"]')\n",
    "    top_level_dict = {}\n",
    "    if category_system is not None:\n",
    "        for cat_elem in category_system.findall('{*}category'):\n",
    "            cat_data = build_category_dict(cat_elem)\n",
    "            top_level_dict[cat_data['name']] = cat_data\n",
    "    else:\n",
    "        print(\"Warning: Could not find <categories dataType='Process'> in XML.\")\n",
    "    return top_level_dict\n",
    "\n",
    "def build_category_dict(xml_element):\n",
    "    category_id = xml_element.get('id')\n",
    "    category_name = xml_element.get('name')\n",
    "    \n",
    "    children_dict = {}\n",
    "    for child_elem in xml_element.findall('{*}category'):\n",
    "        child_data = build_category_dict(child_elem)\n",
    "        children_dict[child_data['name']] = child_data\n",
    "\n",
    "    return {\n",
    "        'id': category_id,\n",
    "        'name': category_name,\n",
    "        'children': children_dict\n",
    "    }\n",
    "\n",
    "def get_category_path_info(category_hierarchy, categories_dict):\n",
    "    \"\"\"Given a list of category names and your category_dict, \n",
    "       return [(name, id), (name, id), ...] for each level.\"\"\"\n",
    "    path_info = []\n",
    "    if not category_hierarchy:\n",
    "        return path_info\n",
    "\n",
    "    current_dict = categories_dict\n",
    "    for i, cat_name in enumerate(category_hierarchy):\n",
    "        if cat_name not in current_dict:\n",
    "            raise ValueError(f\"Category '{cat_name}' not found at level {i}.\")\n",
    "        this_cat = current_dict[cat_name]\n",
    "        path_info.append((this_cat['name'], this_cat['id']))\n",
    "        current_dict = this_cat['children']\n",
    "    return path_info\n",
    "\n",
    "def insert_classification(doc_data, classification_system_in_json, target_class=TARGET_CLASSIFICATION):\n",
    "    classification_str = target_class\n",
    "    category_names = [x.strip() for x in classification_str.split(\">\")]\n",
    "    try:\n",
    "        path_info = get_category_path_info(category_names, category_dict)\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: Could not resolve categories. {e}\")\n",
    "        return classification_system_in_json\n",
    "\n",
    "    class_array = []\n",
    "    for level, (cat_name, cat_id) in enumerate(path_info):\n",
    "        class_array.append({\n",
    "            \"value\": cat_name,\n",
    "            \"level\": level,\n",
    "            \"classId\": cat_id\n",
    "        })\n",
    "\n",
    "    classifications = (\n",
    "        doc_data\n",
    "        .get(\"processInformation\", {})\n",
    "        .get(\"dataSetInformation\", {})\n",
    "        .get(\"classificationInformation\", {})\n",
    "        .get(\"classification\", [])\n",
    "    )\n",
    "    for classification_item in classifications:\n",
    "        possible_name = classification_item.get(\"name\")\n",
    "        if possible_name:\n",
    "            classification_system_in_json = possible_name\n",
    "            break\n",
    "\n",
    "    new_classification_item = {\n",
    "        \"class\": class_array,\n",
    "        \"name\": \"OEKOBAU.DAT\"\n",
    "    }\n",
    "    process_info = doc_data.setdefault(\"processInformation\", {})\n",
    "    data_set_info = process_info.setdefault(\"dataSetInformation\", {})\n",
    "    classification_info = data_set_info.setdefault(\"classificationInformation\", {})\n",
    "    existing_classifications = classification_info.get(\"classification\")\n",
    "    if not isinstance(existing_classifications, list):\n",
    "        existing_classifications = []\n",
    "    existing_classifications.append(new_classification_item)\n",
    "    classification_info[\"classification\"] = existing_classifications\n",
    "\n",
    "    return classification_system_in_json\n",
    "\n",
    "xml_path = \"../data/pipeline2/xml/OEKOBAU.DAT_Categories_EN_API.xml\"\n",
    "category_dict = parse_category_xml(xml_path)\n",
    "\n",
    "# Pick up to 100 EPDs\n",
    "number_epds = epd_count\n",
    "final_list = EPDs_with_all_data[:number_epds]\n",
    "\n",
    "# Uncomment for testing\n",
    "# modified_json_docs = {}\n",
    "\n",
    "selected_uuids = []\n",
    "classification_system_in_json = None\n",
    "\n",
    "for (uuid_val, doc_data) in final_list:\n",
    "    # Summation A1-A3\n",
    "    sum_a1_a2_a3(doc_data)\n",
    "    # Insert classification\n",
    "    classification_system_in_json = insert_classification(doc_data, classification_system_in_json)\n",
    "    # Store\n",
    "    modified_json_docs[uuid_val] = doc_data\n",
    "    selected_uuids.append(uuid_val)\n",
    "\n",
    "print(f\"\\nFinal selection: {len(modified_json_docs)} EPDs that have location, compressive strength, and gross density.\\n\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 4: Output final\n",
    "# =============================================================================\n",
    "\n",
    "# output_file = \"selected_epds_norge.json\"\n",
    "# output_data = {\n",
    "#     \"classificationSystem\": classification_system_in_json,\n",
    "#     \"uuids\": list(modified_json_docs.keys())\n",
    "# }\n",
    "# with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "#     json.dump(output_data, out_f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# print(f\"Wrote {len(modified_json_docs)} final EPDs to '{output_file}'.\\n\")\n",
    "\n",
    "output_jsonl = \"../data/pipeline2/json/edited_epds.jsonl\"\n",
    "with jsonlines.open(output_jsonl, mode='a') as writer:\n",
    "    idx = len(modified_json_docs) - len(selected_uuids) + 1\n",
    "    for epd_uuid in selected_uuids:\n",
    "        doc = modified_json_docs[epd_uuid]\n",
    "        # Possibly get classification system from doc:\n",
    "        classifications = (\n",
    "            doc.get(\"processInformation\", {})\n",
    "               .get(\"dataSetInformation\", {})\n",
    "               .get(\"classificationInformation\", {})\n",
    "               .get(\"classification\", [])\n",
    "        )\n",
    "        existing_class_sys = classifications[0].get(\"name\") if classifications else \"Unknown\"\n",
    "\n",
    "        # baseName\n",
    "        base_names = (\n",
    "            doc.get(\"processInformation\", {})\n",
    "               .get(\"dataSetInformation\", {})\n",
    "               .get(\"name\", {})\n",
    "               .get(\"baseName\", [])\n",
    "        )\n",
    "        epd_name = base_names[0].get(\"value\", \"\") if base_names else \"\"\n",
    "\n",
    "        writer.write({\n",
    "            \"id\": idx,\n",
    "            \"uuid\": epd_uuid,\n",
    "            \"epd_name\": epd_name,\n",
    "            \"classificationSys\": existing_class_sys,\n",
    "            \"document\": doc\n",
    "        })\n",
    "        idx += 1\n",
    "\n",
    "print(f\"Wrote full JSON docs for {len(selected_uuids)} EPDs to '{output_jsonl}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Rename JSON keys (updated with new logic)\n",
    "\n",
    "import json\n",
    "\n",
    "def recursive_rename_uri(obj):\n",
    "    \"\"\"\n",
    "    Recursively rename any key 'uri' to 'refObjectUri' in the given object (dict/list).\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        new_obj = {}\n",
    "        for key, value in obj.items():\n",
    "            new_key = \"refObjectUri\" if key == \"uri\" else key\n",
    "            new_obj[new_key] = recursive_rename_uri(value)\n",
    "        return new_obj\n",
    "    elif isinstance(obj, list):\n",
    "        return [recursive_rename_uri(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def remove_raw_strings_in_anies(obj):\n",
    "    \"\"\"\n",
    "    Recursively traverse the object and remove any raw string elements from lists\n",
    "    that belong to a key named 'anies'.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        new_obj = {}\n",
    "        for key, value in obj.items():\n",
    "            if key == \"anies\" and isinstance(value, list):\n",
    "                # Filter out raw string elements from this list.\n",
    "                new_obj[key] = [\n",
    "                    remove_raw_strings_in_anies(item)\n",
    "                    for item in value\n",
    "                    if not isinstance(item, str)\n",
    "                ]\n",
    "            else:\n",
    "                new_obj[key] = remove_raw_strings_in_anies(value)\n",
    "        return new_obj\n",
    "    elif isinstance(obj, list):\n",
    "        return [remove_raw_strings_in_anies(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def transform_json(data):\n",
    "    \"\"\"\n",
    "    Transform an Environmental Product Declaration (EPD) JSON instance by renaming keys\n",
    "    and restructuring its contents to standardize the schema.\n",
    "    \n",
    "    Major changes in this version:\n",
    "      - Globally rename every \"uri\" to \"refObjectUri\"\n",
    "      - Globally remove raw string elements from any 'anies' lists\n",
    "      - Keep the rest of your original rename logic: processInformation, modellingAndValidation, \n",
    "        administrativeInformation, exchanges, LCIAResults, etc.\n",
    "      - Exclude the key \"relativeStandardDeviation95In\" from LCIAResults.LCIAResult and exchanges.exchange.\n",
    "\n",
    "    Returns: The updated dictionary (in-memory).\n",
    "    \"\"\"\n",
    "    # --- processInformation transformations ---\n",
    "    process_info = data.get(\"processInformation\", {})\n",
    "    data_set_info = process_info.get(\"dataSetInformation\", {})\n",
    "\n",
    "    # Rename \"dataSetInformation.name\" -> \"dataSetName\"\n",
    "    if \"name\" in data_set_info:\n",
    "        data_set_info[\"dataSetName\"] = data_set_info.pop(\"name\")\n",
    "\n",
    "    # Rename dataSetInformation.other -> otherDSI; remove 'componentsAndMaterialsAndSubstances' if found\n",
    "    if \"other\" in data_set_info:\n",
    "        data_set_info[\"otherDSI\"] = data_set_info.pop(\"other\")\n",
    "        # If any item in otherDSI.anies contains \"componentsAndMaterialsAndSubstances\", remove the entire \"anies\" key.\n",
    "        if \"anies\" in data_set_info[\"otherDSI\"]:\n",
    "            for item in data_set_info[\"otherDSI\"][\"anies\"]:\n",
    "                if (\n",
    "                    isinstance(item, dict)\n",
    "                    and \"componentsAndMaterialsAndSubstances\" in item\n",
    "                ):\n",
    "                    data_set_info[\"otherDSI\"].pop(\"anies\")\n",
    "                    break\n",
    "            # Rename dataSetInformation.other.anies.scenario -> objectScenario;\n",
    "            if \"scenario\" in data_set_info[\"otherDSI\"]:\n",
    "                data_set_info[\"objectScenario\"] = data_set_info[\"otherDSI\"].pop(\"scenario\")\n",
    "\n",
    "    # For classification entries, rename \"class\" -> \"classEntries\"\n",
    "    classification_info = data_set_info.get(\"classificationInformation\", {})\n",
    "    classifications = classification_info.get(\"classification\", [])\n",
    "    for cls_obj in classifications:\n",
    "        if \"class\" in cls_obj:\n",
    "            cls_obj[\"classEntries\"] = cls_obj.pop(\"class\")\n",
    "\n",
    "    # Rename \"time\" -> \"timeInformation\", then rename \"other\" -> \"otherTime\", and \"value\" -> \"timestampValue\"\n",
    "    if \"time\" in process_info:\n",
    "        process_info[\"timeInformation\"] = process_info.pop(\"time\")\n",
    "        time_info = process_info[\"timeInformation\"]\n",
    "        if \"other\" in time_info:\n",
    "            time_info[\"otherTime\"] = time_info.pop(\"other\")\n",
    "            for item in time_info[\"otherTime\"].get(\"anies\", []):\n",
    "                if isinstance(item, dict) and \"value\" in item:\n",
    "                    item[\"timestampValue\"] = item.pop(\"value\")\n",
    "\n",
    "    # --- modellingAndValidation transformations ---\n",
    "    mod_val = data.get(\"modellingAndValidation\", {})\n",
    "\n",
    "    # LCIMethodAndAllocation.other -> otherMAA\n",
    "    lci_method = mod_val.get(\"LCIMethodAndAllocation\", {})\n",
    "    if \"other\" in lci_method:\n",
    "        lci_method[\"otherMAA\"] = lci_method.pop(\"other\")\n",
    "\n",
    "    # dataSourcesTreatmentAndRepresentativeness.other -> otherDSTAR\n",
    "    dstar = mod_val.get(\"dataSourcesTreatmentAndRepresentativeness\", {})\n",
    "    if \"other\" in dstar:\n",
    "        dstar[\"otherDSTAR\"] = dstar.pop(\"other\")\n",
    "        if \"anies\" in dstar[\"otherDSTAR\"]:\n",
    "            dstar[\"otherDSTAR\"][\"aniesDSTAR\"] = dstar[\"otherDSTAR\"].pop(\"anies\")\n",
    "            for item in dstar[\"otherDSTAR\"][\"aniesDSTAR\"]:\n",
    "                if isinstance(item, dict) and \"value\" in item and isinstance(item[\"value\"], dict):\n",
    "                    # rename \"value\" -> \"valueDSTAR\", then rename subkeys\n",
    "                    item[\"valueDSTAR\"] = item.pop(\"value\")\n",
    "                    val = item[\"valueDSTAR\"]\n",
    "                    if \"shortDescription\" in val:\n",
    "                        val[\"shortDescriptionExtended\"] = val.pop(\"shortDescription\")\n",
    "                    if \"version\" in val:\n",
    "                        version = val.pop(\"version\")\n",
    "                        if \"version\" in version:\n",
    "                            version[\"versionInt\"] = version.pop(\"version\")\n",
    "                        val[\"versionDict\"] = version\n",
    "                    if \"uuid\" in val:\n",
    "                        uuid_obj = val.pop(\"uuid\")\n",
    "                        if \"uuid\" in uuid_obj:\n",
    "                            uuid_obj[\"uuidValue\"] = uuid_obj.pop(\"uuid\")\n",
    "                        val[\"uuidDict\"] = uuid_obj\n",
    "\n",
    "    # Rename \"validation\" -> \"validationInfo\"\n",
    "    if \"validation\" in mod_val:\n",
    "        mod_val[\"validationInfo\"] = mod_val.pop(\"validation\")\n",
    "\n",
    "    # modellingAndValidation.other -> otherMAV; if \"value\" is a dict, rename it to \"objectValue\"\n",
    "    if \"other\" in mod_val:\n",
    "        mod_val[\"otherMAV\"] = mod_val.pop(\"other\")\n",
    "        for item in mod_val[\"otherMAV\"].get(\"anies\", []):\n",
    "            if isinstance(item, dict) and \"value\" in item and isinstance(item[\"value\"], dict):\n",
    "                item[\"objectValue\"] = item.pop(\"value\")\n",
    "\n",
    "    # --- administrativeInformation transformations ---\n",
    "    admin_info = data.get(\"administrativeInformation\", {})\n",
    "\n",
    "    # publicationAndOwnership.other -> otherPAO; rename \"value\" -> \"objectValue\" if it's a dict\n",
    "    pub_own = admin_info.get(\"publicationAndOwnership\", {})\n",
    "    if \"other\" in pub_own:\n",
    "        pub_own[\"otherPAO\"] = pub_own.pop(\"other\")\n",
    "        for item in pub_own[\"otherPAO\"].get(\"anies\", []):\n",
    "            if isinstance(item, dict) and \"value\" in item and isinstance(item[\"value\"], dict):\n",
    "                item[\"objectValue\"] = item.pop(\"value\")\n",
    "\n",
    "    # --- exchanges transformations ---\n",
    "    exchanges = data.get(\"exchanges\", {}).get(\"exchange\", [])\n",
    "    for exchange in exchanges:\n",
    "        # flowProperties: rename name->nameFP, uuid->uuidFP\n",
    "        for fp in exchange.get(\"flowProperties\", []):\n",
    "            if \"name\" in fp:\n",
    "                fp[\"nameFP\"] = fp.pop(\"name\")\n",
    "            if \"uuid\" in fp:\n",
    "                fp[\"uuidFP\"] = fp.pop(\"uuid\")\n",
    "\n",
    "        # rename \"exchange direction\" -> \"exchangeDirection\"\n",
    "        if \"exchange direction\" in exchange:\n",
    "            exchange[\"exchangeDirection\"] = exchange.pop(\"exchange direction\")\n",
    "\n",
    "        # rename \"other\" -> \"otherEx\", if \"value\" is dict -> \"objectValue\"\n",
    "        if \"other\" in exchange:\n",
    "            exchange[\"otherEx\"] = exchange.pop(\"other\")\n",
    "            for item in exchange[\"otherEx\"].get(\"anies\", []):\n",
    "                if isinstance(item, dict) and \"value\" in item and isinstance(item[\"value\"], dict):\n",
    "                    item[\"objectValue\"] = item.pop(\"value\")\n",
    "\n",
    "        # rename \"classification\" -> \"classificationEx\" and inside, rename \"name\"->\"nameClass\"\n",
    "        if \"classification\" in exchange:\n",
    "            exchange[\"classificationEx\"] = exchange.pop(\"classification\")\n",
    "            if \"name\" in exchange[\"classificationEx\"]:\n",
    "                exchange[\"classificationEx\"][\"nameClass\"] = exchange[\"classificationEx\"].pop(\"name\")\n",
    "        \n",
    "        # exclude relativeStandardDeviation95In from exchanges.exchange ---\n",
    "        if \"relativeStandardDeviation95In\" in exchange:\n",
    "            exchange.pop(\"relativeStandardDeviation95In\")\n",
    "\n",
    "    # --- LCIAResults transformations ---\n",
    "    # rename \"LCIAResults\" -> \"lciaResults\" if present\n",
    "    if \"LCIAResults\" in data:\n",
    "        data[\"lciaResults\"] = data.pop(\"LCIAResults\")\n",
    "\n",
    "    lcia_results = data.get(\"lciaResults\", {}).get(\"LCIAResult\", [])\n",
    "    for result in lcia_results:\n",
    "        # rename \"other\" -> \"otherLCIA\", if \"value\" is dict -> \"objectValue\"\n",
    "        if \"other\" in result:\n",
    "            result[\"otherLCIA\"] = result.pop(\"other\")\n",
    "            for item in result[\"otherLCIA\"].get(\"anies\", []):\n",
    "                if isinstance(item, dict) and \"value\" in item and isinstance(item[\"value\"], dict):\n",
    "                    item[\"objectValue\"] = item.pop(\"value\")\n",
    "        \n",
    "        # exclude relativeStandardDeviation95In from each LCIA result ---\n",
    "        if \"relativeStandardDeviation95In\" in result:\n",
    "            result.pop(\"relativeStandardDeviation95In\")\n",
    "\n",
    "    # --- Removal ---\n",
    "    # Remove top-level \"otherAttributes\" key if it exists\n",
    "    if \"otherAttributes\" in data:\n",
    "        data.pop(\"otherAttributes\")\n",
    "    \n",
    "    # if \"modellingAndValidation\" in data:\n",
    "    #     data.pop(\"modellingAndValidation\")\n",
    "    \n",
    "    # if \"administrativeInformation\" in data:\n",
    "    #     data.pop(\"administrativeInformation\")\n",
    "    \n",
    "    # if \"exchanges\" in data:\n",
    "    #     data.pop(\"exchanges\")\n",
    "    \n",
    "    # if \"lciaResults\" in data:\n",
    "    #     data.pop(\"lciaResults\")\n",
    "    \n",
    "    if \"locations\" in data:\n",
    "        data.pop(\"locations\")\n",
    "\n",
    "\n",
    "    # --- Global step: remove raw string elements from any 'anies' list\n",
    "    data = remove_raw_strings_in_anies(data)\n",
    "\n",
    "    # --- Global step: rename \"uri\" -> \"refObjectUri\"\n",
    "    data = recursive_rename_uri(data)\n",
    "\n",
    "    return data\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Example pipeline usage (in memory, no disk I/O):\n",
    "# -----------------------------------------------------------\n",
    "for uuid_val, original_doc in modified_json_docs.items():\n",
    "    final_doc = transform_json(original_doc)\n",
    "\n",
    "    # If you want to store the transformed version back in the dictionary:\n",
    "    modified_json_docs[uuid_val] = final_doc\n",
    "\n",
    "    # Just print for verification\n",
    "    # print(f\"\\nTransformed JSON for UUID = {uuid_val}:\")\n",
    "    # print(json.dumps(final_doc, indent=2))\n",
    "    # print(\"--------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Add ids\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "# --------------------------- Utility functions ---------------------------\n",
    "\n",
    "def load_yaml_schema(file_path):\n",
    "    \"\"\"Load the LinkML YAML schema from disk.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def reorder_dict_keys(d):\n",
    "    \"\"\"\n",
    "    Reorders a dictionary so that if 'id' exists, it appears as the first key.\n",
    "    (For human readability.)\n",
    "    \"\"\"\n",
    "    if \"id\" in d:\n",
    "        id_value = d.pop(\"id\")\n",
    "        new_d = {\"id\": id_value}\n",
    "        new_d.update(d)\n",
    "        d.clear()\n",
    "        d.update(new_d)\n",
    "\n",
    "def clean_epd_name(name):\n",
    "    \"\"\"\n",
    "    Cleans the EPD name by replacing non-alphanumeric characters with underscores,\n",
    "    collapsing multiple underscores, and stripping leading/trailing underscores.\n",
    "    \"\"\"\n",
    "    cleaned = re.sub(r\"[^A-Za-z0-9]\", \"_\", name)\n",
    "    return re.sub(r\"_+\", \"_\", cleaned).strip(\"_\")\n",
    "\n",
    "# Create new ID\n",
    "\n",
    "def generate_id_from_path(acc_path, prefix=\"ilcd\"):\n",
    "    \"\"\"\n",
    "    Given an accumulated path, returns the full ID as: prefix:acc_path\n",
    "    \"\"\"\n",
    "    return f\"{prefix}:{acc_path}\"\n",
    "\n",
    "def get_suffix(item, index):\n",
    "    \"\"\"\n",
    "    If the list element (item) is a dict with a 'module' field,\n",
    "    return 'module' + its value (dashes removed);\n",
    "    otherwise, return the 1-based index as a two-digit string.\n",
    "    \"\"\"\n",
    "    if isinstance(item, dict) and \"module\" in item:\n",
    "        return f\"module{item['module'].replace('-', '')}\"\n",
    "    return f\"{index + 1:02d}\"\n",
    "\n",
    "def assign_ids_by_path(obj, epd_uuid, acc_path, parent_is_list, prefix=\"ilcd\"):\n",
    "    \"\"\"\n",
    "    Recursively assigns IDs based on the accumulated path.\n",
    "\n",
    "    Parameters:\n",
    "      obj          : current object (dict, list, or primitive)\n",
    "      epd_uuid     : the top-level EPD UUID (dashes removed), used as a base\n",
    "      acc_path     : the accumulated path string\n",
    "      parent_is_list : bool indicating whether the parent container was a list\n",
    "      prefix       : the string prefix to use, e.g. \"ilcd\"\n",
    "    \"\"\"\n",
    "    # If this is a dict and has no 'id', generate one\n",
    "    if isinstance(obj, dict) and \"id\" not in obj:\n",
    "        # ID => prefix:epd_uuid_accPath\n",
    "        obj[\"id\"] = generate_id_from_path(f\"{epd_uuid}_{acc_path}\", prefix)\n",
    "        reorder_dict_keys(obj)\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        # Traverse each key\n",
    "        for key, value in obj.items():\n",
    "            if key == \"id\":\n",
    "                continue\n",
    "            if isinstance(value, dict):\n",
    "                # If parent is a list, extend path with \"_key\"\n",
    "                new_acc = f\"{acc_path}_{key}\" if parent_is_list else key\n",
    "                assign_ids_by_path(value, epd_uuid, new_acc, parent_is_list=False, prefix=prefix)\n",
    "            elif isinstance(value, list):\n",
    "                # For a list, extend path with \"_key\"\n",
    "                new_acc = f\"{acc_path}_{key}\"\n",
    "                for i, item in enumerate(value):\n",
    "                    suffix = get_suffix(item, i)\n",
    "                    # element path => new_acc + \"_\" + suffix\n",
    "                    element_acc = f\"{new_acc}_{suffix}\"\n",
    "                    assign_ids_by_path(item, epd_uuid, element_acc, parent_is_list=True, prefix=prefix)\n",
    "            # If it's a primitive, do nothing\n",
    "    elif isinstance(obj, list):\n",
    "        # If this is a list, iterate elements\n",
    "        for i, item in enumerate(obj):\n",
    "            suffix = get_suffix(item, i)\n",
    "            new_acc = f\"{acc_path}_{suffix}\"\n",
    "            assign_ids_by_path(item, epd_uuid, new_acc, parent_is_list=True, prefix=prefix)\n",
    "\n",
    "\n",
    "# MAIN LOGIC: In-memory Example \n",
    "\n",
    "SCHEMA_PATH = \"../data/linkml/yaml/linkml_ILCDmergedSchemas_schema.yaml\"\n",
    "schema = load_yaml_schema(SCHEMA_PATH)\n",
    "\n",
    "# Fallback if your schema has a default prefix; else use \"ilcd\"\n",
    "default_prefix = schema.get(\"default_prefix\", \"ilcd\")\n",
    "\n",
    "for uuid_val, doc in modified_json_docs.items():\n",
    "    # 1) Get the \"real\" UUID from the doc (with dashes)\n",
    "    try:\n",
    "        raw_uuid = doc[\"processInformation\"][\"dataSetInformation\"][\"UUID\"]\n",
    "    except KeyError as e:\n",
    "        raise KeyError(f\"Missing processInformation.dataSetInformation.UUID in doc {uuid_val}\") from e\n",
    "    \n",
    "    # 2) Remove dashes to build a base\n",
    "    epd_uuid = raw_uuid.replace(\"-\", \"\")\n",
    "\n",
    "    # 3) Assign top-level doc ID (prefix:epd_uuid)\n",
    "    doc[\"id\"] = f\"{default_prefix.lower()}:{epd_uuid}\"\n",
    "\n",
    "    # 4) For each top-level key (besides 'id'/'version'), set a sub-ID and recursively assign deeper IDs\n",
    "    for top_key, top_obj in doc.items():\n",
    "        if top_key in [\"id\", \"version\"]:\n",
    "            continue\n",
    "        if isinstance(top_obj, dict):\n",
    "            # e.g. \"processInformation\" => prefix:epd_uuid_processInformation\n",
    "            top_obj[\"id\"] = f\"{default_prefix.lower()}:{epd_uuid}_{top_key}\"\n",
    "            reorder_dict_keys(top_obj)\n",
    "            assign_ids_by_path(\n",
    "                top_obj,\n",
    "                epd_uuid=epd_uuid,\n",
    "                acc_path=top_key,\n",
    "                parent_is_list=False,\n",
    "                prefix=default_prefix.lower()\n",
    "            )\n",
    "        elif isinstance(top_obj, list):\n",
    "            # If it's a list at top level, handle each item\n",
    "            for i, item in enumerate(top_obj):\n",
    "                suffix = get_suffix(item, i)\n",
    "                # top path => top_key + \"_\" + suffix\n",
    "                top_path = f\"{top_key}_{suffix}\"\n",
    "                assign_ids_by_path(\n",
    "                    item,\n",
    "                    epd_uuid=epd_uuid,\n",
    "                    acc_path=top_path,\n",
    "                    parent_is_list=True,\n",
    "                    prefix=default_prefix.lower()\n",
    "                )\n",
    "\n",
    "    # 5) Print final JSON to confirm\n",
    "    # print(f\"\\n=== JSON with newly assigned IDs for doc (pipeline UUID): {uuid_val} ===\")\n",
    "    # print(json.dumps(doc, indent=2))\n",
    "    # print(\"-----------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Convert to RDF\n",
    "\n",
    "import rdflib\n",
    "from linkml.validator import Validator\n",
    "from linkml_runtime.loaders import YAMLLoader\n",
    "from linkml_runtime.dumpers import RDFLibDumper\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "\n",
    "# Import your generated Python dataclass for the schema\n",
    "# e.g., from data.linkml.py.linkml_processDataSet_schema import ProcessDataSet\n",
    "from data.linkml.py.linkml_processDataSet_schema import ProcessDataSet\n",
    "\n",
    "\n",
    "def generate_turtle_from_docs(\n",
    "    docs_dict,\n",
    "    schema_path,\n",
    "    turtle_output_path,\n",
    "    validate: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a dictionary of JSON documents (each conforming to 'ProcessDataSet'),\n",
    "    generate a single TTL file that contains all instances. If loading fails\n",
    "    for any doc, store it in a separate dictionary `failed_docs` for debugging.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    docs_dict : dict\n",
    "        A dict of {UUID: JSON-Dict} storing your EPD JSON objects in memory.\n",
    "    schema_path : str\n",
    "        Path to the LinkML YAML schema (e.g. '../data/linkml/yaml/linkml_processDataSet_schema.yaml').\n",
    "    turtle_output_path : str\n",
    "        Where to write the combined Turtle RDF graph (overwrites each run).\n",
    "    validate : bool\n",
    "        If True, runs the LinkML Validator on each instance before RDF conversion.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    failed_docs : dict\n",
    "        A dictionary of {UUID: JSON-Dict} for any documents that failed to load.\n",
    "    \"\"\"\n",
    "    # 1) Create an empty rdflib graph to combine all instance graphs\n",
    "    combined_graph = rdflib.Graph()\n",
    "\n",
    "    # 2) Load the schema into a SchemaView for RDF generation\n",
    "    sv = SchemaView(schema_path)\n",
    "\n",
    "    # (Optional) set up a validator if needed\n",
    "    validator = None\n",
    "    if validate:\n",
    "        validator = Validator(schema_path, strict=False)\n",
    "\n",
    "    # A container to track documents that fail to load\n",
    "    failed_docs = {}\n",
    "\n",
    "    # 3) For each doc, wrap in a top-level \"processDataSet\" key,\n",
    "    #    load as an object, optionally validate, and convert to RDF.\n",
    "    dumper = RDFLibDumper()\n",
    "\n",
    "    success_count = 0\n",
    "\n",
    "    for uuid_val, json_doc in docs_dict.items():\n",
    "        yaml_wrapper = {\"processDataSet\": json_doc}\n",
    "\n",
    "        # (A) Validate the doc if requested\n",
    "        if validator:\n",
    "            report = validator.validate(yaml_wrapper, \"ProcessDataSet\")\n",
    "            if report.results:\n",
    "                print(f\"[VALIDATION] Errors for UUID={uuid_val}:\")\n",
    "                for result in report.results:\n",
    "                    print(\"  -\", result.message)\n",
    "                # We can decide to skip, but let's let the user decide:\n",
    "                # continue\n",
    "            else:\n",
    "                print(f\"[VALIDATION] Document {uuid_val} is valid according to the schema.\")\n",
    "\n",
    "        # (B) Attempt to load as a ProcessDataSet\n",
    "        try:\n",
    "            instance_obj = YAMLLoader().load(yaml_wrapper[\"processDataSet\"], target_class=ProcessDataSet)\n",
    "        except (ValueError, TypeError) as e:\n",
    "            print(f\"[ERROR] Failed to load doc {uuid_val} as ProcessDataSet. Reason:\\n  {e}\")\n",
    "            failed_docs[uuid_val] = json_doc\n",
    "            continue  # Skip adding to the graph\n",
    "\n",
    "        # (C) Convert to RDF (rdflib.Graph) and accumulate\n",
    "        instance_graph = dumper.as_rdf_graph(instance_obj, schemaview=sv)\n",
    "        combined_graph += instance_graph\n",
    "        success_count += 1\n",
    "\n",
    "    # 4) Write the combined graph to Turtle\n",
    "    combined_graph.serialize(destination=turtle_output_path, format=\"turtle\")\n",
    "    print(f\"\\nSuccessfully wrote {success_count} instances to the Turtle file:\\n  {turtle_output_path}\")\n",
    "    if failed_docs:\n",
    "        print(f\"{len(failed_docs)} documents failed to load and were skipped.\")\n",
    "\n",
    "    # Return the dictionary of failed docs for further handling or debugging\n",
    "    return failed_docs\n",
    "\n",
    "# Generate RDF\n",
    "schema_file = \"../data/linkml/yaml/linkml_processDataSet_schema.yaml\"\n",
    "ttl_output = \"../data/linkml/rdf/epd_rdf_instance_datastore.ttl\"\n",
    "\n",
    "failed = generate_turtle_from_docs(\n",
    "    docs_dict=modified_json_docs,\n",
    "    schema_path=schema_file,\n",
    "    turtle_output_path=ttl_output,\n",
    "    validate=True\n",
    ")\n",
    "\n",
    "if failed:\n",
    "    print(\"\\nFailed doc details:\")\n",
    "    for bad_uuid, bad_doc in failed.items():\n",
    "        print(\" -\", bad_uuid, \"(Doc not loaded successfully)\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "failed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
