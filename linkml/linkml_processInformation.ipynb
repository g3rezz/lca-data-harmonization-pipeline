{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate only the `ProcessInformation` part of the Schema and JSON Instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overlapping type and slot names: time\n",
      "\t\n",
      "\t\n",
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pydantic model generated and written to ../linkml/data/py/linkml_processInformation_schema.py.\n"
     ]
    }
   ],
   "source": [
    "# Produce Python module from YAML schema using LinkML's Python generator\n",
    "\n",
    "from linkml.generators.pythongen import PythonGenerator\n",
    "\n",
    "\n",
    "# Define an import map so that types like \"String\", \"Boolean\", etc.\n",
    "# are resolved to their LinkML runtime equivalents.\n",
    "import_map = {\n",
    "    \"Boolean\": \"linkml_runtime.linkml_model.types.Boolean\",\n",
    "    \"String\": \"linkml_runtime.linkml_model.types.String\",\n",
    "    \"Integer\": \"linkml_runtime.linkml_model.types.Integer\",\n",
    "    \"Float\": \"linkml_runtime.linkml_model.types.Float\",\n",
    "    \"dateTime\": \"linkml_runtime.linkml_model.types.dateTime\",\n",
    "    # \"Version\": \"linkml_runtime.linkml_model.types.Version\", # Not in LinkML runtime\n",
    "    # \"anyURI\": \"linkml_runtime.linkml_model.types.anyURI\"  # Not in LinkML runtime\n",
    "}\n",
    "\n",
    "generator = PythonGenerator(\n",
    "    schema=\"../linkml/data/yaml/linkml_processInformation_schema.yaml\",\n",
    "    gen_slots=True,\n",
    "    gen_classvars=True,\n",
    "    mergeimports=True,\n",
    "    metadata=True,\n",
    "    genmeta=False,\n",
    "    importmap=import_map,\n",
    ")\n",
    "\n",
    "\n",
    "python_code = generator.serialize()\n",
    "\n",
    "python_gen_file = \"../linkml/data/py/linkml_processInformation_schema.py\"\n",
    "\n",
    "with open(python_gen_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "print(f\"Pydantic model generated and written to {python_gen_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processInformation EPD YAML instance written to: ../linkml/data/yaml/linkml_processInformation_instance.yaml\n",
      "The minimal instance is valid!\n",
      "Instance loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Instance Generation and Loading processInformation EPD YAML\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "from data.py.linkml_processInformation_schema import ProcessInformation\n",
    "from linkml.validator import Validator\n",
    "from linkml_runtime.loaders.yaml_loader import YAMLLoader\n",
    "\n",
    "\n",
    "def odict_to_dict(obj):\n",
    "    \"\"\"\n",
    "    Recursively convert an OrderedDict (or list/dict containing OrderedDicts) to a standard dict.\n",
    "\n",
    "    Parameters:\n",
    "        obj: An OrderedDict, list, or dict potentially containing OrderedDicts.\n",
    "\n",
    "    Returns:\n",
    "        A new structure where all OrderedDict instances are replaced with standard dicts.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, OrderedDict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [odict_to_dict(item) for item in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: odict_to_dict(v) for k, v in obj.items()}\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def build_minimal_epd_structure(data):\n",
    "    \"\"\"\n",
    "    Build a minimal nested dictionary according to our minimal schema.\n",
    "    This function assumes that the JSON instance already contains a \"processInformation\" key.\n",
    "\n",
    "    Parameters:\n",
    "        data (dict): A dictionary loaded from a JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary structured with a top-level 'processInformation' key.\n",
    "    \"\"\"\n",
    "    instance_dict = {\"processInformation\": data.get(\"processInformation\", {})}\n",
    "    return instance_dict\n",
    "\n",
    "\n",
    "def process_instance(input_path, schema_path, output_yaml_path):\n",
    "    \"\"\"\n",
    "    Process an instance by loading a minimal JSON instance, converting it, dumping it to YAML,\n",
    "    validating it against a schema, and loading it with YAMLLoader.\n",
    "\n",
    "    Steps:\n",
    "      1. Load a minimal JSON instance containing only the processInformation section.\n",
    "      2. Convert OrderedDicts to dicts.\n",
    "      3. Wrap the data in a top-level ProcessInformation structure.\n",
    "      4. Dump the resulting instance to a YAML file.\n",
    "      5. Validate the instance using a schema validator.\n",
    "      6. Load the YAML instance with YAMLLoader.\n",
    "\n",
    "    Parameters:\n",
    "        input_path (str): Path to the input JSON file.\n",
    "        schema_path (str): Path to the YAML schema file.\n",
    "        output_yaml_path (str): Path where the output YAML file will be written.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If an error occurs during validation or instance loading.\n",
    "    \"\"\"\n",
    "    # Load the minimal JSON instance (containing only processInformation)\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    data = odict_to_dict(data)\n",
    "\n",
    "    # Build the nested instance structure.\n",
    "    instance_data = build_minimal_epd_structure(data)\n",
    "\n",
    "    # Dump to YAML\n",
    "    with open(output_yaml_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        yaml.dump(instance_data, out, sort_keys=False, allow_unicode=True)\n",
    "    print(f\"processInformation EPD YAML instance written to: {output_yaml_path}\")\n",
    "\n",
    "    # Validate using the Validator\n",
    "    try:\n",
    "        validator = Validator(schema_path, strict=False)\n",
    "        report = validator.validate(instance_data, \"ProcessInformation\")\n",
    "        if report.results:\n",
    "            print(\"Validation errors:\")\n",
    "            for result in report.results:\n",
    "                print(result.message)\n",
    "        else:\n",
    "            print(\"The minimal instance is valid!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during validation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Load the YAML instance using YAMLLoader\n",
    "    try:\n",
    "        loader = YAMLLoader()\n",
    "        # If the instance is wrapped in a \"processInformation\" key, unwrap it.\n",
    "        if \"processInformation\" in instance_data:\n",
    "            instance_data = instance_data[\"processInformation\"]\n",
    "        loader.load(instance_data, target_class=ProcessInformation)\n",
    "        print(\"Instance loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading via YAMLLoader:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Usage\n",
    "input_file = \"../linkml/data/json/instance_processInformation.json\"\n",
    "schema_file = \"../linkml/data/yaml/linkml_processInformation_schema.yaml\"\n",
    "output_yaml = \"../linkml/data/yaml/linkml_processInformation_instance.yaml\"\n",
    "\n",
    "process_instance(input_file, schema_file, output_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Georgi\\.conda\\envs\\linkml-env\\Lib\\site-packages\\linkml_runtime\\linkml_model\\model\\schema\\types does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance loaded from: ../linkml/data/yaml/linkml_processInformation_instance.yaml\n",
      "SchemaView created successfully.\n",
      "RDF (JSON‑LD) successfully written to: ../linkml/data/rdf/linkml_processInformation_instance.jsonld\n",
      "RDF (Turtle) successfully written to: ../linkml/data/rdf/linkml_processInformation_instance.ttl\n"
     ]
    }
   ],
   "source": [
    "# Loading and RDF (JSON-LD, Turtle) Generation processInformation EPD YAML Instance\n",
    "\n",
    "import yaml\n",
    "from linkml_runtime.dumpers import RDFLibDumper\n",
    "from linkml_runtime.loaders import YAMLLoader\n",
    "from linkml_runtime.utils.schemaview import SchemaView\n",
    "from data.py.linkml_processInformation_schema import ProcessInformation\n",
    "\n",
    "\n",
    "def generate_rdf(schema_path: str, instance_path: str, output_jsonld_path: str, output_turtle_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Load a validated LinkML YAML instance, create a SchemaView, and dump the instance to RDF in JSON-LD and Turtle formats.\n",
    "\n",
    "    Steps:\n",
    "        1. Load the instance from a YAML file.\n",
    "        2. Create a SchemaView from the schema file.\n",
    "        3. Generate RDF in JSON-LD and Turtle formats.\n",
    "        4. Write the RDF to files.\n",
    "    \n",
    "    Parameters:\n",
    "        schema_path (str): Path to the LinkML YAML schema file.\n",
    "        instance_path (str): Path to the validated LinkML YAML instance file.\n",
    "        output_jsonld_path (str): Path where the output JSON-LD file will be written.\n",
    "        output_turtle_path (str): Path where the output Turtle file will be written.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during instance loading, SchemaView creation, or RDF generation.\n",
    "    \"\"\"\n",
    "    # Load instance from file\n",
    "    try:\n",
    "        with open(instance_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            loaded_data = yaml.safe_load(f)  # This is now a Python dict\n",
    "        # Extract \"processInformation\"\n",
    "        if \"processInformation\" in loaded_data:\n",
    "            loaded_data = loaded_data[\"processInformation\"]\n",
    "        # Feed loaded_data dictionary into the loader\n",
    "        instance_obj = YAMLLoader().load(loaded_data, target_class=ProcessInformation)\n",
    "        print(f\"Instance loaded from: {instance_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during instance loading:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Create SchemaView\n",
    "    try:\n",
    "        sv = SchemaView(schema_path)\n",
    "        print(\"SchemaView created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during SchemaView creation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    # Generate and write RDF (JSON-LD, Turtle)\n",
    "    try:\n",
    "        dumper = RDFLibDumper()\n",
    "        # JSON‑LD format\n",
    "        rdf_jsonld = dumper.dumps(instance_obj, schemaview=sv, fmt=\"json-ld\")\n",
    "        with open(output_jsonld_path, \"w\", encoding=\"utf-8\") as rdf_file:\n",
    "            rdf_file.write(rdf_jsonld)\n",
    "        print(f\"RDF (JSON‑LD) successfully written to: {output_jsonld_path}\")\n",
    "        # Turtle format\n",
    "        rdf_turtle = dumper.dumps(instance_obj, schemaview=sv, fmt=\"turtle\")\n",
    "        with open(output_turtle_path, \"w\", encoding=\"utf-8\") as ttl_file:\n",
    "            ttl_file.write(rdf_turtle)\n",
    "        print(f\"RDF (Turtle) successfully written to: {output_turtle_path}\")\n",
    "    except Exception as e:\n",
    "        print(\"Exception during RDF generation:\")\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "schema_path = \"../linkml/data/yaml/linkml_processInformation_schema.yaml\"\n",
    "instance_path = \"../linkml/data/yaml/linkml_processInformation_instance.yaml\"\n",
    "output_jsonld_path = \"../linkml/data/rdf/linkml_processInformation_instance.jsonld\"\n",
    "output_turtle_path = \"../linkml/data/rdf/linkml_processInformation_instance.ttl\"\n",
    "\n",
    "generate_rdf(schema_path, instance_path, output_jsonld_path, output_turtle_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
