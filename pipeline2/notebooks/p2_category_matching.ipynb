{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unmatched Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "import gc\n",
    "import re\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Custom SentenceTransformer Embeddings\n",
    "##################################################\n",
    "class CustomSentenceTransformerEmbeddings(Embeddings):\n",
    "    \"\"\"\n",
    "    Allows using a SentenceTransformer model within a LangChain-based FAISS store.\n",
    "    Handles initialization of different models with specific arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model_name: str):\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.model = self._initialize_model()\n",
    "\n",
    "    def _initialize_model(self) -> SentenceTransformer:\n",
    "        model_configs = {\n",
    "            \"jinaai/jina-embeddings-v3\": {\n",
    "                \"trust_remote_code\": True,\n",
    "                \"revision\": \"main\",\n",
    "                \"device\": \"cuda\",\n",
    "                \"model_kwargs\": {\"use_flash_attn\": False},\n",
    "            },\n",
    "            \"HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5\": {\n",
    "                \"local_files_only\": True,\n",
    "                \"device\": \"cuda\",\n",
    "                \"model_kwargs\": {\"attn_implementation\": \"eager\"},\n",
    "            },\n",
    "            \"Alibaba-NLP/gte-large-en-v1.5\": {\n",
    "                \"trust_remote_code\": True,\n",
    "                \"revision\": \"main\",\n",
    "                \"device\": \"cuda\",\n",
    "                \"model_kwargs\": {\"attn_implementation\": \"eager\"},\n",
    "            },\n",
    "        }\n",
    "        config = model_configs.get(\n",
    "            self.embedding_model_name,\n",
    "            {\"device\": \"cuda\", \"model_kwargs\": {}},  # default fallback\n",
    "        )\n",
    "        try:\n",
    "            model = SentenceTransformer(self.embedding_model_name, **config)\n",
    "            print(f\"Initialized SentenceTransformer model: {self.embedding_model_name}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing model {self.embedding_model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text).tolist()\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(texts).tolist()\n",
    "\n",
    "    def unload_model(self):\n",
    "        if self.model:\n",
    "            del self.model\n",
    "            self.model = None\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            print(f\"Unloaded embedding model: {self.embedding_model_name}\")\n",
    "        else:\n",
    "            print(\"[DEBUG] Embedding model was already None or not set.\")\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Build Context and Save as JSON\n",
    "##################################################\n",
    "def build_context_and_save(\n",
    "    df: pd.DataFrame,\n",
    "    vectorstore: FAISS,\n",
    "    k_value: int = 5,\n",
    "    output_file: str = \"../../data/pipeline2/json/context.json\",\n",
    "):\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame:\n",
    "      1) Build a query from the row (using Product Name, Technology Description, Classification, and Applicability).\n",
    "      2) Use similarity_search_with_score to get (Document, original L2 distance) tuples.\n",
    "      3) Compute transformed_score = 1 - distance so that higher is better.\n",
    "      4) Sort the results by transformed_score descending.\n",
    "      5) Keep only those documents whose transformed_score is within 0.2 of the best score.\n",
    "      6) Build a JSON object for each product row with all qualifying category_score entries.\n",
    "      7) Aggregate all JSON objects and save them to one JSON file.\n",
    "    \"\"\"\n",
    "    all_payloads = []\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        product_name = row.get(\"Product Name\", \"N/A\")\n",
    "        classification = row.get(\"Classification\", \"N/A\")\n",
    "        technology_description = row.get(\"Technology Description\", \"N/A\")\n",
    "        technological_applicability = row.get(\"Technological Applicability\", \"N/A\")\n",
    "        flow_prop_name = row.get(\"Flow Property Name\", \"N/A\")\n",
    "        flow_prop_mean_value = row.get(\"Flow Property Mean Value\", \"N/A\")\n",
    "        flow_prop_ref_unit = row.get(\"Flow Property Reference Unit\", \"N/A\")\n",
    "        uuid_val = row.get(\"UUID\", \"N/A\")\n",
    "\n",
    "        # Ensure non-NaN values\n",
    "        if pd.isna(technology_description):\n",
    "            technology_description = \"N/A\"\n",
    "        if pd.isna(technological_applicability):\n",
    "            technological_applicability = \"N/A\"\n",
    "        if pd.isna(flow_prop_name):\n",
    "            flow_prop_name = \"N/A\"\n",
    "        if pd.isna(flow_prop_mean_value):\n",
    "            flow_prop_mean_value = \"N/A\"\n",
    "        if pd.isna(flow_prop_ref_unit):\n",
    "            flow_prop_ref_unit = \"N/A\"\n",
    "\n",
    "        # Build the query string\n",
    "        query_str = (\n",
    "            f\"Product: {product_name}\\n\"\n",
    "        )\n",
    "\n",
    "        if classification != \"N/A\":\n",
    "            query_str += f\"Classification: {classification}\\n\"\n",
    "        if technology_description != \"N/A\":\n",
    "            query_str += f\"Description: {technology_description}\\n\"\n",
    "        if technological_applicability != \"N/A\":\n",
    "            query_str += f\"Applicability: {technological_applicability}\\n\"\n",
    "        if flow_prop_name != \"N/A\":\n",
    "            query_str += f\"Flow Property Name: {flow_prop_name}\\n\"\n",
    "        if flow_prop_mean_value != \"N/A\":\n",
    "            query_str += f\"Flow Property Mean Value: {flow_prop_mean_value}\\n\"\n",
    "        if flow_prop_ref_unit != \"N/A\":\n",
    "            query_str += f\"Flow Property Reference Unit: {flow_prop_ref_unit}\\n\"\n",
    "\n",
    "        print(\"=== Query ===\")\n",
    "        print(query_str)\n",
    "\n",
    "        # Retrieve documents with original L2 distance scores (lower is better)\n",
    "        docs_with_scores = vectorstore.similarity_search_with_score(\n",
    "            query_str, k=k_value\n",
    "        )\n",
    "\n",
    "        # Compute transformed scores (1 - original_score) so that higher is better\n",
    "        transformed_docs = [\n",
    "            (doc, 1 - orig_score) for doc, orig_score in docs_with_scores\n",
    "        ]\n",
    "\n",
    "        print(\"=== Retrieved Documents ===\")\n",
    "        for doc, transformed_score in transformed_docs:\n",
    "            print(f\"[{transformed_score:.4f}] {doc.page_content}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        if not transformed_docs:\n",
    "            print(\"No documents retrieved for this query.\\n\")\n",
    "            continue\n",
    "\n",
    "        # Sort by transformed score descending (highest first)\n",
    "        transformed_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        best_score = transformed_docs[0][1]\n",
    "\n",
    "        # Dynamic filtering: include docs within 0.2 of the best score\n",
    "        filtered_docs = [\n",
    "            (doc, score)\n",
    "            for doc, score in transformed_docs\n",
    "            if (best_score - score) <= 0.1\n",
    "        ]\n",
    "\n",
    "        if not filtered_docs:\n",
    "            # Fallback: always include the best document\n",
    "            filtered_docs = [transformed_docs[0]]\n",
    "            print(\n",
    "                \"No documents passed the dynamic filtering; using the top result only.\\n\"\n",
    "            )\n",
    "\n",
    "        # Group filtered documents for the current product into one payload\n",
    "        payload = {\n",
    "            \"Product\": product_name,\n",
    "            \"UUID\": uuid_val,\n",
    "            \"Description\": technology_description,\n",
    "            \"Applicability\": technological_applicability,\n",
    "            \"Flow Property Name\": flow_prop_name,\n",
    "            \"Flow Property Mean Value\": flow_prop_mean_value,\n",
    "            \"Flow Property Reference Unit\": flow_prop_ref_unit,\n",
    "            \"category_score\": [],\n",
    "        }\n",
    "        for doc, transformed_score in filtered_docs:\n",
    "            payload[\"category_score\"].append(\n",
    "                {\"category\": doc.page_content, \"score\": float(transformed_score)}\n",
    "            )\n",
    "        all_payloads.append(payload)\n",
    "\n",
    "    # Save all JSON payloads into one file as a list\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_payloads, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\nSaved context JSON to: {output_file}\")\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Main Flow\n",
    "##################################################\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 1) Load the CSV file\n",
    "    summaries_path = \"../../data/pipeline2/sql/regex_classified/filtered_epd_data02_classified_concrete05.csv\"\n",
    "    df = pd.read_csv(summaries_path)\n",
    "    df = df[df[\"RegEx Classification\"].str.match(r\"^Concrete$\", case=True, na=False)]\n",
    "    print(f\"Loaded {len(df)} rows from {summaries_path}\")\n",
    "\n",
    "    # 2) Load FAISS vectorstore\n",
    "    embedding_model_name = \"mxbai-embed-large:latest\"\n",
    "    embeddings = OllamaEmbeddings(model=embedding_model_name)\n",
    "    faiss_path = \"../../data/pipeline2/embeddings/pipeline2/mxbai-embed-large/faiss_index_COS_EN\"\n",
    "    vectorstore = FAISS.load_local(\n",
    "        faiss_path,\n",
    "        embeddings=embeddings,\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "\n",
    "    k_value = 10\n",
    "\n",
    "    # 4) Create JSON output\n",
    "    build_context_and_save(\n",
    "        df=df,\n",
    "        vectorstore=vectorstore,\n",
    "        k_value=k_value,\n",
    "        output_file=\"../../data/pipeline2/json/TIES_concrete.json\",\n",
    "    )\n",
    "\n",
    "    # Unload model to free GPU memory\n",
    "    if isinstance(embeddings, CustomSentenceTransformerEmbeddings):\n",
    "        embeddings.unload_model()\n",
    "    else:\n",
    "        print(f\"Done with {embedding_model_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select One Category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Batch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "# 1) Load your API key\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Pydantic model to enforce structured output\n",
    "class BestCategoryResponse(BaseModel):\n",
    "    best_category: str\n",
    "\n",
    "# 2) Read your product data\n",
    "with open(\n",
    "    \"../../data/pipeline2/json/TIES_concrete.json\",\n",
    "    \"r\",\n",
    "    encoding=\"utf-8\",\n",
    ") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "base_prompt = \"\"\"\\\n",
    "You are an expert in product categorization. The following product information comes from an Environmental Product Declaration (EPD).\n",
    "Review the details below and choose the best category from the list, considering both the score values and the contextual product details.\n",
    "\n",
    "Pick only one best category.\n",
    "\"\"\"\n",
    "\n",
    "def build_request(i, product):\n",
    "    # Remove keys with \"N/A\"\n",
    "    clean_product = {k: v for k, v in product.items() if v != \"N/A\"}\n",
    "\n",
    "    # Build context string\n",
    "    fields = [\n",
    "        (\"Product\", \"Product\"),\n",
    "        (\"Description\", \"Description\"),\n",
    "        (\"Applicability\", \"Applicability\"),\n",
    "        (\"Flow Property Name\", \"Flow Property Name\"),\n",
    "        (\"Flow Property Mean Value\", \"Flow Property Mean Value\"),\n",
    "        (\"Flow Property Reference Unit\", \"Flow Property Reference Unit\"),\n",
    "    ]\n",
    "    context_parts = [\n",
    "        f\"- {label}: {value}\"\n",
    "        for key, label in fields\n",
    "        for value in [clean_product.get(key, \"\")]\n",
    "        if value  # only include if non-empty\n",
    "    ]\n",
    "\n",
    "    # Category scores with two decimals\n",
    "    category_scores = clean_product.get(\"category_score\", [])\n",
    "    categories_str = \"\\n\".join(\n",
    "        f\"- {c['category']} (score: {c['score']:.2f})\"\n",
    "        for c in category_scores\n",
    "    )\n",
    "\n",
    "    # Final user prompt\n",
    "    final_prompt = (\n",
    "        \"Product Details:\\n\"\n",
    "        + \"\\n\".join(context_parts)\n",
    "        + \"\\n\\nPossible Categories:\\n\"\n",
    "        + categories_str\n",
    "        + \"\\n\\nWhich category is best? Please respond in valid json format.\"\n",
    "    )\n",
    "\n",
    "    # (Optional) Print the final prompt for debugging\n",
    "    print(\"\\n============ Final Prompt ============\")\n",
    "    print(final_prompt)\n",
    "    print(\"======================================\\n\")\n",
    "\t\n",
    "    # Generate JSON Schema from the Pydantic model\n",
    "    schema = BestCategoryResponse.model_json_schema()\n",
    "    schema[\"additionalProperties\"] = False\n",
    "\n",
    "    return {\n",
    "        \"custom_id\": f\"req_{i}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"o3-mini\",\n",
    "            \"reasoning_effort\": \"low\",\n",
    "            \"messages\": [\n",
    "\t\t\t\t{\"role\": \"system\", \"content\": base_prompt},\n",
    "                {\"role\": \"user\", \"content\": final_prompt}\n",
    "            ],\n",
    "            \"response_format\": {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"BestCategoryResponse\",\n",
    "                    \"schema\": schema,\n",
    "                    \"strict\": True\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "# 3) Write out the batch‐input JSONL file\n",
    "input_jsonl_file = \"../../data/pipeline2/json/openai/batch_input_TIES_concrete.jsonl\"\n",
    "os.makedirs(os.path.dirname(input_jsonl_file), exist_ok=True)\n",
    "with open(input_jsonl_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for i, product in enumerate(products):\n",
    "        req_obj = build_request(i, product)\n",
    "        out_file.write(json.dumps(req_obj) + \"\\n\")\n",
    "\n",
    "print(f\"Created {input_jsonl_file} with {len(products)} lines.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 4) Upload the input file to OpenAI for batch processing\n",
    "batch_input_file = client.files.create(\n",
    "    file=open(input_jsonl_file, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "print(\"Uploaded file:\", batch_input_file)\n",
    "\n",
    "# 5) Create the batch job (with a 24-hour completion window)\n",
    "batch = client.batches.create(\n",
    "    input_file_id=batch_input_file.id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\"description\": \"The International EPD System concrete product category classification\"}\n",
    ")\n",
    "print(\"Created Batch:\", batch)\n",
    "\n",
    "# 6) Save the batch id for later status checks\n",
    "batch_id = batch.id\n",
    "print(f\"Batch submitted with id: {batch_id}.\")\n",
    "print(\"You can check the status later with:\")\n",
    "print(f\"client.batches.retrieve('{batch_id}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual batch id\n",
    "batch_id = \"batch_6814cdb67bc08190902c4a7f1940df1d\"\n",
    "batch_status = client.batches.retrieve(batch_id)\n",
    "print(\"Current batch status:\", batch_status.status)\n",
    "\n",
    "if batch_status.status == \"completed\":\n",
    "    output_file_id = batch_status.output_file_id\n",
    "    if output_file_id:\n",
    "        output_jsonl_file = f\"../../data/pipeline2/json/openai/batch_output_{batch_id}.jsonl\"\n",
    "        file_response = client.files.content(output_file_id)\n",
    "        with open(output_jsonl_file, \"wb\") as f:\n",
    "            f.write(file_response.content)\n",
    "        print(f\"Batch results saved to {output_jsonl_file}\")\n",
    "    else:\n",
    "        print(\"No output file available yet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel the batch if needed\n",
    "client.batches.cancel(\"batch_6814d7b49e988190a46ba0036dfc9df6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# Print batch jobs and their status\n",
    "for batch in list(client.batches.list()):\n",
    "    pprint.pprint(batch.model_dump())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output file\n",
    "output_file_id = \"file-KtupsEnFunSJxoUQcycSQ3\"\n",
    "output_file = client.files.content(output_file_id)\n",
    "print(output_file.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check error file\n",
    "error_file_id = \"file-X2X7sMjyw2tpLoomziXjwz\"\n",
    "error_file = client.files.content(error_file_id)\n",
    "print(error_file.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPDNorge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "import gc\n",
    "import re\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Custom SentenceTransformer Embeddings\n",
    "##################################################\n",
    "class CustomSentenceTransformerEmbeddings(Embeddings):\n",
    "    \"\"\"\n",
    "    Allows using a SentenceTransformer model within a LangChain-based FAISS store.\n",
    "    Handles initialization of different models with specific arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model_name: str):\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.model = self._initialize_model()\n",
    "\n",
    "    def _initialize_model(self) -> SentenceTransformer:\n",
    "        model_configs = {\n",
    "            \"jinaai/jina-embeddings-v3\": {\n",
    "                \"trust_remote_code\": True,\n",
    "                \"revision\": \"main\",\n",
    "                \"device\": \"cuda\",\n",
    "                \"model_kwargs\": {\"use_flash_attn\": False},\n",
    "            },\n",
    "            \"HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5\": {\n",
    "                \"local_files_only\": True,\n",
    "                \"device\": \"cuda\",\n",
    "                \"model_kwargs\": {\"attn_implementation\": \"eager\"},\n",
    "            },\n",
    "            \"Alibaba-NLP/gte-large-en-v1.5\": {\n",
    "                \"trust_remote_code\": True,\n",
    "                \"revision\": \"main\",\n",
    "                \"device\": \"cuda\",\n",
    "                \"model_kwargs\": {\"attn_implementation\": \"eager\"},\n",
    "            },\n",
    "        }\n",
    "        config = model_configs.get(\n",
    "            self.embedding_model_name,\n",
    "            {\"device\": \"cuda\", \"model_kwargs\": {}},  # default fallback\n",
    "        )\n",
    "        try:\n",
    "            model = SentenceTransformer(self.embedding_model_name, **config)\n",
    "            print(f\"Initialized SentenceTransformer model: {self.embedding_model_name}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing model {self.embedding_model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text).tolist()\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(texts).tolist()\n",
    "\n",
    "    def unload_model(self):\n",
    "        if self.model:\n",
    "            del self.model\n",
    "            self.model = None\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            print(f\"Unloaded embedding model: {self.embedding_model_name}\")\n",
    "        else:\n",
    "            print(\"[DEBUG] Embedding model was already None or not set.\")\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Build Context and Save as JSON\n",
    "##################################################\n",
    "def build_context_and_save(\n",
    "    df: pd.DataFrame,\n",
    "    vectorstore: FAISS,\n",
    "    k_value: int = 5,\n",
    "    output_file: str = \"../../data/pipeline2/json/context.json\",\n",
    "):\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame:\n",
    "      1) Build a query from the row (using Product Name, Technology Description, Classification, and Applicability).\n",
    "      2) Use similarity_search_with_score to get (Document, original L2 distance) tuples.\n",
    "      3) Compute transformed_score = 1 - distance so that higher is better.\n",
    "      4) Sort the results by transformed_score descending.\n",
    "      5) Keep only those documents whose transformed_score is within 0.2 of the best score.\n",
    "      6) Build a JSON object for each product row with all qualifying category_score entries.\n",
    "      7) Aggregate all JSON objects and save them to one JSON file.\n",
    "    \"\"\"\n",
    "    all_payloads = []\n",
    "    payload_id = 0  \n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        product_name = row.get(\"Product Name\", \"N/A\")\n",
    "        classification = row.get(\"Classification\", \"N/A\")\n",
    "        lca_method_details = row.get(\"LCA Method Details\", \"N/A\")\n",
    "        technology_description = row.get(\"Technology Description\", \"N/A\")\n",
    "        technological_applicability = row.get(\"Technological Applicability\", \"N/A\")\n",
    "        flow_prop_name = row.get(\"Flow Property Name\", \"N/A\")\n",
    "        flow_prop_mean_value = row.get(\"Flow Property Mean Value\", \"N/A\")\n",
    "        flow_prop_ref_unit = row.get(\"Flow Property Reference Unit\", \"N/A\")\n",
    "        uuid_val = row.get(\"UUID\", \"N/A\")\n",
    "\n",
    "        # Ensure non-NaN values\n",
    "        if pd.isna(lca_method_details):\n",
    "            lca_method_details = \"N/A\"\n",
    "        if pd.isna(technology_description):\n",
    "            technology_description = \"N/A\"\n",
    "        if pd.isna(technological_applicability):\n",
    "            technological_applicability = \"N/A\"\n",
    "        if pd.isna(flow_prop_name):\n",
    "            flow_prop_name = \"N/A\"\n",
    "        if pd.isna(flow_prop_mean_value):\n",
    "            flow_prop_mean_value = \"N/A\"\n",
    "        if pd.isna(flow_prop_ref_unit):\n",
    "            flow_prop_ref_unit = \"N/A\"\n",
    "\n",
    "        # Build the query string\n",
    "        query_str = (\n",
    "            f\"Product: {product_name}\\n\"\n",
    "        )\n",
    "\n",
    "        if classification != \"N/A\":\n",
    "            query_str += f\"Classification: {classification}\\n\"\n",
    "        if lca_method_details != \"N/A\":\n",
    "            query_str += f\"PCR: {lca_method_details}\\n\"\n",
    "        if technology_description != \"N/A\":\n",
    "            query_str += f\"Description: {technology_description}\\n\"\n",
    "        if technological_applicability != \"N/A\":\n",
    "            query_str += f\"Applicability: {technological_applicability}\\n\"\n",
    "        # if flow_prop_name != \"N/A\":\n",
    "        #     query_str += f\"Flow Property Name: {flow_prop_name}\\n\"\n",
    "        # if flow_prop_mean_value != \"N/A\":\n",
    "        #     query_str += f\"Flow Property Mean Value: {flow_prop_mean_value}\\n\"\n",
    "        # if flow_prop_ref_unit != \"N/A\":\n",
    "        #     query_str += f\"Flow Property Reference Unit: {flow_prop_ref_unit}\\n\"\n",
    "\n",
    "        print(\"=== Query ===\")\n",
    "        print(query_str)\n",
    "\n",
    "        # Retrieve documents with original L2 distance scores (lower is better)\n",
    "        docs_with_scores = vectorstore.similarity_search_with_score(\n",
    "            query_str, k=k_value\n",
    "        )\n",
    "\n",
    "        # Compute transformed scores (1 - original_score) so that higher is better\n",
    "        transformed_docs = [\n",
    "            (doc, 1 - orig_score) for doc, orig_score in docs_with_scores\n",
    "        ]\n",
    "\n",
    "        print(\"=== Retrieved Documents ===\")\n",
    "        for doc, transformed_score in transformed_docs:\n",
    "            print(f\"[{transformed_score:.4f}] {doc.page_content}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        if not transformed_docs:\n",
    "            print(\"No documents retrieved for this query.\\n\")\n",
    "            continue\n",
    "\n",
    "        # Sort by transformed score descending (highest first)\n",
    "        transformed_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        best_score = transformed_docs[0][1]\n",
    "\n",
    "        # Dynamic filtering: include docs within 0.2 of the best score\n",
    "        filtered_docs = [\n",
    "            (doc, score)\n",
    "            for doc, score in transformed_docs\n",
    "            if (best_score - score) <= 0.1\n",
    "        ]\n",
    "        filtered_docs = transformed_docs\n",
    "\n",
    "        if not filtered_docs:\n",
    "            # Fallback: always include the best document\n",
    "            filtered_docs = [transformed_docs[0]]\n",
    "            print(\n",
    "                \"No documents passed the dynamic filtering; using the top result only.\\n\"\n",
    "            )\n",
    "\n",
    "        # Group filtered documents for the current product into one payload\n",
    "        payload = {\n",
    "            \"id\": payload_id, \n",
    "            \"Product\": product_name,\n",
    "            \"UUID\": uuid_val,\n",
    "            \"Classification\": classification,\n",
    "            \"PCR\": lca_method_details,\n",
    "            \"Description\": technology_description,\n",
    "            \"Applicability\": technological_applicability,\n",
    "            \"Flow Property Name\": flow_prop_name,\n",
    "            \"Flow Property Mean Value\": flow_prop_mean_value,\n",
    "            \"Flow Property Reference Unit\": flow_prop_ref_unit,\n",
    "            \"category_score\": [],\n",
    "        }\n",
    "        for doc, transformed_score in filtered_docs:\n",
    "            payload[\"category_score\"].append(\n",
    "                {\"category\": doc.page_content, \"score\": float(transformed_score)}\n",
    "            )\n",
    "        all_payloads.append(payload)\n",
    "        payload_id += 1\n",
    "\n",
    "    # Save all JSON payloads into one file as a list\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_payloads, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\nSaved context JSON to: {output_file}\")\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Main Flow\n",
    "##################################################\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 1) Load the CSV file\n",
    "    summaries_path = \"../../data/pipeline2/sql/filtered_epd_other_categories.csv\"\n",
    "    df = pd.read_csv(summaries_path)\n",
    "    df = df[df[\"Classification System\"].str.match(r\"EPDNorge\", case=True, na=False)]\n",
    "    print(f\"Loaded {len(df)} rows from {summaries_path}\")\n",
    "\n",
    "    # 2) Load FAISS vectorstore\n",
    "    embedding_model_name = \"jinaai/jina-embeddings-v3\"\n",
    "    embeddings = CustomSentenceTransformerEmbeddings(embedding_model_name)\n",
    "    \n",
    "    faiss_path = \"../../data/pipeline2/embeddings/pipeline2/jinaai_jina-embeddings-v3/faiss_index_COS_EN\"\n",
    "    vectorstore = FAISS.load_local(\n",
    "        faiss_path,\n",
    "        embeddings=embeddings,\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "\n",
    "    k_value = 20\n",
    "\n",
    "    timestamp_str = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    if embedding_model_name == \"jinaai/jina-embeddings-v3\":\n",
    "        model_name = embedding_model_name.replace(\"/\", \"_\")\n",
    "    elif embedding_model_name.startswith(\"HIT-TMG\") or embedding_model_name.startswith(\"Alibaba-NLP\"):\n",
    "        model_name = embedding_model_name.replace(\"/\", \"_\")\n",
    "    elif embedding_model_name.startswith(\"granite-embedding\"):\n",
    "        model_name = embedding_model_name.replace(\":\", \"-\")\n",
    "    elif \":\" in embedding_model_name:\n",
    "        model_name = embedding_model_name.split(\":\")[0].replace(\"/\", \"_\")\n",
    "    else:\n",
    "        model_name = embedding_model_name.replace(\"/\", \"_\")\n",
    "\n",
    "    # 4) Create JSON output\n",
    "    build_context_and_save(\n",
    "        df=df,\n",
    "        vectorstore=vectorstore,\n",
    "        k_value=k_value,\n",
    "        output_file=f\"../../data/pipeline2/json/context_{model_name}_{timestamp_str}.json\",\n",
    "    )\n",
    "\n",
    "    # Unload model to free GPU memory\n",
    "    if isinstance(embeddings, CustomSentenceTransformerEmbeddings):\n",
    "        embeddings.unload_model()\n",
    "    else:\n",
    "        print(f\"Done with {embedding_model_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# 1) Load your API key\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "\n",
    "# Pydantic model to enforce structured output\n",
    "class BestCategoryResponse(BaseModel):\n",
    "    best_category: str\n",
    "\n",
    "\n",
    "input_jsonl_file = \"../../data/pipeline2/json/openai/batch_input_EPDNorge_concrete_20250503134414.jsonl\"  # The file we'll create & upload\n",
    "\n",
    "# 2) Read your product data (all products or a subset)\n",
    "with open(\n",
    "    \"../../data/pipeline2/json/context_jinaai_jina-embeddings-v3_20250503134414.json\",\n",
    "    \"r\",\n",
    "    encoding=\"utf-8\",\n",
    ") as file:\n",
    "    products = json.load(file)\n",
    "\n",
    "base_prompt = \"\"\"\\\n",
    "You are an expert in product categorization. The following product information comes from an Environmental Product Declaration (EPD).\n",
    "\n",
    "Your task:\n",
    "- Review the product details and the list of possible categories.\n",
    "- Treat the numeric scores only as guidance—choose the category that best fits the product based on its description and applicability, even if it is not the top score.\n",
    "- Do not simply pick the highest-scoring category.\n",
    "- Use exactly one of the listed categories, matching its name character-for-character.\n",
    "- Do not invent any new categories.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def build_request(i, product):\n",
    "    # Remove keys with \"N/A\"\n",
    "    clean_product = {k: v for k, v in product.items() if v != \"N/A\"}\n",
    "\n",
    "    # Build context string\n",
    "    fields = [\n",
    "        (\"Product\", \"Product\"),\n",
    "        (\"Classification\", \"Classification\"),\n",
    "        (\"PCR\", \"PCR\"),\n",
    "        (\"Description\", \"Description\"),\n",
    "        (\"Applicability\", \"Applicability\"),\n",
    "        (\"Flow Property Name\", \"Flow Property Name\"),\n",
    "        (\"Flow Property Mean Value\", \"Flow Property Mean Value\"),\n",
    "        (\"Flow Property Reference Unit\", \"Flow Property Reference Unit\"),\n",
    "    ]\n",
    "\n",
    "    context_parts = [\n",
    "        f\"- {label}: {value}\"\n",
    "        for key, label in fields\n",
    "        for value in [clean_product.get(key, \"\")]\n",
    "        if value  # only include if non-empty\n",
    "    ]\n",
    "    # Category scores with two decimals\n",
    "    category_scores = clean_product.get(\"category_score\", [])\n",
    "    categories_str = \"\\n\".join(\n",
    "        f\"- {c['category']} (score: {c['score']:.2f})\" for c in category_scores\n",
    "    )\n",
    "\n",
    "    final_prompt = (\n",
    "        \"Product Details:\\n\"\n",
    "        + \"\\n\".join(context_parts)\n",
    "        + \"\\n\\nPossible Categories:\\n\"\n",
    "        + categories_str\n",
    "        + \"\\n\\nWhich category is best? Please respond in valid json format.\"\n",
    "    )\n",
    "\n",
    "    # (Optional) Print the final prompt for debugging\n",
    "    print(\"\\n============ Final Prompt ============\")\n",
    "    print(final_prompt)\n",
    "    print(\"======================================\\n\")\n",
    "\n",
    "    # Generate schema from the Pydantic model and add \"additionalProperties\": false\n",
    "    schema = BestCategoryResponse.model_json_schema()\n",
    "    schema[\"additionalProperties\"] = False\n",
    "\n",
    "    # Return a dict for a single request\n",
    "    return {\n",
    "        \"custom_id\": f\"req_{i}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"o3-mini\",\n",
    "            \"reasoning_effort\": \"high\", # price: low <0.50; high <2.00\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": base_prompt},\n",
    "                {\"role\": \"user\", \"content\": final_prompt},\n",
    "            ],\n",
    "            \"response_format\": {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"BestCategoryResponse\",\n",
    "                    \"schema\": schema,\n",
    "                    \"strict\": True,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "# 3) Create the .jsonl file with one request per product\n",
    "with open(input_jsonl_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for i, product in enumerate(products):\n",
    "        req_obj = build_request(i, product)\n",
    "        out_file.write(json.dumps(req_obj) + \"\\n\")\n",
    "\n",
    "print(f\"Created {input_jsonl_file} with {len(products)} lines.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 4) Upload the input file to OpenAI for batch processing\n",
    "batch_input_file = client.files.create(\n",
    "    file=open(input_jsonl_file, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "print(\"Uploaded file:\", batch_input_file)\n",
    "\n",
    "# 5) Create the batch job (with a 24-hour completion window)\n",
    "batch = client.batches.create(\n",
    "    input_file_id=batch_input_file.id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\"description\": \"EPDNorge product category classification (o3-mini-high)\"}\n",
    ")\n",
    "print(\"Created Batch:\", batch)\n",
    "\n",
    "# 6) Save the batch id for later status checks\n",
    "batch_id = batch.id\n",
    "print(f\"Batch submitted with id: {batch_id}.\")\n",
    "print(\"You can check the status later with:\")\n",
    "print(f\"client.batches.retrieve('{batch_id}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual batch id\n",
    "batch_id = \"batch_6816185b6cc08190a28a2829a6c1f780\"\n",
    "batch_status = client.batches.retrieve(batch_id)\n",
    "print(\"Current batch status:\", batch_status.status)\n",
    "\n",
    "if batch_status.status == \"completed\":\n",
    "    output_file_id = batch_status.output_file_id\n",
    "    if output_file_id:\n",
    "        output_jsonl_file = f\"../../data/pipeline2/json/openai/batch_output_{batch_id}.jsonl\"\n",
    "        file_response = client.files.content(output_file_id)\n",
    "        with open(output_jsonl_file, \"wb\") as f:\n",
    "            f.write(file_response.content)\n",
    "        print(f\"Batch results saved to {output_jsonl_file}\")\n",
    "    else:\n",
    "        print(\"No output file available yet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel the batch if needed\n",
    "client.batches.cancel(\"batch_xxxxxxxxxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# Print batch jobs and their status\n",
    "for batch in list(client.batches.list()):\n",
    "    pprint.pprint(batch.model_dump())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output file\n",
    "output_file_id = \"file-KtupsEnFunSJxoUQcycSQ3\"\n",
    "output_file = client.files.content(output_file_id)\n",
    "print(output_file.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check error file\n",
    "error_file_id = \"file-X2X7sMjyw2tpLoomziXjwz\"\n",
    "error_file = client.files.content(error_file_id)\n",
    "print(error_file.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
