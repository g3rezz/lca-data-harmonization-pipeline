{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize `technologyDescriptionAndIncludedProcesses`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama  # Use your Ollama integration\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a JSON schema for the summarization response.\n",
    "json_schema = {\n",
    "    \"title\": \"SummarizationResponse\",\n",
    "    \"description\": \"Response containing a summarized version of the technology description.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A concise summary of the technology description emphasizing key raw materials and its use in construction.\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"summary\"]\n",
    "}\n",
    "\n",
    "# Create a prompt template that only uses the Technology Description.\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert in summarizing technical manufacturing processes for building material products.\n",
    "Summarize the following technology description into a concise paragraph that emphasizes:\n",
    "- The key raw materials (e.g., PMMA, titanium dioxide, aluminum hydroxide, etc.)\n",
    "Avoid excessive process details; focus on the material composition and intended use.\n",
    "\n",
    "Technology Description:\n",
    "{text}\n",
    "\n",
    "Return your answer in JSON format following this schema:\n",
    "{json_schema}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Set your Ollama model name (adjust as needed for your environment)\n",
    "llm_model = \"deepseek-r1:8b\"  # e.g., \"deepseek-r1:8b\"\n",
    "model = ChatOllama(model=llm_model)\n",
    "\n",
    "def summarize_technology_description(text):\n",
    "    \"\"\"Uses Ollama to summarize a long Technology Description.\"\"\"\n",
    "    final_prompt = prompt_template.format_prompt(\n",
    "        text=text, json_schema=json.dumps(json_schema)\n",
    "    ).to_string()\n",
    "    \n",
    "    structured_llm = model.with_structured_output(\n",
    "        json_schema, method=\"json_schema\", include_raw=True\n",
    "    )\n",
    "    raw_response = structured_llm.invoke(final_prompt)\n",
    "    structured_response = raw_response.get(\"parsed\", None)\n",
    "    if structured_response and \"summary\" in structured_response:\n",
    "        return structured_response[\"summary\"]\n",
    "    return None\n",
    "\n",
    "# Path to the CSV file.\n",
    "csv_path = \"../../data/pipeline2/sql/filtered_epd_data.csv\"\n",
    "\n",
    "number_entries = 100\n",
    "\n",
    "# Read the CSV file and select only the first 100 entries.\n",
    "df = pd.read_csv(csv_path).head(number_entries)\n",
    "\n",
    "# Container to hold all responses.\n",
    "summaries = []\n",
    "\n",
    "# Process each row.\n",
    "for idx, row in df.iterrows():\n",
    "    tech_description = row.get(\"Technology Description\", \"\")\n",
    "    if tech_description and isinstance(tech_description, str):\n",
    "        summary = summarize_technology_description(tech_description)\n",
    "        # Create a dictionary for this row, including a row index for reference.\n",
    "        summaries.append({\n",
    "            \"row_index\": idx,\n",
    "            \"Product Name\": row.get(\"Product Name\", \"\"),\n",
    "            \"Classification\": row.get(\"Classification\", \"\"),\n",
    "            \"Technology Description Summary\": summary\n",
    "        })\n",
    "        print(f\"Summary for row {idx}:\\n{summary}\\n{'-'*50}\\n\")\n",
    "    else:\n",
    "        print(f\"Row {idx} has no Technology Description.\")\n",
    "\n",
    "# Ensure the output directory exists.\n",
    "output_dir = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f\"{number_entries}_technology_summaries.json\")\n",
    "\n",
    "# Write the JSON responses to file.\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summaries, f, indent=2)\n",
    "\n",
    "print(f\"All summaries saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama  # Use your Ollama integration\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a JSON schema for the summarization response.\n",
    "json_schema = {\n",
    "    \"title\": \"SummarizationResponse\",\n",
    "    \"description\": \"Response containing a summarized version of the technology description.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A concise summary of the technology description emphasizing key raw materials and its use in construction.\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"summary\"]\n",
    "}\n",
    "\n",
    "# Create a prompt template that only uses the Technology Description.\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert in summarizing technical documents of building material products to facilitate categorization.\n",
    "Summarize the following technology description into a concise paragraph emphasizing key raw materials and its use in construction.\n",
    "If no materials are mentioned, avoid excessive process details.\n",
    "\n",
    "Technology Description:\n",
    "{text}\n",
    "\n",
    "Return your answer in JSON format following this schema:\n",
    "{json_schema}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Set your Ollama model name (adjust as needed for your environment)\n",
    "llm_model = \"deepseek-r1:8b\"  # e.g., \"deepseek-r1:8b\"\n",
    "model = ChatOllama(model=llm_model)\n",
    "\n",
    "def summarize_technology_description(text):\n",
    "    \"\"\"Uses Ollama to summarize a long Technology Description.\"\"\"\n",
    "    final_prompt = prompt_template.format_prompt(\n",
    "        text=text, json_schema=json.dumps(json_schema)\n",
    "    ).to_string()\n",
    "    \n",
    "    structured_llm = model.with_structured_output(\n",
    "        json_schema, method=\"json_schema\", include_raw=True\n",
    "    )\n",
    "    raw_response = structured_llm.invoke(final_prompt)\n",
    "    structured_response = raw_response.get(\"parsed\", None)\n",
    "    if structured_response and \"summary\" in structured_response:\n",
    "        return structured_response[\"summary\"]\n",
    "    return None\n",
    "\n",
    "# Path to the CSV file.\n",
    "csv_path = \"../../data/pipeline2/sql/filtered_epd_data.csv\"\n",
    "\n",
    "number_entries = 100\n",
    "\n",
    "# Read the CSV file and select only the first 100 entries.\n",
    "df = pd.read_csv(csv_path).head(number_entries)\n",
    "\n",
    "# Container to hold all responses.\n",
    "summaries = []\n",
    "\n",
    "# Process each row.\n",
    "for idx, row in df.iterrows():\n",
    "    tech_description = row.get(\"Technology Description\", \"\")\n",
    "    if tech_description and isinstance(tech_description, str):\n",
    "        summary = summarize_technology_description(tech_description)\n",
    "        # Create a dictionary for this row, including a row index for reference.\n",
    "        summaries.append({\n",
    "            \"row_index\": idx,\n",
    "            \"Product Name\": row.get(\"Product Name\", \"\"),\n",
    "            \"Classification\": row.get(\"Classification\", \"\"),\n",
    "            \"Technology Description Summary\": summary\n",
    "        })\n",
    "        print(f\"Summary for row {idx}:\\n{summary}\\n{'-'*50}\\n\")\n",
    "    else:\n",
    "        print(f\"Row {idx} has no Technology Description.\")\n",
    "\n",
    "# Ensure the output directory exists.\n",
    "output_dir = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f\"{number_entries}_tech_sum_one_paragraph.json\")\n",
    "\n",
    "# Write the JSON responses to file.\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summaries, f, indent=2)\n",
    "\n",
    "print(f\"All summaries saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Composition Materials from `technologyDescriptionAndIncludedProcesses`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama  # Use your Ollama integration\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a JSON schema for the composition extraction response.\n",
    "json_schema = {\n",
    "    \"title\": \"CompositionExtractionResponse\",\n",
    "    \"description\": \"Response containing a list of key composition materials extracted from the technology description.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"composition\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A list of key raw materials mentioned in the technology description, e.g., PMMA, titanium dioxide, aluminum hydroxide.\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"composition\"]\n",
    "}\n",
    "\n",
    "# Create a prompt template that focuses on extracting composition materials from the Technology Description.\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert in technical manufacturing processes for building material products.\n",
    "Extract and list the key composition materials from the following technology description. \n",
    "Focus exclusively on the raw materials (e.g., PMMA, titanium dioxide, aluminum hydroxide, etc.) used in the product.\n",
    "Avoid including unnecessary process details; only list the materials.\n",
    "\n",
    "Technology Description:\n",
    "{text}\n",
    "\n",
    "Return your answer in JSON format following this schema:\n",
    "{json_schema}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Set your Ollama model name (adjust as needed for your environment)\n",
    "llm_model = \"deepseek-r1:8b\"  # for example, \"deepseek-r1:8b\"\n",
    "model = ChatOllama(model=llm_model)\n",
    "\n",
    "def extract_composition_materials(text):\n",
    "    \"\"\"Uses Ollama to extract the composition materials from a Technology Description.\"\"\"\n",
    "    final_prompt = prompt_template.format_prompt(\n",
    "        text=text, json_schema=json.dumps(json_schema)\n",
    "    ).to_string()\n",
    "    \n",
    "    structured_llm = model.with_structured_output(\n",
    "        json_schema, method=\"json_schema\", include_raw=True\n",
    "    )\n",
    "    raw_response = structured_llm.invoke(final_prompt)\n",
    "    structured_response = raw_response.get(\"parsed\", None)\n",
    "    if structured_response and \"composition\" in structured_response:\n",
    "        return structured_response[\"composition\"]\n",
    "    return None\n",
    "\n",
    "# Path to the CSV file.\n",
    "csv_path = \"../../data/pipeline2/sql/filtered_epd_data.csv\"\n",
    "\n",
    "number_entries = 100\n",
    "\n",
    "# Read the CSV file and select only the first 100 entries.\n",
    "df = pd.read_csv(csv_path).head(number_entries)\n",
    "\n",
    "# Container to hold all responses.\n",
    "composition_extractions = []\n",
    "\n",
    "# Process each row.\n",
    "for idx, row in df.iterrows():\n",
    "    tech_description = row.get(\"Technology Description\", \"\")\n",
    "    if tech_description and isinstance(tech_description, str):\n",
    "        composition = extract_composition_materials(tech_description)\n",
    "        # Create a dictionary for this row, including a row index for reference.\n",
    "        composition_extractions.append({\n",
    "            \"row_index\": idx,\n",
    "            \"Product Name\": row.get(\"Product Name\", \"\"),\n",
    "            \"Classification\": row.get(\"Classification\", \"\"),\n",
    "            \"Extracted Composition\": composition\n",
    "        })\n",
    "        print(f\"Extracted composition for row {idx}:\\n{composition}\\n{'-'*50}\\n\")\n",
    "    else:\n",
    "        print(f\"Row {idx} has no Technology Description.\\n{'-'*50}\\n\")\n",
    "\n",
    "# Ensure the output directory exists.\n",
    "output_dir = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f\"{number_entries}_technology_compositions.json\")\n",
    "\n",
    "# Write the JSON responses to file.\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(composition_extractions, f, indent=2)\n",
    "\n",
    "print(f\"All composition extractions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Both Summarization and Composition Material Extraction from `technologyDescriptionAndIncludedProcesses`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama  # Use your Ollama integration\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a JSON schema for the combined summarization and composition extraction response.\n",
    "json_schema = {\n",
    "    \"title\": \"SummarizationAndCompositionResponse\",\n",
    "    \"description\": \"Response containing a summarized version of the technology description and a list of key composition materials.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A concise summary of the technology description emphasizing key raw materials and its use in construction.\"\n",
    "        },\n",
    "        \"composition\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A list of key raw materials mentioned in the technology description, e.g., PMMA, titanium dioxide, aluminum hydroxide.\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"summary\", \"composition\"]\n",
    "}\n",
    "\n",
    "# Create a prompt template that instructs the model to provide both outputs.\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert in technical manufacturing processes for building material products.\n",
    "Given the following technology description, perform two tasks:\n",
    "1. Summarize the description into a concise paragraph that emphasizes the key raw materials.\n",
    "2. Extract and list the key composition materials mentioned (e.g., PMMA, titanium dioxide, aluminum hydroxide).\n",
    "\n",
    "Avoid excessive process details; focus on material composition and intended use.\n",
    "\n",
    "Technology Description:\n",
    "{text}\n",
    "\n",
    "Return your answer in JSON format following this schema:\n",
    "{json_schema}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Set your Ollama model name (this is dynamic so you can test different models)\n",
    "llm_model = \"deepseek-r1:8b\"  # For example, \"deepseek-r1:8b\", \"falcon3:7b-instruct-q4_K_M\",\n",
    "model = ChatOllama(model=llm_model)\n",
    "\n",
    "def process_technology_description(text):\n",
    "    \"\"\"Uses Ollama to get both a summary and the extracted composition materials from a Technology Description.\"\"\"\n",
    "    final_prompt = prompt_template.format_prompt(\n",
    "        text=text, json_schema=json.dumps(json_schema)\n",
    "    ).to_string()\n",
    "\n",
    "    structured_llm = model.with_structured_output(\n",
    "        json_schema, method=\"json_schema\", include_raw=True\n",
    "    )\n",
    "    raw_response = structured_llm.invoke(final_prompt)\n",
    "    structured_response = raw_response.get(\"parsed\", None)\n",
    "    if structured_response and \"summary\" in structured_response and \"composition\" in structured_response:\n",
    "        return structured_response\n",
    "    return {\"summary\": None, \"composition\": None}\n",
    "\n",
    "# Path to the CSV file.\n",
    "csv_path = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"sql\", \"filtered_epd_data.csv\")\n",
    "\n",
    "number_entries = 100\n",
    "\n",
    "# Read the CSV file and select only the first 100 entries.\n",
    "df = pd.read_csv(csv_path).head(number_entries)\n",
    "\n",
    "# Container to hold all responses.\n",
    "results = []\n",
    "\n",
    "# Process each row.\n",
    "for idx, row in df.iterrows():\n",
    "    tech_description = row.get(\"Technology Description\", \"\")\n",
    "    if tech_description and isinstance(tech_description, str):\n",
    "        response = process_technology_description(tech_description)\n",
    "        results.append({\n",
    "            \"row_index\": idx,\n",
    "            \"Product Name\": row.get(\"Product Name\", \"\"),\n",
    "            \"Classification\": row.get(\"Classification\", \"\"),\n",
    "            \"Technology Description Summary\": response.get(\"summary\"),\n",
    "            \"Extracted Composition\": response.get(\"composition\")\n",
    "        })\n",
    "        print(f\"Summary and material composition for row {idx}:\\n{response.get(\"summary\")}\\n{response.get(\"composition\")}\\n{'-'*50}\\n\")\n",
    "    else:\n",
    "        print(f\"Row {idx} has no Technology Description.\")\n",
    "\n",
    "# Ensure the output directory exists.\n",
    "output_dir = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Include the model name in the output file name.\n",
    "model_name = llm_model.split(\":\")[0]\n",
    "output_file = os.path.join(output_dir, f\"{number_entries}_technology_summary_composition_{model_name}.json\")\n",
    "\n",
    "# Write the JSON responses to file.\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"All responses saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize starting with Product Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama  # Use your Ollama integration\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a JSON schema for the summarization response (only a summary field).\n",
    "json_schema = {\n",
    "    \"title\": \"TechnologySummaryResponse\",\n",
    "    \"description\": \"Response containing a concise technology summary that begins with the product name.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A concise summary of the technology description that starts with the product name.\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"summary\"]\n",
    "}\n",
    "\n",
    "# Create a prompt template that uses both the Product Name and Technology Description.\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert in technical manufacturing processes for building material products.\n",
    "Using the information provided, produce a concise technology summary that begins with the product name.\n",
    "The summary should start with \"The product {product_name}\" and then describe the manufacturing process and key raw materials.\n",
    "\n",
    "Technology Description: {text}\n",
    "\n",
    "Return your answer in JSON format following this schema:\n",
    "{json_schema}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Set your Ollama model name (this can be changed dynamically)\n",
    "llm_model = \"deepseek-r1:8b\"  # e.g., \"deepseek-r1:8b\" or another model name\n",
    "model = ChatOllama(model=llm_model)\n",
    "\n",
    "def summarize_technology_description(product_name, text):\n",
    "    \"\"\"Uses Ollama to generate a summary for the technology description that starts with the product name.\"\"\"\n",
    "    final_prompt = prompt_template.format_prompt(\n",
    "        product_name=product_name,\n",
    "        text=text,\n",
    "        json_schema=json.dumps(json_schema)\n",
    "    ).to_string()\n",
    "    \n",
    "    structured_llm = model.with_structured_output(\n",
    "        json_schema, method=\"json_schema\", include_raw=True\n",
    "    )\n",
    "    raw_response = structured_llm.invoke(final_prompt)\n",
    "    structured_response = raw_response.get(\"parsed\", None)\n",
    "    if structured_response and \"summary\" in structured_response:\n",
    "        return structured_response[\"summary\"]\n",
    "    return None\n",
    "\n",
    "# Path to the CSV file.\n",
    "csv_path = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"sql\", \"filtered_epd_data.csv\")\n",
    "number_entries = 100\n",
    "\n",
    "# Read the CSV file and select only the first 100 entries.\n",
    "df = pd.read_csv(csv_path).head(number_entries)\n",
    "\n",
    "# Container to hold all responses.\n",
    "results = []\n",
    "\n",
    "# Process each row.\n",
    "for idx, row in df.iterrows():\n",
    "    product_name = row.get(\"Product Name\", \"\")\n",
    "    tech_description = row.get(\"Technology Description\", \"\")\n",
    "    if product_name and tech_description and isinstance(tech_description, str):\n",
    "        summary = summarize_technology_description(product_name, tech_description)\n",
    "        results.append({\n",
    "            \"row_index\": idx,\n",
    "            \"Product Name\": product_name,\n",
    "            \"Classification\": row.get(\"Classification\", \"\"),\n",
    "            \"Technology Description Summary\": summary\n",
    "        })\n",
    "        print(f\"Processed row {idx}\\n{product_name}\\n\\n{summary}\\n{'-'*50}\\n\")\n",
    "    else:\n",
    "        print(f\"Row {idx} missing product name or technology description.\")\n",
    "\n",
    "# Ensure the output directory exists.\n",
    "output_dir = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Include the model name in the output file name.\n",
    "model_name = llm_model.split(\":\")[0]\n",
    "output_file = os.path.join(output_dir, f\"{number_entries}_tech_sum_prodname_{model_name}.json\")\n",
    "\n",
    "# Write the JSON responses to file.\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"All summaries saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama  # Use your Ollama integration\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a JSON schema for the summarization response (only a summary field).\n",
    "json_schema = {\n",
    "    \"title\": \"TechnologySummaryResponse\",\n",
    "    \"description\": \"Response containing a concise technology summary that begins with the product name.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A concise summary of the technology description that starts with the product name.\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"summary\"]\n",
    "}\n",
    "\n",
    "# Create a prompt template that uses both the Product Name and Technology Description.\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert in technical manufacturing processes for building material products.\n",
    "Using the information provided, produce a one sentence summary that begins with the product name.\n",
    "The summary should start with \"The product {product_name}\" and then describe concisely the product.\n",
    "\n",
    "Technology Description: {text}\n",
    "\n",
    "Return your answer in JSON format following this schema:\n",
    "{json_schema}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Set your Ollama model name (this can be changed dynamically)\n",
    "llm_model = \"deepseek-r1:8b\"  # e.g., \"deepseek-r1:8b\" or another model name\n",
    "model = ChatOllama(model=llm_model)\n",
    "\n",
    "def summarize_technology_description(product_name, text):\n",
    "    \"\"\"Uses Ollama to generate a summary for the technology description that starts with the product name.\"\"\"\n",
    "    final_prompt = prompt_template.format_prompt(\n",
    "        product_name=product_name,\n",
    "        text=text,\n",
    "        json_schema=json.dumps(json_schema)\n",
    "    ).to_string()\n",
    "    \n",
    "    structured_llm = model.with_structured_output(\n",
    "        json_schema, method=\"json_schema\", include_raw=True\n",
    "    )\n",
    "    raw_response = structured_llm.invoke(final_prompt)\n",
    "    structured_response = raw_response.get(\"parsed\", None)\n",
    "    if structured_response and \"summary\" in structured_response:\n",
    "        return structured_response[\"summary\"]\n",
    "    return None\n",
    "\n",
    "# Path to the CSV file.\n",
    "csv_path = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"sql\", \"filtered_epd_data.csv\")\n",
    "number_entries = 100\n",
    "\n",
    "# Read the CSV file and select only the first 100 entries.\n",
    "df = pd.read_csv(csv_path).head(number_entries)\n",
    "\n",
    "# Container to hold all responses.\n",
    "results = []\n",
    "\n",
    "# Process each row.\n",
    "for idx, row in df.iterrows():\n",
    "    product_name = row.get(\"Product Name\", \"\")\n",
    "    tech_description = row.get(\"Technology Description\", \"\")\n",
    "    if product_name and tech_description and isinstance(tech_description, str):\n",
    "        summary = summarize_technology_description(product_name, tech_description)\n",
    "        results.append({\n",
    "            \"row_index\": idx,\n",
    "            \"Product Name\": product_name,\n",
    "            \"Classification\": row.get(\"Classification\", \"\"),\n",
    "            \"Technology Description Summary\": summary\n",
    "        })\n",
    "        print(f\"Processed row {idx}\\n{product_name}\\n\\n{summary}\\n{'-'*50}\\n\")\n",
    "    else:\n",
    "        print(f\"Row {idx} missing product name or technology description.\")\n",
    "\n",
    "# Ensure the output directory exists.\n",
    "output_dir = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Include the model name in the output file name.\n",
    "model_name = llm_model.split(\":\")[0]\n",
    "output_file = os.path.join(output_dir, f\"{number_entries}_tech_sum_prodname_{model_name}.json\")\n",
    "\n",
    "# Write the JSON responses to file.\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"All summaries saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Technology Descriptions in one Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama  # Use your Ollama integration\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a JSON schema for the summarization response.\n",
    "json_schema = {\n",
    "    \"title\": \"SummarizationResponse\",\n",
    "    \"description\": \"Response containing a summarized version of the technology description.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A concise summary of the technology description emphasizing key raw materials and its use in construction.\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"summary\"]\n",
    "}\n",
    "\n",
    "# Create a prompt template that only uses the Technology Description.\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert in summarizing technical documents of building material products.\n",
    "Summarize the following technology description into a concise sentence that facilitates the categorization of the building product.\n",
    "\n",
    "Technology Description:\n",
    "{text}\n",
    "\n",
    "Return your answer in JSON format following this schema:\n",
    "{json_schema}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Set your Ollama model name (adjust as needed for your environment)\n",
    "llm_model = \"deepseek-r1:8b\"  # e.g., \"deepseek-r1:8b\"\n",
    "model = ChatOllama(model=llm_model)\n",
    "\n",
    "def summarize_technology_description(text):\n",
    "    \"\"\"Uses Ollama to summarize a long Technology Description.\"\"\"\n",
    "    final_prompt = prompt_template.format_prompt(\n",
    "        text=text, json_schema=json.dumps(json_schema)\n",
    "    ).to_string()\n",
    "    \n",
    "    structured_llm = model.with_structured_output(\n",
    "        json_schema, method=\"json_schema\", include_raw=True\n",
    "    )\n",
    "    raw_response = structured_llm.invoke(final_prompt)\n",
    "    structured_response = raw_response.get(\"parsed\", None)\n",
    "    if structured_response and \"summary\" in structured_response:\n",
    "        return structured_response[\"summary\"]\n",
    "    return None\n",
    "\n",
    "# Path to the CSV file.\n",
    "csv_path = \"../../data/pipeline2/sql/filtered_epd_data.csv\"\n",
    "\n",
    "number_entries = 100\n",
    "\n",
    "# Read the CSV file and select only the first 100 entries.\n",
    "df = pd.read_csv(csv_path).head(number_entries)\n",
    "\n",
    "# Container to hold all responses.\n",
    "summaries = []\n",
    "\n",
    "# Process each row.\n",
    "for idx, row in df.iterrows():\n",
    "    tech_description = row.get(\"Technology Description\", \"\")\n",
    "    if tech_description and isinstance(tech_description, str):\n",
    "        summary = summarize_technology_description(tech_description)\n",
    "        # Create a dictionary for this row, including a row index for reference.\n",
    "        summaries.append({\n",
    "            \"row_index\": idx,\n",
    "            \"Product Name\": row.get(\"Product Name\", \"\"),\n",
    "            \"Classification\": row.get(\"Classification\", \"\"),\n",
    "            \"Technology Description Summary\": summary\n",
    "        })\n",
    "        print(f\"Summary for row {idx}:\\n{summary}\\n{'-'*50}\\n\")\n",
    "    else:\n",
    "        print(f\"Row {idx} has no Technology Description.\")\n",
    "\n",
    "# Ensure the output directory exists.\n",
    "output_dir = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f\"{number_entries}_tech_sum_one_sentence.json\")\n",
    "\n",
    "# Write the JSON responses to file.\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summaries, f, indent=2)\n",
    "\n",
    "print(f\"All summaries saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama  # Use your Ollama integration\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a JSON schema for the summarization response.\n",
    "json_schema = {\n",
    "    \"title\": \"SummarizationResponse\",\n",
    "    \"description\": \"Response containing a summarized version of the technology description.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A concise summary of the technology description emphasizing key raw materials and its use in construction.\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"summary\"]\n",
    "}\n",
    "\n",
    "# Create a prompt template that only uses the Technology Description.\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert in summarizing technical documents of building material products to facilitate categorization.\n",
    "Summarize the following technology description into a concise sentence emphasizing key raw materials and its use in construction.\n",
    "\n",
    "Technology Description:\n",
    "{text}\n",
    "\n",
    "Return your answer in JSON format following this schema:\n",
    "{json_schema}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Set your Ollama model name (adjust as needed for your environment)\n",
    "llm_model = \"deepseek-r1:8b\"  # e.g., \"deepseek-r1:8b\"\n",
    "model = ChatOllama(model=llm_model)\n",
    "\n",
    "def summarize_technology_description(text):\n",
    "    \"\"\"Uses Ollama to summarize a long Technology Description.\"\"\"\n",
    "    final_prompt = prompt_template.format_prompt(\n",
    "        text=text, json_schema=json.dumps(json_schema)\n",
    "    ).to_string()\n",
    "    \n",
    "    structured_llm = model.with_structured_output(\n",
    "        json_schema, method=\"json_schema\", include_raw=True\n",
    "    )\n",
    "    raw_response = structured_llm.invoke(final_prompt)\n",
    "    structured_response = raw_response.get(\"parsed\", None)\n",
    "    if structured_response and \"summary\" in structured_response:\n",
    "        return structured_response[\"summary\"]\n",
    "    return None\n",
    "\n",
    "# Path to the CSV file.\n",
    "csv_path = \"../../data/pipeline2/sql/filtered_epd_data.csv\"\n",
    "\n",
    "number_entries = 100\n",
    "\n",
    "# Read the CSV file and select only the first 100 entries.\n",
    "df = pd.read_csv(csv_path).head(number_entries)\n",
    "\n",
    "# Container to hold all responses.\n",
    "summaries = []\n",
    "\n",
    "# Process each row.\n",
    "for idx, row in df.iterrows():\n",
    "    tech_description = row.get(\"Technology Description\", \"\")\n",
    "    if tech_description and isinstance(tech_description, str):\n",
    "        summary = summarize_technology_description(tech_description)\n",
    "        # Create a dictionary for this row, including a row index for reference.\n",
    "        summaries.append({\n",
    "            \"row_index\": idx,\n",
    "            \"Product Name\": row.get(\"Product Name\", \"\"),\n",
    "            \"Classification\": row.get(\"Classification\", \"\"),\n",
    "            \"Technology Description Summary\": summary\n",
    "        })\n",
    "        print(f\"Summary for row {idx}:\\n{summary}\\n{'-'*50}\\n\")\n",
    "    else:\n",
    "        print(f\"Row {idx} has no Technology Description.\")\n",
    "\n",
    "# Ensure the output directory exists.\n",
    "output_dir = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f\"{number_entries}_tech_sum_one_sentence02.json\")\n",
    "\n",
    "# Write the JSON responses to file.\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summaries, f, indent=2)\n",
    "\n",
    "print(f\"All summaries saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Character and Token Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import statistics\n",
    "import os\n",
    "\n",
    "# File path for the JSON summaries\n",
    "json_file_path = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"json\", \"100_technology_summaries.json\")\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "else:\n",
    "    # Load JSON data\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        summaries = json.load(f)\n",
    "    \n",
    "    # Convert JSON list to a DataFrame\n",
    "    df = pd.DataFrame(summaries)\n",
    "    \n",
    "    # We assume that the summarized text is under the key \"Technology Description Summary\"\n",
    "    # If your key is different (e.g., \"summary\"), adjust accordingly.\n",
    "    if \"Technology Description Summary\" not in df.columns:\n",
    "        print(\"Key 'Technology Description Summary' not found in JSON data.\")\n",
    "    else:\n",
    "        # Compute the character count of each summary.\n",
    "        df[\"char_count\"] = df[\"Technology Description Summary\"].astype(str).apply(len)\n",
    "        \n",
    "        # Initialize the tokenizer.\n",
    "        enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "        df[\"token_count\"] = df[\"Technology Description Summary\"].astype(str).apply(lambda text: len(enc.encode(text)))\n",
    "        \n",
    "        # Collect counts into lists.\n",
    "        lengths = df[\"char_count\"].tolist()\n",
    "        token_counts = df[\"token_count\"].tolist()\n",
    "        \n",
    "        # Calculate basic statistics.\n",
    "        mean_length = statistics.mean(lengths) if lengths else 0\n",
    "        mean_tokens = statistics.mean(token_counts) if token_counts else 0\n",
    "        median_length = np.median(lengths) if lengths else 0\n",
    "        median_tokens = np.median(token_counts) if token_counts else 0\n",
    "        std_length = np.std(lengths, ddof=1) if len(lengths) > 1 else 0\n",
    "        std_tokens = np.std(token_counts, ddof=1) if len(token_counts) > 1 else 0\n",
    "        percentiles_length = np.percentile(lengths, [25, 50, 75, 90, 95]) if lengths else [0] * 5\n",
    "        percentiles_tokens = np.percentile(token_counts, [25, 50, 75, 90, 95]) if token_counts else [0] * 5\n",
    "        iqr_length = percentiles_length[2] - percentiles_length[0]\n",
    "        iqr_tokens = percentiles_tokens[2] - percentiles_tokens[0]\n",
    "        \n",
    "        # Print statistical measures.\n",
    "        print(f\"Mean character count: {mean_length:.2f}\")\n",
    "        print(f\"Median character count: {median_length:.2f}\")\n",
    "        print(f\"Standard deviation (chars): {std_length:.2f}\")\n",
    "        print(f\"25th, 50th, 75th, 90th, 95th percentiles (chars): {percentiles_length}\")\n",
    "        print(f\"IQR (chars): {iqr_length:.2f}\\n\")\n",
    "        \n",
    "        print(f\"Mean token count: {mean_tokens:.2f}\")\n",
    "        print(f\"Median token count: {median_tokens:.2f}\")\n",
    "        print(f\"Standard deviation (tokens): {std_tokens:.2f}\")\n",
    "        print(f\"25th, 50th, 75th, 90th, 95th percentiles (tokens): {percentiles_tokens}\")\n",
    "        print(f\"IQR (tokens): {iqr_tokens:.2f}\\n\")\n",
    "        \n",
    "        # Plot histogram for character counts.\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(lengths, bins=20, edgecolor=\"black\")\n",
    "        plt.title(\"Character Count Distribution of Summaries\")\n",
    "        plt.xlabel(\"Character Count\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot histogram for token counts.\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(token_counts, bins=20, edgecolor=\"black\")\n",
    "        plt.title(\"Token Count Distribution of Summaries\")\n",
    "        plt.xlabel(\"Token Count\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Function to print histogram data numerically.\n",
    "        def print_histogram_data(data, bins=20, title=\"Histogram\"):\n",
    "            counts, bin_edges = np.histogram(data, bins=bins)\n",
    "            print(title)\n",
    "            for i in range(len(counts)):\n",
    "                lower = bin_edges[i]\n",
    "                upper = bin_edges[i + 1]\n",
    "                print(f\"{lower:5.1f} - {upper:5.1f}: {counts[i]}\")\n",
    "        \n",
    "        print_histogram_data(lengths, bins=20, title=\"Character Count Histogram Data\")\n",
    "        print_histogram_data(token_counts, bins=20, title=\"Token Count Histogram Data\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
