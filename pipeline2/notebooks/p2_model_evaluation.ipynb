{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "import gc\n",
    "import re\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "##################################################\n",
    "# Custom SentenceTransformer Embeddings\n",
    "##################################################\n",
    "class CustomSentenceTransformerEmbeddings(Embeddings):\n",
    "    \"\"\"\n",
    "    Allows using a SentenceTransformer model within a LangChain-based FAISS store.\n",
    "    Handles initialization of different models with specific arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model_name: str):\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.model = self._initialize_model()\n",
    "\n",
    "    def _initialize_model(self) -> SentenceTransformer:\n",
    "        \"\"\"\n",
    "        Initializes the SentenceTransformer model based on the embedding_model_name.\n",
    "        \"\"\"\n",
    "        # Define initialization configurations for each model\n",
    "        model_configs = {\n",
    "            \"jinaai/jina-embeddings-v3\": {\n",
    "                \"trust_remote_code\": True,\n",
    "                \"revision\": \"main\",\n",
    "                \"device\": \"cuda\",\n",
    "                \"model_kwargs\": {\"use_flash_attn\": False},\n",
    "            },\n",
    "            \"HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5\": {\n",
    "                \"local_files_only\": True,\n",
    "                \"device\": \"cuda\",\n",
    "                \"model_kwargs\": {\"attn_implementation\": \"eager\"},\n",
    "            },\n",
    "            \"Alibaba-NLP/gte-large-en-v1.5\": {\n",
    "                \"trust_remote_code\": True,\n",
    "                \"revision\": \"main\",\n",
    "                \"device\": \"cuda\",\n",
    "                \"model_kwargs\": {\"attn_implementation\": \"eager\"},\n",
    "            },\n",
    "        }\n",
    "\n",
    "        config = model_configs.get(\n",
    "            self.embedding_model_name,\n",
    "            {\"device\": \"cuda\", \"model_kwargs\": {}},  # default fallback\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            model = SentenceTransformer(self.embedding_model_name, **config)\n",
    "            print(f\"Initialized SentenceTransformer model: {self.embedding_model_name}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing model {self.embedding_model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text).tolist()\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(texts).tolist()\n",
    "\n",
    "    def unload_model(self):\n",
    "        \"\"\"\n",
    "        Remove the model from memory after processing to free up GPU resources.\n",
    "        \"\"\"\n",
    "        if self.model:\n",
    "            del self.model\n",
    "            self.model = None\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            print(f\"Unloaded embedding model: {self.embedding_model_name}\")\n",
    "        else:\n",
    "            print(\"[DEBUG] Embedding model was already None or not set.\")\n",
    "\n",
    "\n",
    "################################\n",
    "# Evaluation Helper Functions\n",
    "################################\n",
    "def strip_content_between_dashes(chunk):\n",
    "    \"\"\"\n",
    "    Removes content between `---` markers, inclusive, if it exists in the chunk.\n",
    "    \"\"\"\n",
    "    if \"---\" in chunk:\n",
    "        return re.sub(r\"---.*?---\", \"\", chunk, flags=re.DOTALL).strip()\n",
    "    return chunk\n",
    "\n",
    "\n",
    "def evaluate_retrieval(retrieved_chunks, expected_chunks, comparison_length=100):\n",
    "    \"\"\"\n",
    "    Strips content between `---` markers in the retrieved chunks, then compares\n",
    "    them with the expected chunks based on the first `comparison_length` characters.\n",
    "    \"\"\"\n",
    "    processed_retrieved_chunks = [\n",
    "        strip_content_between_dashes(chunk) for chunk in retrieved_chunks\n",
    "    ]\n",
    "\n",
    "    expected_prefixes = set([c[:comparison_length].strip() for c in expected_chunks])\n",
    "    retrieved_prefixes = set([c[:comparison_length].strip() for c in processed_retrieved_chunks])\n",
    "\n",
    "    correctly_retrieved = expected_prefixes.intersection(retrieved_prefixes)\n",
    "    missed_chunks = expected_prefixes.difference(retrieved_prefixes)\n",
    "\n",
    "    print(f\"\\nExact-Match Evaluation (First {comparison_length} characters):\")\n",
    "    print(f\"  Correctly retrieved chunks: {len(correctly_retrieved)}/{len(expected_chunks)}\")\n",
    "    for prefix in correctly_retrieved:\n",
    "        print(f\"    ✔ Retrieved Prefix: {prefix}\")\n",
    "\n",
    "    print(f\"  Missed chunks: {len(missed_chunks)}/{len(expected_chunks)}\")\n",
    "    for prefix in missed_chunks:\n",
    "        print(f\"    ✘ Missed Prefix: {prefix}\")\n",
    "\n",
    "    return correctly_retrieved, missed_chunks\n",
    "\n",
    "\n",
    "def evaluate_ranked_retrieval(docs_with_ranks, expected_chunks, comparison_length=100):\n",
    "    \"\"\"\n",
    "    Rank-based check: for each expected chunk, see at which rank its prefix appears\n",
    "    in the retrieved list, then compute Mean Rank and MRR.\n",
    "    \"\"\"\n",
    "    processed_docs_with_ranks = [\n",
    "        (rank, strip_content_between_dashes(doc.page_content))\n",
    "        for rank, doc in docs_with_ranks\n",
    "    ]\n",
    "    processed_expected_chunks = [\n",
    "        strip_content_between_dashes(chunk) for chunk in expected_chunks\n",
    "    ]\n",
    "\n",
    "    ranks = []\n",
    "    for chunk in processed_expected_chunks:\n",
    "        chunk_prefix = chunk[:comparison_length].strip()\n",
    "        matched_positions = [\n",
    "            rank\n",
    "            for (rank, doc) in processed_docs_with_ranks\n",
    "            if doc[:comparison_length].strip() == chunk_prefix\n",
    "        ]\n",
    "        if matched_positions:\n",
    "            ranks.append(matched_positions[0])\n",
    "        else:\n",
    "            ranks.append(None)\n",
    "\n",
    "    found_ranks = [r for r in ranks if r is not None]\n",
    "    missed_count = sum(r is None for r in ranks)\n",
    "\n",
    "    if found_ranks:\n",
    "        if len(found_ranks) == 1:\n",
    "            mean_rank = found_ranks[0]\n",
    "            rank = int(mean_rank)\n",
    "        else:\n",
    "            mean_rank = sum(found_ranks) / len(found_ranks)\n",
    "            rank = None\n",
    "        mrr = sum((1.0 / r) for r in found_ranks) / len(found_ranks)\n",
    "    else:\n",
    "        mean_rank = None\n",
    "        rank = None\n",
    "        mrr = None\n",
    "\n",
    "    print(f\"\\nRanked Evaluation (First {comparison_length} characters):\")\n",
    "    print(f\"  Found: {len(found_ranks)}/{len(expected_chunks)}\")\n",
    "    print(f\"  Missed: {missed_count}/{len(expected_chunks)}\")\n",
    "    if mean_rank is not None:\n",
    "        print(f\"  Mean Rank: {mean_rank:.2f}\")\n",
    "    else:\n",
    "        print(f\"  Mean Rank: N/A\")\n",
    "    if rank is not None:\n",
    "        print(f\"  Rank: {rank}\")\n",
    "    else:\n",
    "        print(f\"  Rank: N/A\")\n",
    "    if mrr is not None:\n",
    "        print(f\"  MRR: {mrr:.3f}\")\n",
    "    else:\n",
    "        print(f\"  MRR: N/A\")\n",
    "\n",
    "    return mean_rank, rank, mrr\n",
    "\n",
    "\n",
    "def save_csv(evaluation_data, k_value, lambda_mult, output_csv_path):\n",
    "    df_eval = pd.DataFrame(evaluation_data)\n",
    "    df_eval[\"k_value\"] = k_value\n",
    "    df_eval[\"lambda_mult\"] = lambda_mult\n",
    "\n",
    "    columns_order = [\n",
    "        \"model\",\n",
    "        \"row_index\",\n",
    "        \"product_name\",\n",
    "        \"k_value\",\n",
    "        \"lambda_mult\",\n",
    "        \"retrieved_docs\",\n",
    "        \"found_exact\",\n",
    "        \"missed_exact\",\n",
    "        \"mean_rank\",\n",
    "        \"rank\",\n",
    "        \"mrr\",\n",
    "    ]\n",
    "    df_eval = df_eval.reindex(columns=columns_order, fill_value=\"N/A\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
    "    if not os.path.exists(output_csv_path):\n",
    "        df_eval.to_csv(output_csv_path, index=False, mode=\"w\", encoding=\"utf-8\")\n",
    "    else:\n",
    "        df_eval.to_csv(output_csv_path, index=False, mode=\"a\", header=False, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "############################################################\n",
    "# compare_rows: For each row in the CSV (first 100),\n",
    "# build a query using Product Name + Classification +\n",
    "# Technology Description. Then retrieve & evaluate.\n",
    "############################################################\n",
    "def compare_rows(\n",
    "    df: pd.DataFrame,\n",
    "    bench_dict: Dict[str, str],\n",
    "    embedding_model_name: str,\n",
    "    schema: str,\n",
    "    k_value: int,\n",
    "    output_csv_path: str,\n",
    "    comparison_length: int = 100,\n",
    "    fetch_k: int = 100,\n",
    "    lambda_mult: float = 0.8,\n",
    "):\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame, build a query that includes:\n",
    "      - Product Name\n",
    "      - Classification\n",
    "      - Technology Description (summarized)\n",
    "    Then retrieve documents using the vectorstore retriever and\n",
    "    evaluate them against the expected chunk from bench_dict,\n",
    "    keyed by the Product Name.\n",
    "    \"\"\"\n",
    "    evaluation_data = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # Use for JSON\n",
    "        product_name = row[\"Product Name\"]\n",
    "        # classification = row[\"Classification Suggestion\"]\n",
    "        classification = row[\"Classification Result\"]\n",
    "        # tech_description = row[\"Technology Description Summary\"]\n",
    "        tech_description = row[\"Technology Description\"]\n",
    "        # comp_materials = row[\"Extracted Composition\"]\n",
    "\n",
    "        # Use for CSV\n",
    "        product_name = row[\"Product Name\"]\n",
    "        classification = row[\"Classification Result\"]\n",
    "        tech_description = row[\"Technology Description\"]\n",
    "\n",
    "        # Build the query from the entire row\n",
    "        query_str = (\n",
    "            # f\"Product Name: {product_name}\\n\"\n",
    "            # f\"Classification: {classification}\\n\"\n",
    "            # f\"Technology Description: {tech_description}\\n\"\n",
    "            # f\"Composition Materials: {comp_materials}\"\n",
    "            f\"Product: {product_name}\\n\"\n",
    "            f\"Description: {tech_description}\\n\"\n",
    "            # f\"{tech_description}\\n\"\n",
    "        )\n",
    "\n",
    "        if classification != \"Other\":\n",
    "            query_str += f\"Classification: {classification}\\n\"\n",
    "\n",
    "        # The expected chunk for ranking/evaluation:\n",
    "        # If the product name doesn't exist in bench_dict,\n",
    "        # we treat the expected chunk as empty or skip evaluation.\n",
    "        expected_chunk = bench_dict.get(product_name.lower(), \"\")\n",
    "\n",
    "        # If there's no known expected chunk, we can optionally skip or do partial eval\n",
    "        if not expected_chunk:\n",
    "            print(f\"[Warning] No expected benchmark info for product '{product_name}'\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[Model: {embedding_model_name}]\")\n",
    "        print(f\"[Row Index: {idx}]\")\n",
    "        print(f\"Query:\\n{query_str}\")  # Print query for inspection\n",
    "\n",
    "        # Retrieve using MMR-based approach\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\n",
    "                # \"filter\": {\"schema_type\": schema}, # There is no filter\n",
    "                \"k\": k_value,\n",
    "                \"fetch_k\": fetch_k,\n",
    "                \"lambda_mult\": lambda_mult,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        retrieved_docs = retriever.invoke(query_str)\n",
    "        print(f\"Number of retrieved documents: {len(retrieved_docs)}\")\n",
    "\n",
    "        retrieved_chunks = [doc.page_content for doc in retrieved_docs]\n",
    "\n",
    "        # Evaluate retrieval\n",
    "        found_exact, missed_chunks = evaluate_retrieval(\n",
    "            retrieved_chunks, [expected_chunk], comparison_length=comparison_length\n",
    "        )\n",
    "\n",
    "        # Evaluate ranking\n",
    "        docs_with_ranks = list(enumerate(retrieved_docs, start=1))\n",
    "        mean_rank, rank, mrr = evaluate_ranked_retrieval(\n",
    "            docs_with_ranks, [expected_chunk], comparison_length=comparison_length\n",
    "        )\n",
    "\n",
    "        # Show top 5 for inspection (optional)\n",
    "        top_docs = retrieved_chunks[:5]\n",
    "        processed_top_docs = [strip_content_between_dashes(doc) for doc in top_docs]\n",
    "        print(f\"\\n[Top 5 Retrieved Docs]\")\n",
    "        for i, doc_text in enumerate(processed_top_docs, start=1):\n",
    "            print(f\"  {i}. {doc_text}\")\n",
    "\n",
    "        # Collect the evaluation results\n",
    "        evaluation_data.append(\n",
    "            {\n",
    "                \"model\": embedding_model_name,\n",
    "                \"row_index\": idx,\n",
    "                \"product_name\": product_name,\n",
    "                \"retrieved_docs\": len(retrieved_docs),\n",
    "                \"found_exact\": len(found_exact),\n",
    "                \"missed_exact\": len(missed_chunks),\n",
    "                \"mean_rank\": mean_rank,\n",
    "                \"rank\": rank,\n",
    "                \"mrr\": mrr,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Save results for all rows\n",
    "    save_csv(evaluation_data, k_value, lambda_mult, output_csv_path)\n",
    "\n",
    "##################################################\n",
    "# Load the JSON summaries\n",
    "##################################################\n",
    "# summaries_path = \"../../data/pipeline2/json/100_tech_sum_one_paragraph_regex.json\"\n",
    "# summaries_path = \"../../data/pipeline2/json/100_technology_compositions.json\"\n",
    "# with open(summaries_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     tech_summaries = json.load(f)\n",
    "\n",
    "# Create a DataFrame from the JSON data\n",
    "# df = pd.DataFrame(tech_summaries)\n",
    "\n",
    "##################################################\n",
    "# Load the CSV raw data\n",
    "##################################################\n",
    "summaries_path = \"../../data/pipeline2/sql/filtered_epd_data02.csv\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(summaries_path)\n",
    "\n",
    "\n",
    "##################################################\n",
    "# Load the JSON benchmark data (100_matched_bench.json)\n",
    "# and map each \"epd_name\" to \"epd_category\" to use\n",
    "# as our \"expected chunk\" for that product.\n",
    "##################################################\n",
    "bench_path = \"../../data/pipeline2/json/200_matched_bench_EN.json\"\n",
    "with open(bench_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    bench_data = json.load(f)\n",
    "\n",
    "bench_dict = {\n",
    "    item[\"epd_name\"]: item[\"epd_category\"]\n",
    "    for item in bench_data\n",
    "}\n",
    "\n",
    "\n",
    "#########\n",
    "# Usage\n",
    "#########\n",
    "k_values = [10, 20, 30, 40, 50]\n",
    "# k_values = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80]\n",
    "# k_values = [40, 50, 60, 70, 80]\n",
    "# lambda_mults = [0.7, 0.8, 0.9, 1.0]\n",
    "lambda_mults = [1.0]\n",
    "# lambda_mults = [1.0]\n",
    "\n",
    "# Paths for the FAISS vectorstores (one for each model)\n",
    "embeddings_name = \"faiss_index_COS_EN\"\n",
    "embeddings_name_512 = \"faiss_index_COS_EN\"\n",
    "vectorstore_paths = {\n",
    "    # \"bge-m3:latest\": f\"../../embeddings/pipeline2/bge-m3/{embeddings_name}\",\n",
    "    # \"snowflake-arctic-embed2:latest\": f\"../../embeddings/pipeline2/snowflake-arctic-embed2/{embeddings_name}\",\n",
    "    # \"jina/jina-embeddings-v2-base-de:latest\": f\"../../embeddings/pipeline2/jina_jina-embeddings-v2-base-de/{embeddings_name}\",\n",
    "    # \"paraphrase-multilingual:latest\": f\"../../embeddings/pipeline2/paraphrase-multilingual/{embeddings_name}\",\n",
    "    # \"jeffh/intfloat-multilingual-e5-large-instruct:f32\": f\"../../embeddings/pipeline2/jeffh_intfloat-multilingual-e5-large-instruct/{embeddings_name_512}\",\n",
    "    # \"granite-embedding:278m\": f\"../../embeddings/pipeline2/granite-embedding/{embeddings_name_512}\",\n",
    "    # \"bge-large:latest\": f\"../../embeddings/pipeline2/bge-large/{embeddings_name_512}\",\n",
    "    \"mxbai-embed-large:latest\": f\"../../embeddings/pipeline2/mxbai-embed-large/{embeddings_name_512}\",\n",
    "    # \"jinaai/jina-embeddings-v3\": f\"../../embeddings/pipeline2/jinaai_jina-embeddings-v3/{embeddings_name}\",\n",
    "    # \"HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5\": f\"../../embeddings/pipeline2/HIT-TMG_KaLM-embedding-multilingual-mini-instruct-v1.5/{embeddings_name}\",\n",
    "    # \"Alibaba-NLP/gte-large-en-v1.5\": f\"../../embeddings/pipeline2/Alibaba-NLP_gte-large-en-v1.5/{embeddings_name_512}\",\n",
    "}\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_csv_path = f\"../../data/pipeline2/embed_eval/embed_eval_res_{timestamp}.csv\"\n",
    "\n",
    "# We'll evaluate row-by-row for each embedding model\n",
    "schema_type = \"epd_bench_schema\"  # a placeholder schema type for filtering\n",
    "for embedding_model_name, faiss_path in vectorstore_paths.items():\n",
    "    print(f\"\\n=== Processing Embedding Model: {embedding_model_name} ===\")\n",
    "\n",
    "    # Choose embeddings class\n",
    "    if embedding_model_name in [\n",
    "        \"jinaai/jina-embeddings-v3\",\n",
    "        \"HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5\",\n",
    "        \"Alibaba-NLP/gte-large-en-v1.5\",\n",
    "    ]:\n",
    "        embeddings = CustomSentenceTransformerEmbeddings(embedding_model_name)\n",
    "    else:\n",
    "        embeddings = OllamaEmbeddings(model=embedding_model_name)\n",
    "        print(f\"Initialized Ollama embedding model: {embedding_model_name}\")\n",
    "\n",
    "    # Load vectorstore\n",
    "    vectorstore = FAISS.load_local(\n",
    "        faiss_path,\n",
    "        embeddings=embeddings,\n",
    "        allow_dangerous_deserialization=True,\n",
    "    )\n",
    "\n",
    "    for lm in lambda_mults:\n",
    "        print(f\"\\n--- Evaluating with lambda_mult = {lm} ---\")\n",
    "        for kv in k_values:\n",
    "            print(f\"\\n--- Evaluating with k_value = {kv} ---\")\n",
    "            compare_rows(\n",
    "                df=df,\n",
    "                bench_dict=bench_dict,\n",
    "                embedding_model_name=embedding_model_name,\n",
    "                schema=schema_type,\n",
    "                k_value=kv,\n",
    "                output_csv_path=output_csv_path,\n",
    "                comparison_length=100,\n",
    "                fetch_k=100,\n",
    "                lambda_mult=lm,\n",
    "            )\n",
    "\n",
    "    # Unload the model to free up GPU memory\n",
    "    if isinstance(embeddings, CustomSentenceTransformerEmbeddings):\n",
    "        embeddings.unload_model()\n",
    "\n",
    "print(\"\\n>>> All embedding models have been processed and unloaded.\")\n",
    "print(f\"\\n>>> Completed. See results in {output_csv_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Visualizations Embedding Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV = \"../../data/pipeline2/embed_eval/embed_eval_res_20250208_130510.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "\n",
    "\n",
    "def abbreviate_model_name(name):\n",
    "    \"\"\"\n",
    "    Abbreviates the model name to show only the first 5 and last 5 characters.\n",
    "\n",
    "    Parameters:\n",
    "        name (str): The original model name.\n",
    "\n",
    "    Returns:\n",
    "        str: The abbreviated model name.\n",
    "    \"\"\"\n",
    "    if len(name) <= 10:\n",
    "        return name\n",
    "    return f\"{name[:10]}...{name[-6:]}\"\n",
    "\n",
    "\n",
    "def load_and_prepare_data(csv_file_path):\n",
    "    \"\"\"\n",
    "    Loads the CSV data, performs basic cleanup, converts specified columns to numeric,\n",
    "    and organizes the 'model' and 'lambda_mult' columns to preserve their order of appearance.\n",
    "\n",
    "    Parameters:\n",
    "        csv_file_path (str): The file path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): The cleaned and prepared DataFrame.\n",
    "        model_order (list): List of models in the order they appear in the CSV.\n",
    "        lambda_order (list): List of lambda_mult values in sorted order.\n",
    "    \"\"\"\n",
    "    # 1) Load Data\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # A. Basic Cleanup & Info\n",
    "    print(f\"Loaded {len(df)} rows from {csv_file_path}\")\n",
    "    # Uncomment below lines if you wish to see the first few rows\n",
    "    # print(\"\\n--- First 5 Rows of the DataFrame ---\")\n",
    "    # print(df.head())\n",
    "\n",
    "    # B. Convert columns to numeric (MRR, mean_rank, etc.) if needed\n",
    "    numeric_cols = [\n",
    "        \"k_value\",\n",
    "        \"lambda_mult\",\n",
    "        \"retrieved_docs\",\n",
    "        \"found_exact\",\n",
    "        \"missed_exact\",\n",
    "        \"mean_rank\",\n",
    "        \"mrr\",\n",
    "    ]\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # C. Visual Setup\n",
    "    sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "    # D. Organize 'model' Column Order\n",
    "    # Extract unique models in the order they appear\n",
    "    model_order = df[\"model\"].drop_duplicates().tolist()\n",
    "\n",
    "    # Set 'model' as a categorical variable with the specified order\n",
    "    df[\"model\"] = pd.Categorical(df[\"model\"], categories=model_order, ordered=True)\n",
    "\n",
    "    # E. Organize 'lambda_mult' Column Order (sorted)\n",
    "    lambda_order = sorted(df[\"lambda_mult\"].dropna().unique().tolist())\n",
    "    df[\"lambda_mult\"] = pd.Categorical(\n",
    "        df[\"lambda_mult\"], categories=lambda_order, ordered=True\n",
    "    )\n",
    "\n",
    "    return df, model_order, lambda_order\n",
    "\n",
    "\n",
    "def plot_found_vs_missed(df, model_order, lambda_order):\n",
    "    \"\"\"\n",
    "    Creates a series of bar charts comparing 'found_exact' vs. 'missed_exact' for each model segmented by 'lambda_mult'.\n",
    "    Each lambda_mult value is plotted in its own subplot with abbreviated model names.\n",
    "    A single consolidated legend is positioned below all subplots.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The prepared DataFrame.\n",
    "        model_order (list): List of models in the desired order.\n",
    "        lambda_order (list): List of lambda_mult values in sorted order.\n",
    "\n",
    "    Returns:\n",
    "        agg_dict (dict): Dictionary containing aggregated DataFrames for each lambda_mult\n",
    "    \"\"\"\n",
    "    num_lambdas = len(lambda_order)\n",
    "    cols = 2  # Define number of columns in subplot grid\n",
    "    rows = (num_lambdas + 1) // cols  # Calculate number of rows needed\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(16, rows * 5), sharey=True)\n",
    "    axes = axes.flatten()  # Flatten in case of multiple rows\n",
    "\n",
    "    agg_dict = {}  # To store aggregated data for each lambda_mult\n",
    "\n",
    "    for idx, lambda_val in enumerate(lambda_order):\n",
    "        ax = axes[idx]\n",
    "        subset = df[df[\"lambda_mult\"] == lambda_val]\n",
    "        agg = (\n",
    "            subset.groupby(\"model\", observed=True)[[\"found_exact\", \"missed_exact\"]]\n",
    "            .sum()\n",
    "            .reindex(model_order)\n",
    "        )\n",
    "\n",
    "        # Abbreviate model names\n",
    "        abbreviated_models = [abbreviate_model_name(name) for name in agg.index]\n",
    "        agg.index = abbreviated_models\n",
    "\n",
    "        # Plotting\n",
    "        agg.plot(kind=\"bar\", ax=ax, width=0.8, legend=False)\n",
    "        ax.set_title(f\"λ = {lambda_val}\")\n",
    "        ax.set_xlabel(\"Models\")\n",
    "        ax.set_ylabel(\"Count of Chunks\")\n",
    "        ax.tick_params(axis=\"x\", rotation=45)\n",
    "        plt.setp(\n",
    "            ax.get_xticklabels(), ha=\"right\"\n",
    "        )  # Set horizontal alignment separately\n",
    "\n",
    "        agg_dict[lambda_val] = agg  # Store aggregated data\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for j in range(idx + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Create a single legend for all subplots\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"upper center\", ncol=5, bbox_to_anchor=(0.5, 0))\n",
    "\n",
    "    fig.suptitle(\"Found vs. Missed Chunks per Model and Lambda Multiplier\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    return agg_dict\n",
    "\n",
    "\n",
    "def plot_mean_rank_vs_kvalue(df, model_order, lambda_order):\n",
    "    \"\"\"\n",
    "    Generates a series of line plots showing 'mean_rank' versus 'k_value' for each model segmented by 'lambda_mult'.\n",
    "    Each lambda_mult value is plotted in its own subplot with a single legend positioned below all subplots.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The prepared DataFrame.\n",
    "        model_order (list): List of models in the desired order.\n",
    "        lambda_order (list): List of lambda_mult values in sorted order.\n",
    "\n",
    "    Returns:\n",
    "        agg_dict (dict): Dictionary containing aggregated DataFrames for each lambda_mult.\n",
    "    \"\"\"\n",
    "    num_lambdas = len(lambda_order)\n",
    "    cols = 2  # Define number of columns in subplot grid\n",
    "    rows = (num_lambdas + 1) // cols  # Calculate number of rows needed\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        rows, cols, figsize=(16, rows * 5), sharex=True, sharey=True\n",
    "    )\n",
    "    axes = axes.flatten()  # Flatten in case of multiple rows\n",
    "\n",
    "    palette = sns.color_palette(\"husl\", n_colors=len(model_order))\n",
    "\n",
    "    agg_dict = {}  # To store aggregated data for each lambda_mult\n",
    "\n",
    "    for idx, lambda_val in enumerate(lambda_order):\n",
    "        ax = axes[idx]\n",
    "        subset = df[df[\"lambda_mult\"] == lambda_val]\n",
    "        agg = (\n",
    "            subset.groupby([\"k_value\", \"model\"], observed=True)[\"mean_rank\"]\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        for i, model in enumerate(model_order):\n",
    "            model_data = agg[agg[\"model\"] == model]\n",
    "            ax.plot(\n",
    "                model_data[\"k_value\"],\n",
    "                model_data[\"mean_rank\"],\n",
    "                marker=\"o\",\n",
    "                label=abbreviate_model_name(model),\n",
    "                color=palette[i % len(palette)],\n",
    "            )\n",
    "\n",
    "        ax.set_title(f\"λ = {lambda_val}\")\n",
    "        ax.set_xlabel(\"k_value\")\n",
    "        ax.set_ylabel(\"Mean Rank\")\n",
    "        ax.tick_params(axis=\"x\", rotation=45)\n",
    "        plt.setp(\n",
    "            ax.get_xticklabels(), ha=\"right\"\n",
    "        )  # Set horizontal alignment separately\n",
    "\n",
    "        agg_dict[lambda_val] = agg  # Store aggregated data\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for j in range(idx + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Create a single legend for all subplots\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"upper center\", ncol=5, bbox_to_anchor=(0.5, 0))\n",
    "\n",
    "    fig.suptitle(\"Mean Rank vs. k_value per Model and Lambda Multiplier\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    return agg_dict\n",
    "\n",
    "\n",
    "def plot_missed_chunks_vs_kvalue(df, model_order, lambda_order):\n",
    "    \"\"\"\n",
    "    Produces a series of line plots illustrating 'missed_exact' versus 'k_value' for each model segmented by 'lambda_mult'.\n",
    "    Each lambda_mult value is plotted in its own subplot with a single legend positioned below all subplots.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The prepared DataFrame.\n",
    "        model_order (list): List of models in the desired order.\n",
    "        lambda_order (list): List of lambda_mult values in sorted order.\n",
    "\n",
    "    Returns:\n",
    "        agg_dict (dict): Dictionary containing aggregated DataFrames for each lambda_mult.\n",
    "    \"\"\"\n",
    "    num_lambdas = len(lambda_order)\n",
    "    cols = 2  # Define number of columns in subplot grid\n",
    "    rows = (num_lambdas + 1) // cols  # Calculate number of rows needed\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        rows, cols, figsize=(16, rows * 5), sharex=True, sharey=True\n",
    "    )\n",
    "    axes = axes.flatten()  # Flatten in case of multiple rows\n",
    "\n",
    "    palette = sns.color_palette(\"husl\", n_colors=len(model_order))\n",
    "\n",
    "    agg_dict = {}  # To store aggregated data for each lambda_mult\n",
    "\n",
    "    for idx, lambda_val in enumerate(lambda_order):\n",
    "        ax = axes[idx]\n",
    "        subset = df[df[\"lambda_mult\"] == lambda_val]\n",
    "        agg = (\n",
    "            subset.groupby([\"k_value\", \"model\"], observed=True)[\"missed_exact\"]\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        for i, model in enumerate(model_order):\n",
    "            model_data = agg[agg[\"model\"] == model]\n",
    "            ax.plot(\n",
    "                model_data[\"k_value\"],\n",
    "                model_data[\"missed_exact\"],\n",
    "                marker=\"o\",\n",
    "                label=abbreviate_model_name(model),\n",
    "                color=palette[i % len(palette)],\n",
    "            )\n",
    "\n",
    "        ax.set_title(f\"λ = {lambda_val}\")\n",
    "        ax.set_xlabel(\"k_value\")\n",
    "        ax.set_ylabel(\"Missed Chunks\")\n",
    "        ax.tick_params(axis=\"x\", rotation=45)\n",
    "        plt.setp(\n",
    "            ax.get_xticklabels(), ha=\"right\"\n",
    "        )  # Set horizontal alignment separately\n",
    "\n",
    "        agg_dict[lambda_val] = agg  # Store aggregated data\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for j in range(idx + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Create a single legend for all subplots\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=\"upper center\", ncol=5, bbox_to_anchor=(0.5, 0.02))\n",
    "\n",
    "    fig.suptitle(\n",
    "        \"Missed Chunks vs. k_value per Model and Lambda Multiplier\", fontsize=16\n",
    "    )\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    return agg_dict\n",
    "\n",
    "\n",
    "def interpret_results(df, model_order, lambda_order):\n",
    "    \"\"\"\n",
    "    Determines the best model based on the following criteria for each lambda_mult and returns the lambda_mult\n",
    "    where the overall best model is found:\n",
    "    1. Prefer models with zero total missed chunks (summed across all attributes).\n",
    "       - Among these, choose the one with the lowest k_value.\n",
    "       - If tied, choose the one with the lowest mean_rank.\n",
    "    2. If no model has zero misses for a lambda_mult, choose the model with the lowest number of misses.\n",
    "       - Among these, choose the one with the lowest k_value.\n",
    "       - If tied, choose the one with the lowest mean_rank.\n",
    "\n",
    "    Prints:\n",
    "        Models sorted by k_value and then by mean_rank within each lambda_mult.\n",
    "        DataFrame table of each model's best performance, sorted by lowest misses and then by lowest mean rank.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The prepared DataFrame.\n",
    "        model_order (list): List of models in the desired order.\n",
    "        lambda_order (list): List of lambda_mult values in sorted order.\n",
    "\n",
    "    Returns:\n",
    "        best_lambda_mult (float): The lambda_mult corresponding to the overall best model.\n",
    "    \"\"\"\n",
    "    # Build summary table\n",
    "    agg = (\n",
    "        df.groupby([\"model\", \"lambda_mult\", \"k_value\"], observed=True)\n",
    "        .agg(\n",
    "            total_missed_exact=pd.NamedAgg(column=\"missed_exact\", aggfunc=\"sum\"),\n",
    "            average_mean_rank=pd.NamedAgg(column=\"mean_rank\", aggfunc=\"mean\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    results = {}\n",
    "    for model in model_order:\n",
    "        model_rows = agg[agg[\"model\"] == model]\n",
    "        if model_rows.empty:\n",
    "            continue\n",
    "        model_rows = model_rows.copy()\n",
    "        # Sort key: lower misses, then lower mean rank, then lower k_value\n",
    "        model_rows[\"sort_key\"] = model_rows.apply(\n",
    "            lambda row: (\n",
    "                row[\"total_missed_exact\"],\n",
    "                row[\"average_mean_rank\"],\n",
    "                row[\"k_value\"],\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        best = model_rows.sort_values(\"sort_key\").iloc[0]\n",
    "        criteria = \"Zero Misses\" if best[\"total_missed_exact\"] == 0 else \"Lowest Misses\"\n",
    "        results[model] = {\n",
    "            \"lambda_mult\": best[\"lambda_mult\"],\n",
    "            \"k_value\": best[\"k_value\"],\n",
    "            \"misses\": best[\"total_missed_exact\"],\n",
    "            \"mean_rank\": best[\"average_mean_rank\"],\n",
    "            \"criteria\": criteria,\n",
    "        }\n",
    "\n",
    "    rows = []\n",
    "    for model in model_order:\n",
    "        if model in results:\n",
    "            r = results[model]\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"Model\": model,\n",
    "                    \"Lambda\": r[\"lambda_mult\"],\n",
    "                    \"k_value\": r[\"k_value\"],\n",
    "                    \"Misses\": r[\"misses\"],\n",
    "                    \"Mean Rank\": r[\"mean_rank\"],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"Model\": model,\n",
    "                    \"Lambda\": \"N/A\",\n",
    "                    \"k_value\": \"N/A\",\n",
    "                    \"Misses\": float(\"inf\"),\n",
    "                    \"Mean Rank\": float(\"inf\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    summary_df = pd.DataFrame(rows)\n",
    "    summary_df[\"Misses\"] = pd.to_numeric(summary_df[\"Misses\"], errors=\"coerce\")\n",
    "    summary_df[\"Mean Rank\"] = pd.to_numeric(summary_df[\"Mean Rank\"], errors=\"coerce\")\n",
    "    summary_df = summary_df.sort_values(\n",
    "        by=[\"Misses\", \"Mean Rank\"], ascending=[True, True]\n",
    "    )\n",
    "    summary_df[\"Mean Rank\"] = summary_df[\"Mean Rank\"].apply(\n",
    "        lambda x: f\"{x:.2f}\" if pd.notnull(x) else \"N/A\"\n",
    "    )\n",
    "\n",
    "    # Set column header justification to left\n",
    "    pd.set_option('display.colheader_justify', 'left')\n",
    "    print(\"\\n====== BEST PERFORMANCE PER MODEL ======\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    # Save summary as markdown\n",
    "    markdown_str = tabulate(summary_df, headers=\"keys\", tablefmt=\"pipe\", showindex=False)\n",
    "\n",
    "    md_dir = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"md\")\n",
    "    os.makedirs(md_dir, exist_ok=True)\n",
    "    csv_timestamp = CSV.split(\"/\")[-1].replace(\".csv\", \"\")\n",
    "    md_path = os.path.join(md_dir, f\"summary_table_{csv_timestamp}.md\")\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(markdown_str)\n",
    "    print(f\"\\nMarkdown summary saved to: {md_path}\")\n",
    "\n",
    "    # Determine overall best model per lambda\n",
    "    best_overall_criteria = None\n",
    "    best_overall_list = []\n",
    "\n",
    "    for lambda_val in lambda_order:\n",
    "        print(f\"\\n===== Lambda Multiplier: {lambda_val} =====\")\n",
    "        subset = df[df[\"lambda_mult\"] == lambda_val]\n",
    "        group_agg = (\n",
    "            subset.groupby([\"model\", \"k_value\"], observed=True)\n",
    "            .agg(\n",
    "                total_missed_exact=pd.NamedAgg(column=\"missed_exact\", aggfunc=\"sum\"),\n",
    "                average_mean_rank=pd.NamedAgg(column=\"mean_rank\", aggfunc=\"mean\"),\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        sorted_group = group_agg.sort_values(\n",
    "            by=[\"total_missed_exact\", \"k_value\", \"average_mean_rank\"],\n",
    "            ascending=[True, True, True],\n",
    "        )\n",
    "\n",
    "        best_model_row = sorted_group.iloc[0]\n",
    "        criteria = (\n",
    "            \"Zero Misses\"\n",
    "            if best_model_row[\"total_missed_exact\"] == 0\n",
    "            else \"Lowest Misses\"\n",
    "        )\n",
    "        best_model = best_model_row[\"model\"]\n",
    "        best_k = best_model_row[\"k_value\"]\n",
    "        best_misses = best_model_row[\"total_missed_exact\"]\n",
    "        best_mean_rank = best_model_row[\"average_mean_rank\"]\n",
    "\n",
    "        print(\"\\n------ SUMMARY PER LAMBDA ------\")\n",
    "        if criteria == \"Zero Misses\":\n",
    "            print(\n",
    "                f\"The best model for lambda {lambda_val} is '{best_model}' with \"\n",
    "                f\"k_value={best_k} and mean_rank={best_mean_rank:.2f} (Zero Misses).\"\n",
    "            )\n",
    "        else:\n",
    "            print(\n",
    "                f\"No model had zero misses. Best for lambda {lambda_val} is '{best_model}' with \"\n",
    "                f\"{int(best_misses)} misses, k_value={best_k}, mean_rank={best_mean_rank:.2f}.\"\n",
    "            )\n",
    "\n",
    "        # Now compare with the overall best\n",
    "        if not best_overall_list:\n",
    "            # This is the first best found; store it\n",
    "            best_overall_criteria = criteria\n",
    "            best_overall_list.append(\n",
    "                {\n",
    "                    \"model\": best_model,\n",
    "                    \"lambda_mult\": lambda_val,\n",
    "                    \"k_value\": best_k,\n",
    "                    \"misses\": best_misses,\n",
    "                    \"mean_rank\": best_mean_rank,\n",
    "                    \"criteria\": criteria,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            # Compare with current best\n",
    "            current_best = best_overall_list[0]  # All in list share same metrics\n",
    "            # If new candidate has Zero Misses and old was Lowest Misses\n",
    "            if criteria == \"Zero Misses\" and best_overall_criteria != \"Zero Misses\":\n",
    "                best_overall_list.clear()\n",
    "                best_overall_criteria = criteria\n",
    "                best_overall_list.append(\n",
    "                    {\n",
    "                        \"model\": best_model,\n",
    "                        \"lambda_mult\": lambda_val,\n",
    "                        \"k_value\": best_k,\n",
    "                        \"misses\": best_misses,\n",
    "                        \"mean_rank\": best_mean_rank,\n",
    "                        \"criteria\": criteria,\n",
    "                    }\n",
    "                )\n",
    "            elif criteria == best_overall_criteria:\n",
    "                # Both are \"Zero Misses\" or both \"Lowest Misses\"\n",
    "                # Compare total misses, k_value, mean_rank\n",
    "                if best_misses < current_best[\"misses\"]:\n",
    "                    best_overall_list.clear()\n",
    "                    best_overall_list.append(\n",
    "                        {\n",
    "                            \"model\": best_model,\n",
    "                            \"lambda_mult\": lambda_val,\n",
    "                            \"k_value\": best_k,\n",
    "                            \"misses\": best_misses,\n",
    "                            \"mean_rank\": best_mean_rank,\n",
    "                            \"criteria\": criteria,\n",
    "                        }\n",
    "                    )\n",
    "                elif best_misses == current_best[\"misses\"]:\n",
    "                    if best_k < current_best[\"k_value\"]:\n",
    "                        best_overall_list.clear()\n",
    "                        best_overall_list.append(\n",
    "                            {\n",
    "                                \"model\": best_model,\n",
    "                                \"lambda_mult\": lambda_val,\n",
    "                                \"k_value\": best_k,\n",
    "                                \"misses\": best_misses,\n",
    "                                \"mean_rank\": best_mean_rank,\n",
    "                                \"criteria\": criteria,\n",
    "                            }\n",
    "                        )\n",
    "                    elif best_k == current_best[\"k_value\"]:\n",
    "                        if best_mean_rank < current_best[\"mean_rank\"]:\n",
    "                            # strictly better mean rank\n",
    "                            best_overall_list.clear()\n",
    "                            best_overall_list.append(\n",
    "                                {\n",
    "                                    \"model\": best_model,\n",
    "                                    \"lambda_mult\": lambda_val,\n",
    "                                    \"k_value\": best_k,\n",
    "                                    \"misses\": best_misses,\n",
    "                                    \"mean_rank\": best_mean_rank,\n",
    "                                    \"criteria\": criteria,\n",
    "                                }\n",
    "                            )\n",
    "                        elif abs(best_mean_rank - current_best[\"mean_rank\"]) < 1e-9:\n",
    "                            # Same misses, same k, same mean rank => It's a tie\n",
    "                            best_overall_list.append(\n",
    "                                {\n",
    "                                    \"model\": best_model,\n",
    "                                    \"lambda_mult\": lambda_val,\n",
    "                                    \"k_value\": best_k,\n",
    "                                    \"misses\": best_misses,\n",
    "                                    \"mean_rank\": best_mean_rank,\n",
    "                                    \"criteria\": criteria,\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "            # If new candidate is \"Lowest Misses\" but current best is \"Zero Misses\", do nothing\n",
    "            # because Zero Misses is always better.\n",
    "    print(\"\\n====== FINAL OVERALL BEST MODEL(S) ======\")\n",
    "    # All entries in best_overall_list share the same performance, but may differ in lambda\n",
    "    # or even be different models with identical performance.\n",
    "    # Print them all:\n",
    "    if best_overall_list:\n",
    "        for i, item in enumerate(best_overall_list, start=1):\n",
    "            if item[\"criteria\"] == \"Zero Misses\":\n",
    "                print(\n",
    "                    f\"({i}) `{item['model']}` with `k_value = {item['k_value']}`, `mean_rank = {item['mean_rank']:.2f}`, \"\n",
    "                    f\"(Zero Misses), `lambda_mult = {item['lambda_mult']}`\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"({i}) `{item['model']}` with `{int(item['misses'])} misses`, `k_value = {item['k_value']}`, \"\n",
    "                    f\"`mean_rank = {item['mean_rank']:.2f}`, `lambda_mult = {item['lambda_mult']}`\"\n",
    "                )\n",
    "        return best_overall_list[0][\"lambda_mult\"]\n",
    "    else:\n",
    "        print(\"No best model was found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def visualize_embedding_results(csv_file_path):\n",
    "    \"\"\"\n",
    "    Orchestrates the loading of data and creation of visualizations for embedding model comparisons,\n",
    "    now segmented by 'lambda_mult'. Aggregated data from all plots are printed at the end.\n",
    "\n",
    "    Parameters:\n",
    "        csv_file_path (str): The file path to the CSV file.\n",
    "    \"\"\"\n",
    "    # Load and prepare data\n",
    "    df, model_order, lambda_order = load_and_prepare_data(csv_file_path)\n",
    "\n",
    "    # Create Visualizations and collect aggregated data\n",
    "    agg_found_vs_missed = plot_found_vs_missed(df, model_order, lambda_order)\n",
    "    agg_mean_rank = plot_mean_rank_vs_kvalue(df, model_order, lambda_order)\n",
    "    agg_missed_chunks = plot_missed_chunks_vs_kvalue(df, model_order, lambda_order)\n",
    "\n",
    "    # Print all aggregated data\n",
    "    print(\"\\n=== Aggregated Data from Plots ===\")\n",
    "\n",
    "    # Found vs. Missed\n",
    "    print(\"\\n--- Found Exact vs. Missed Exact per Model and Lambda Multiplier ---\")\n",
    "    for lambda_val, agg in agg_found_vs_missed.items():\n",
    "        print(f\"\\nLambda Multiplier: {lambda_val}\")\n",
    "        print(agg)\n",
    "\n",
    "    # Mean Rank\n",
    "    print(\"\\n--- Mean Rank per k_value for Each Model and Lambda Multiplier ---\")\n",
    "    for lambda_val, agg in agg_mean_rank.items():\n",
    "        print(f\"\\nLambda Multiplier: {lambda_val}\")\n",
    "        print(agg)\n",
    "\n",
    "    # Missed Chunks\n",
    "    print(\"\\n--- Missed Chunks per k_value for Each Model and Lambda Multiplier ---\")\n",
    "    for lambda_val, agg in agg_missed_chunks.items():\n",
    "        print(f\"\\nLambda Multiplier: {lambda_val}\")\n",
    "        print(agg)\n",
    "\n",
    "    # Interpret Results and determine best lambda\n",
    "    best_lambda_mult = interpret_results(df, model_order, lambda_order)\n",
    "    return best_lambda_mult\n",
    "\n",
    "\n",
    "plot_csv_file = CSV\n",
    "\n",
    "best_lambda_mult = visualize_embedding_results(plot_csv_file)\n",
    "print(\"BEST LAMBDA MULT FROM FIRST CODE:\", best_lambda_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lambda_mult = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def abbreviate_product_name(name):\n",
    "    \"\"\"\n",
    "    Abbreviates the product name.\n",
    "\n",
    "    Parameters:\n",
    "        name (str): The original product name.\n",
    "\n",
    "    Returns:\n",
    "        str: The abbreviated product name.\n",
    "    \"\"\"\n",
    "    if len(name) <= 30:\n",
    "        return name\n",
    "    return f\"{name.lower()[:70]}...\"\n",
    "\n",
    "\n",
    "def load_and_prepare_data(csv_file_path):\n",
    "    \"\"\"\n",
    "    Loads the CSV data, performs basic cleanup, converts specified columns to numeric,\n",
    "    fills missing numeric values with -1, and organizes 'model' and 'lambda_mult'\n",
    "    to preserve their order of appearance.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    print(f\"Loaded {len(df)} rows from {csv_file_path}\")\n",
    "\n",
    "    # Fixed numeric_cols list by adding the missing comma after \"rank\"\n",
    "    numeric_cols = [\n",
    "        \"k_value\",\n",
    "        \"retrieved_docs\",\n",
    "        \"found_exact\",\n",
    "        \"missed_exact\",\n",
    "        \"mean_rank\",\n",
    "        \"rank\",\n",
    "        \"mrr\",\n",
    "        \"lambda_mult\",\n",
    "    ]\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            # Convert the column to numeric, coercing errors to NaN\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "            # Fill NaN values with -1\n",
    "            df[col] = df[col].fillna(-1)\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "    # Organize 'model' by appearance order\n",
    "    model_order = df[\"model\"].drop_duplicates().tolist()\n",
    "    df[\"model\"] = pd.Categorical(df[\"model\"], categories=model_order, ordered=True)\n",
    "\n",
    "    # Organize 'lambda_mult' by sorted order if it exists\n",
    "    if \"lambda_mult\" in df.columns:\n",
    "        lambda_order = sorted(df[\"lambda_mult\"].dropna().unique().tolist())\n",
    "        df[\"lambda_mult\"] = pd.Categorical(\n",
    "            df[\"lambda_mult\"], categories=lambda_order, ordered=True\n",
    "        )\n",
    "    else:\n",
    "        lambda_order = []\n",
    "\n",
    "    return df, model_order, lambda_order\n",
    "\n",
    "def plot_found_vs_missed(df, model_order, best_lambda_mult):\n",
    "    \"\"\"\n",
    "    Side-by-side bar chart for 'found_exact' vs. 'missed_exact' for each model (filtered by best lambda).\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6), constrained_layout=True)\n",
    "    subset = df.groupby(\"model\", as_index=False, observed=True)[\n",
    "        [\"found_exact\", \"missed_exact\"]\n",
    "    ].sum()\n",
    "\n",
    "    # Re-sort models after filtering\n",
    "    subset[\"model\"] = pd.Categorical(\n",
    "        subset[\"model\"], categories=model_order, ordered=True\n",
    "    )\n",
    "    subset = subset.dropna(subset=[\"model\"]).sort_values(\"model\")\n",
    "\n",
    "    X = range(len(subset))\n",
    "    width = 0.35\n",
    "    ax.bar(\n",
    "        [x - width / 2 for x in X],\n",
    "        subset[\"found_exact\"],\n",
    "        width=width,\n",
    "        label=\"Found Exact\",\n",
    "    )\n",
    "    ax.bar(\n",
    "        [x + width / 2 for x in X],\n",
    "        subset[\"missed_exact\"],\n",
    "        width=width,\n",
    "        label=\"Missed Exact\",\n",
    "    )\n",
    "    ax.set_xticks(X)\n",
    "    ax.set_xticklabels(subset[\"model\"], rotation=45, ha=\"right\")\n",
    "    ax.set_title(\n",
    "        f\"Found vs. Missed Chunks \\n(Summed over all k_values with best_lambda_mult={best_lambda_mult})\"\n",
    "    )\n",
    "    ax.set_xlabel(\"Models\")\n",
    "    ax.set_ylabel(\"Count of Chunks\")\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        loc=\"lower center\",\n",
    "        ncol=2,\n",
    "        frameon=True,\n",
    "        bbox_to_anchor=(0.45, 0.1),\n",
    "    )\n",
    "    fig.subplots_adjust(bottom=0.2)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n--- Found Exact vs. Missed Exact per Model (Filtered) ---\")\n",
    "    print(subset)\n",
    "\n",
    "\n",
    "def plot_mean_rank_vs_kvalue(df, model_order, best_lambda_mult):\n",
    "    \"\"\"\n",
    "    Line plot showing 'mean_rank' vs. 'k_value' for each model (filtered by best lambda).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    mean_rank_summary = {}\n",
    "\n",
    "    palette = sns.color_palette(\"husl\", n_colors=len(model_order))\n",
    "\n",
    "    for i, model_name in enumerate(model_order):\n",
    "        model_data = df[df[\"model\"] == model_name]\n",
    "        meanranks = model_data.groupby(\"k_value\")[\"mean_rank\"].mean().reset_index()\n",
    "        mean_rank_summary[model_name] = meanranks\n",
    "        plt.plot(\n",
    "            meanranks[\"k_value\"],\n",
    "            meanranks[\"mean_rank\"],\n",
    "            marker=\"o\",\n",
    "            label=model_name,\n",
    "            color=palette[i % len(palette)],\n",
    "        )\n",
    "\n",
    "    plt.title(\n",
    "        f\"Mean Rank vs. k_value for Each Model \\n(best_lambda_mult={best_lambda_mult})\"\n",
    "    )\n",
    "    plt.xlabel(\"k_value\")\n",
    "    plt.ylabel(\"Mean Rank\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n--- Mean Rank per k_value for Each Model (Filtered) ---\")\n",
    "    for model, summary in mean_rank_summary.items():\n",
    "        print(f\"\\nModel: {model}\")\n",
    "        print(summary.to_string(index=False))\n",
    "\n",
    "\n",
    "def plot_missed_chunks_vs_kvalue(df, model_order, best_lambda_mult):\n",
    "    \"\"\"\n",
    "    Line plot illustrating 'missed_exact' vs. 'k_value' for each model (filtered by best lambda).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    missed_summary = {}\n",
    "\n",
    "    palette = sns.color_palette(\"husl\", n_colors=len(model_order))\n",
    "\n",
    "    for i, model_name in enumerate(model_order):\n",
    "        model_data = df[df[\"model\"] == model_name]\n",
    "        if model_data.empty:\n",
    "            continue\n",
    "        missed_per_k = model_data.groupby(\"k_value\")[\"missed_exact\"].sum().reset_index()\n",
    "        missed_summary[model_name] = missed_per_k\n",
    "        plt.plot(\n",
    "            missed_per_k[\"k_value\"],\n",
    "            missed_per_k[\"missed_exact\"],\n",
    "            marker=\"o\",\n",
    "            label=model_name,\n",
    "            color=palette[i % len(palette)],\n",
    "        )\n",
    "\n",
    "    plt.title(\n",
    "        f\"Missed Chunks vs. k_value for Each Model \\n(best_lambda_mult={best_lambda_mult})\"\n",
    "    )\n",
    "    plt.xlabel(\"k_value\")\n",
    "    plt.ylabel(\"Missed Chunks\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n--- Missed Chunks per k_value for Each Model (Filtered) ---\")\n",
    "    for model, summary in missed_summary.items():\n",
    "        print(f\"\\nModel: {model}\")\n",
    "        print(summary.to_string(index=False))\n",
    "\n",
    "\n",
    "def plot_heatmap_rank(df, model_order, best_lambda_mult, csv_file_path):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of rank values with Product Name on the Y-axis,\n",
    "    ensuring the products appear in the original CSV order and that \n",
    "    missing rank values are replaced with -1. Also, appends part of the\n",
    "    CSV file name to the plot title.\n",
    "    \"\"\"\n",
    "    # Extract the base CSV filename without extension\n",
    "    csv_base = os.path.basename(csv_file_path)\n",
    "    csv_name = os.path.splitext(csv_base)[0]\n",
    "    \n",
    "    # Preserve original CSV order by creating a unique order index\n",
    "    df[\"Product Order\"] = df[\"product_name\"].factorize()[0]\n",
    "    df[\"Product Name (Short)\"] = df[\"product_name\"].apply(abbreviate_product_name)\n",
    "\n",
    "    # Aggregate rank values to ensure unique (Product Order, Product Name, Model) combinations\n",
    "    rank_df = (\n",
    "        df.groupby([\"Product Order\", \"Product Name (Short)\", \"model\"], observed=True)[\"rank\"]\n",
    "          .mean()  # Take the mean rank across different k-values\n",
    "          .reset_index()\n",
    "          .rename(columns={\n",
    "              \"model\": \"Model\",\n",
    "              \"Product Name (Short)\": \"Product Name\",\n",
    "              \"rank\": \"Rank\"\n",
    "          })\n",
    "    )\n",
    "\n",
    "    # Ensure the DataFrame is sorted by the original CSV order\n",
    "    rank_df = rank_df.sort_values(\"Product Order\")\n",
    "\n",
    "    # Pivot using both \"Product Order\" and \"Product Name\" to maintain order\n",
    "    pivot_data = rank_df.pivot_table(\n",
    "        index=[\"Product Order\", \"Product Name\"],\n",
    "        columns=\"Model\",\n",
    "        values=\"Rank\",\n",
    "        aggfunc=\"mean\",\n",
    "        observed=True,\n",
    "    )\n",
    "\n",
    "    # Sort the pivot table by the \"Product Order\" level\n",
    "    pivot_data = pivot_data.sort_index(level=\"Product Order\")\n",
    "\n",
    "    # Drop the \"Product Order\" level so that only the product names appear on the y-axis\n",
    "    pivot_data.index = pivot_data.index.droplevel(\"Product Order\")\n",
    "    \n",
    "    # Plotting the heatmap with the CSV part appended to the title\n",
    "    plt.figure(figsize=(12, 50))  # Adjust height for better visualization\n",
    "    sns.heatmap(\n",
    "        pivot_data,\n",
    "        annot=True,\n",
    "        fmt=\".1f\",\n",
    "        cmap=\"YlGnBu\",\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"label\": \"Rank\"}\n",
    "    )\n",
    "    plt.title(f\"Rank by Product Name per Model ({csv_name})\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(\"Product Name (Original CSV Order)\")\n",
    "    plt.xticks(rotation=0, ha=\"right\")\n",
    "    plt.yticks(rotation=0, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    heatmap_rank_df_string = pivot_data.to_string()\n",
    "\n",
    "    return heatmap_rank_df_string\n",
    "\n",
    "\n",
    "def interpret_results(df):\n",
    "    \"\"\"\n",
    "    Same logic as before, but operates on the already filtered DataFrame (by best_lambda).\n",
    "    Determines the best model under that single lambda context.\n",
    "    \"\"\"\n",
    "    group_agg = (\n",
    "        df.groupby([\"model\", \"k_value\"], observed=True)\n",
    "        .agg(\n",
    "            missed_exact=pd.NamedAgg(column=\"missed_exact\", aggfunc=\"sum\"),\n",
    "            mean_rank=pd.NamedAgg(column=\"mean_rank\", aggfunc=\"mean\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    print(\"\\n====== AGGREGATED RESULTS (Filtered) ======\")\n",
    "    print(group_agg)\n",
    "\n",
    "    sorted_group = group_agg.sort_values(\n",
    "        by=[\"missed_exact\", \"k_value\", \"mean_rank\"],\n",
    "        ascending=[True, True, True],\n",
    "    )\n",
    "\n",
    "    best_model_row = sorted_group.iloc[0]\n",
    "    if best_model_row[\"missed_exact\"] == 0:\n",
    "        criteria = \"Zero Misses\"\n",
    "    else:\n",
    "        criteria = \"Lowest Misses\"\n",
    "\n",
    "    best_model = best_model_row[\"model\"]\n",
    "    best_k = best_model_row[\"k_value\"]\n",
    "    best_misses = best_model_row[\"missed_exact\"]\n",
    "    best_mean_rank = best_model_row[\"mean_rank\"]\n",
    "\n",
    "    print(\"\\n====== MINIMUM k_value WITH ZERO MISSES (Filtered) ======\")\n",
    "    zero_miss_df = group_agg[group_agg[\"missed_exact\"] == 0]\n",
    "    if not zero_miss_df.empty:\n",
    "        models_zero_miss = (\n",
    "            zero_miss_df.groupby(\"model\", observed=True)\n",
    "            .agg(min_k_value=pd.NamedAgg(column=\"k_value\", aggfunc=\"min\"))\n",
    "            .reset_index()\n",
    "        )\n",
    "        merged_zero_miss = pd.merge(\n",
    "            models_zero_miss,\n",
    "            zero_miss_df,\n",
    "            left_on=[\"model\", \"min_k_value\"],\n",
    "            right_on=[\"model\", \"k_value\"],\n",
    "        )\n",
    "        merged_zero_miss_sorted = merged_zero_miss.sort_values(\n",
    "            by=[\"min_k_value\", \"mean_rank\"]\n",
    "        )\n",
    "        for _, row in merged_zero_miss_sorted.iterrows():\n",
    "            print(\n",
    "                f\"{row['model']}: k_value = {row['min_k_value']}, mean_rank = {row['mean_rank']:.2f}\"\n",
    "            )\n",
    "    else:\n",
    "        print(\"No model achieved zero total misses under this lambda.\")\n",
    "\n",
    "    print(\"\\n====== SUMMARY (Filtered) ======\")\n",
    "    if criteria == \"Zero Misses\":\n",
    "        print(\n",
    "            f\"Best model (filtered) is `{best_model}` with \"\n",
    "            f\"`k_value = {best_k}` and `mean_rank = {best_mean_rank:.2f}` (Zero Misses).\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"No zero-miss model under this lambda. Best is `{best_model}` with \"\n",
    "            f\"`{int(best_misses)} misses`, `k_value = {best_k}`, `mean_rank = {best_mean_rank:.2f}`.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def visualize_embedding_results(csv_file_path, best_lambda_mult):\n",
    "    \"\"\"\n",
    "    Orchestrates loading data, filtering on the best lambda, and creating visualizations.\n",
    "    \"\"\"\n",
    "    df, model_order, lambda_order = load_and_prepare_data(csv_file_path)\n",
    "\n",
    "    # Filter to the best lambda identified from the first code\n",
    "    df = df[df[\"lambda_mult\"] == best_lambda_mult]\n",
    "    df = df.dropna(subset=[\"model\"])\n",
    "    # Update model order after filtering\n",
    "    new_model_order = df[\"model\"].drop_duplicates().tolist()\n",
    "    df[\"model\"] = pd.Categorical(df[\"model\"], categories=new_model_order, ordered=True)\n",
    "\n",
    "    print(f\"\\nUsing best_lambda_mult = {best_lambda_mult}\")\n",
    "    print(f\"Remaining rows after filtering: {len(df)}\")\n",
    "\n",
    "    plot_found_vs_missed(df, new_model_order, best_lambda_mult)\n",
    "    plot_mean_rank_vs_kvalue(df, new_model_order, best_lambda_mult)\n",
    "    plot_missed_chunks_vs_kvalue(df, new_model_order, best_lambda_mult)\n",
    "    heatmap_rank_df_string = plot_heatmap_rank(df, new_model_order, best_lambda_mult, csv_file_path)\n",
    "    interpret_results(df)\n",
    "\n",
    "    return heatmap_rank_df_string\n",
    "\n",
    "\n",
    "########\n",
    "# Usage\n",
    "########\n",
    "best_lambda_mult = best_lambda_mult\n",
    "visualize_embedding_results_txt = visualize_embedding_results(plot_csv_file, best_lambda_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the heatmap data in text format\n",
    "print(\"\\nHeatmap Data (Text Format):\\n\")\n",
    "print(visualize_embedding_results_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
