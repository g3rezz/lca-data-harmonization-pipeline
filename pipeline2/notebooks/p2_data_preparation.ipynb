{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract generic and missing categories from SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from IPython.display import display  # For Jupyter Notebook display\n",
    "\n",
    "# SQLite database file path\n",
    "DB_PATH = \"../../data/pipeline2/sql/epd_database.sqlite\"\n",
    "OUTPUT_FILE_PATH = \"../../data/pipeline2/sql/filtered_epd_data05.csv\"\n",
    "\n",
    "# Allowed Classifications (normalized)\n",
    "ALLOWED_CATEGORIES = {\n",
    "    \"construction products, infrastructure and buildings\",\n",
    "    \"construction products\",\n",
    "    \"\"  # Handling missing categories\n",
    "}\n",
    "\n",
    "# Synonym & Unit Normalization Dictionary\n",
    "SYNONYM_DICT = {\n",
    "    \"kilogram\": \"kg\",\n",
    "    \"kilograms\": \"kg\",\n",
    "    \"gram\": \"g\",\n",
    "    \"grams\": \"g\",\n",
    "    \"liter\": \"L\",\n",
    "    \"liters\": \"L\",\n",
    "    \"metre\": \"m\",\n",
    "    \"meter\": \"m\",\n",
    "    \"centimetre\": \"cm\",\n",
    "    \"centimeter\": \"cm\",\n",
    "    \"millimetre\": \"mm\",\n",
    "    \"millimeter\": \"mm\"\n",
    "}\n",
    "\n",
    "def clean_text(text, lowercase=True):\n",
    "    \"\"\"Cleans and normalizes text: removes newlines, extra spaces, normalizes punctuation, and standardizes synonyms/units.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"N/A\"\n",
    "\n",
    "    text = text.strip().replace(\"\\n\", \" \")          # Remove newlines and trim spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)                  # Remove excessive spaces\n",
    "    if lowercase:\n",
    "        text = text.lower()                           # Lowercase text if desired\n",
    "    # Normalize punctuation (replace \"&\" with \"and\")\n",
    "    text = text.replace(\"&\", \"and\")\n",
    "    # Remove special characters (except spaces and common separators)\n",
    "    text = re.sub(r'[^a-zA-Z0-9.:,\\-_/()\\s]', '', text)\n",
    "    # Normalize synonyms and units\n",
    "    for key, value in SYNONYM_DICT.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text\n",
    "\n",
    "def extract_epd_data(json_data):\n",
    "    \"\"\"Extracts key fields from the EPD JSON document, including additional fields.\"\"\"\n",
    "    try:\n",
    "        process_info = json_data.get(\"processInformation\", {})\n",
    "        dataset_info = process_info.get(\"dataSetInformation\", {})\n",
    "        classification_info = dataset_info.get(\"classificationInformation\", {}).get(\"classification\", [])\n",
    "        geography_info = process_info.get(\"geography\", {}).get(\"locationOfOperationSupplyOrProduction\", {})\n",
    "        technology_info = process_info.get(\"technology\", {})\n",
    "        modelling_info = json_data.get(\"modellingAndValidation\", {})\n",
    "        exchanges_info = json_data.get(\"exchanges\", {})\n",
    "\n",
    "        # Extract key fields\n",
    "        uuid = dataset_info.get(\"UUID\", \"\")\n",
    "        base_name = clean_text(dataset_info.get(\"name\", {}).get(\"baseName\", [{}])[0].get(\"value\", \"\"), lowercase=False)\n",
    "        classification = \"\"\n",
    "        database_name = \"\"\n",
    "        if classification_info:\n",
    "            classification = clean_text(classification_info[0][\"class\"][0].get(\"value\", \"\"))\n",
    "            database_name = clean_text(classification_info[0].get(\"name\", \"\"))\n",
    "        geographic_location = geography_info.get(\"location\", \"\")\n",
    "        technology_description = \"\"\n",
    "        if \"technologyDescriptionAndIncludedProcesses\" in technology_info:\n",
    "            tech_desc_list = technology_info[\"technologyDescriptionAndIncludedProcesses\"]\n",
    "            if tech_desc_list:\n",
    "                technology_description = clean_text(tech_desc_list[0].get(\"value\", \"\"), lowercase=False)\n",
    "        technological_applicability = \"\"\n",
    "        if \"technologicalApplicability\" in technology_info:\n",
    "            tech_applicability_list = technology_info[\"technologicalApplicability\"]\n",
    "            if tech_applicability_list and isinstance(tech_applicability_list, list) and len(tech_applicability_list) > 0:\n",
    "                technological_applicability = clean_text(tech_applicability_list[0].get(\"value\", \"\"), lowercase=False)\n",
    "        lca_method_details = \"\"\n",
    "        lci_method_allocation = modelling_info.get(\"LCIMethodAndAllocation\", {})\n",
    "        ref_to_lca_method_details = lci_method_allocation.get(\"referenceToLCAMethodDetails\", [])\n",
    "        if ref_to_lca_method_details and isinstance(ref_to_lca_method_details, list) and len(ref_to_lca_method_details) > 0:\n",
    "            short_desc_list = ref_to_lca_method_details[0].get(\"shortDescription\", [])\n",
    "            if short_desc_list and isinstance(short_desc_list, list) and len(short_desc_list) > 0:\n",
    "                lca_method_details = clean_text(short_desc_list[0].get(\"value\", \"\"), lowercase=False)\n",
    "        flow_dataset_short_desc = \"\"\n",
    "        exchanges = exchanges_info.get(\"exchange\", [])\n",
    "        if exchanges and isinstance(exchanges, list) and len(exchanges) > 0:\n",
    "            ref_to_flow_dataset = exchanges[0].get(\"referenceToFlowDataSet\", {})\n",
    "            short_desc_list = ref_to_flow_dataset.get(\"shortDescription\", [])\n",
    "            if short_desc_list and isinstance(short_desc_list, list) and len(short_desc_list) > 0:\n",
    "                flow_dataset_short_desc = clean_text(short_desc_list[0].get(\"value\", \"\"), lowercase=False)\n",
    "        flow_property_name = \"\"\n",
    "        if exchanges and isinstance(exchanges, list) and len(exchanges) > 0:\n",
    "            flow_properties = exchanges[0].get(\"flowProperties\", [])\n",
    "            if flow_properties and isinstance(flow_properties, list) and len(flow_properties) > 0:\n",
    "                name_list = flow_properties[0].get(\"name\", [])\n",
    "                if name_list and isinstance(name_list, list) and len(name_list) > 0:\n",
    "                    flow_property_name = clean_text(name_list[0].get(\"value\", \"\"), lowercase=False)\n",
    "        flow_property_mean_value = \"\"\n",
    "        if exchanges and isinstance(exchanges, list) and len(exchanges) > 0:\n",
    "            flow_properties = exchanges[0].get(\"flowProperties\", [])\n",
    "            if flow_properties and isinstance(flow_properties, list) and len(flow_properties) > 0:\n",
    "                flow_property_mean_value = flow_properties[0].get(\"meanValue\", \"\")\n",
    "        flow_property_reference_unit = \"\"\n",
    "        if exchanges and isinstance(exchanges, list) and len(exchanges) > 0:\n",
    "            flow_properties = exchanges[0].get(\"flowProperties\", [])\n",
    "            if flow_properties and isinstance(flow_properties, list) and len(flow_properties) > 0:\n",
    "                flow_property_reference_unit = flow_properties[0].get(\"referenceUnit\", \"\")\n",
    "        material_properties_cols = {}\n",
    "        if exchanges:\n",
    "            mat_props = exchanges[0].get(\"materialProperties\", [])\n",
    "            for idx, mp in enumerate(mat_props, start=1):\n",
    "                mp_name  = clean_text(mp.get(\"name\", \"\"),  lowercase=False)\n",
    "                mp_value = mp.get(\"value\", \"\")\n",
    "                mp_unit  = mp.get(\"unit\", \"\")\n",
    "                material_properties_cols[f\"Material Property {idx}\"] = f\"{mp_name}: {mp_value} {mp_unit}\".strip()\n",
    "\n",
    "        \n",
    "        # Filter the data based on allowed classifications.\n",
    "        if classification in ALLOWED_CATEGORIES:\n",
    "            record = {\n",
    "                \"EPD Name\": base_name,\n",
    "                \"Technology Description\": technology_description,\n",
    "                \"Classification\": classification,\n",
    "                \"Database Name\": database_name,\n",
    "                \"Technological Applicability\": technological_applicability,\n",
    "                \"LCA Method Details\": lca_method_details,\n",
    "                \"Flow Dataset Short Description\": flow_dataset_short_desc,\n",
    "                \"Flow Property Name\": flow_property_name,\n",
    "                \"Flow Property Mean Value\": flow_property_mean_value,\n",
    "                \"Flow Property Reference Unit\": flow_property_reference_unit,\n",
    "                \"Geographic Location\": geographic_location,\n",
    "                \"UUID\": uuid,\n",
    "            }\n",
    "\n",
    "            # add dynamic material-property columns\n",
    "            record.update(material_properties_cols)\n",
    "            return record\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_epd_data():\n",
    "    \"\"\"Fetches and filters EPD data from SQLite.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(DB_PATH)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Fetch all rows from the table\n",
    "        cursor.execute(\"SELECT document FROM epd_documents\")\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        extracted_data = []\n",
    "        unique_classifications = set()\n",
    "\n",
    "        for row in rows:\n",
    "            try:\n",
    "                json_data = json.loads(row[0])\n",
    "                epd_info = extract_epd_data(json_data)\n",
    "                if epd_info:\n",
    "                    extracted_data.append(epd_info)\n",
    "                    unique_classifications.add(epd_info[\"Classification\"])\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Skipping invalid JSON record.\")\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "        print(\"\\n### Unique Classifications Found ###\")\n",
    "        for cls in sorted(unique_classifications):\n",
    "            print(f\"- {cls}\")\n",
    "\n",
    "        df = pd.DataFrame(extracted_data)\n",
    "        return df\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"SQLite error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run extraction and display results.\n",
    "df_epd = fetch_epd_data()\n",
    "\n",
    "if df_epd is not None and not df_epd.empty:\n",
    "    display(df_epd)\n",
    "    output_dir = os.path.dirname(OUTPUT_FILE_PATH)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    try:\n",
    "        df_epd.to_csv(OUTPUT_FILE_PATH, index=False, encoding=\"utf-8\")\n",
    "        print(f\"Filtered EPD data saved to: {OUTPUT_FILE_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving CSV file: {e}\")\n",
    "else:\n",
    "    print(\"No matching EPDs found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Category Distribuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define file path\n",
    "file_path = \"../../data/pipeline2/sql/filtered_epd_data05.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found: {file_path}\")\n",
    "else:\n",
    "    # Read the CSV file\n",
    "    df_epd = pd.read_csv(file_path)\n",
    "\n",
    "    # Replace missing values in the Classification column with \"Missing\"\n",
    "    df_epd[\"Classification\"] = df_epd[\"Classification\"].fillna(\"Missing\")\n",
    "\n",
    "    # Get classification counts\n",
    "    classification_counts = df_epd[\"Classification\"].value_counts()\n",
    "\n",
    "    # Plot the classification distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = classification_counts.plot(kind=\"bar\", edgecolor=\"black\", color=\"green\")\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Distribution of Problematic EPD Categories (Including Missing)\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Add count labels inside the bars\n",
    "    for p in ax.patches:\n",
    "        # Get bar dimensions\n",
    "        width = p.get_width()\n",
    "        height = p.get_height()\n",
    "        x, y = p.get_xy()\n",
    "        # Place label in the middle of the bar\n",
    "        ax.text(x + width/2, y + height/2, int(height), \n",
    "                horizontalalignment='center', \n",
    "                verticalalignment='center', \n",
    "                color='white', fontsize=12)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Print category counts in text formats\n",
    "    print(\"\\n### Category Counts ###\")\n",
    "    for classification, count in classification_counts.items():\n",
    "        print(f\"{classification}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Character and Token Counts of `technologyDescriptionAndIncludedProcesses`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import statistics\n",
    "import os\n",
    "\n",
    "# File path for the processed EPD data\n",
    "file_path = \"../../data/pipeline2/sql/filtered_epd_data.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found: {file_path}\")\n",
    "else:\n",
    "    # Read the CSV file\n",
    "    df_epd = pd.read_csv(file_path)\n",
    "\n",
    "    # Combine relevant text columns into a single string per row\n",
    "    df_epd[\"tech_descr\"] = df_epd[[\"Technology Description\"]].astype(str).agg(\" \".join, axis=1)\n",
    "\n",
    "    # Compute character and token counts\n",
    "    lengths = df_epd[\"tech_descr\"].apply(len).tolist()\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "    token_counts = [len(enc.encode(text)) for text in df_epd[\"tech_descr\"]]\n",
    "\n",
    "    # Basic statistics\n",
    "    mean_length = statistics.mean(lengths) if lengths else 0\n",
    "    mean_tokens = statistics.mean(token_counts) if token_counts else 0\n",
    "\n",
    "    median_length = np.median(lengths) if lengths else 0\n",
    "    median_tokens = np.median(token_counts) if token_counts else 0\n",
    "\n",
    "    std_length = np.std(lengths, ddof=1) if len(lengths) > 1 else 0\n",
    "    std_tokens = np.std(token_counts, ddof=1) if len(token_counts) > 1 else 0\n",
    "\n",
    "    percentiles_length = np.percentile(lengths, [25, 50, 75, 90, 95]) if lengths else [0] * 5\n",
    "    percentiles_tokens = np.percentile(token_counts, [25, 50, 75, 90, 95]) if token_counts else [0] * 5\n",
    "\n",
    "    iqr_length = percentiles_length[2] - percentiles_length[0]\n",
    "    iqr_tokens = percentiles_tokens[2] - percentiles_tokens[0]\n",
    "\n",
    "    # Print statistical measures\n",
    "    print(f\"Mean character count: {mean_length:.2f}\")\n",
    "    print(f\"Median character count: {median_length:.2f}\")\n",
    "    print(f\"Standard deviation (chars): {std_length:.2f}\")\n",
    "    print(f\"25th, 50th, 75th, 90th, 95th percentiles (chars): {percentiles_length}\")\n",
    "    print(f\"IQR (chars): {iqr_length:.2f}\\n\")\n",
    "\n",
    "    print(f\"Mean token count: {mean_tokens:.2f}\")\n",
    "    print(f\"Median token count: {median_tokens:.2f}\")\n",
    "    print(f\"Standard deviation (tokens): {std_tokens:.2f}\")\n",
    "    print(f\"25th, 50th, 75th, 90th, 95th percentiles (tokens): {percentiles_tokens}\")\n",
    "    print(f\"IQR (tokens): {iqr_tokens:.2f}\\n\")\n",
    "\n",
    "    # Plot histograms for visual inspection\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(lengths, bins=20, edgecolor=\"black\")\n",
    "    plt.title(\"Character Count Distribution\")\n",
    "    plt.xlabel(\"Character Count\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(token_counts, bins=20, edgecolor=\"black\")\n",
    "    plt.title(\"Token Count Distribution\")\n",
    "    plt.xlabel(\"Token Count\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print histogram data in numeric format\n",
    "    def print_histogram_data(data, bins=20, title=\"Histogram\"):\n",
    "        counts, bin_edges = np.histogram(data, bins=bins)\n",
    "        print(title)\n",
    "        for i in range(len(counts)):\n",
    "            lower = bin_edges[i]\n",
    "            upper = bin_edges[i + 1]\n",
    "            print(f\"{lower:5.1f} - {upper:5.1f}: {counts[i]}\")\n",
    "\n",
    "    print_histogram_data(lengths, bins=20, title=\"Character Count Histogram Data\")\n",
    "    print_histogram_data(token_counts, bins=20, title=\"Token Count Histogram Data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch EN Ã–KOBAUDAT Categories from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.parse import quote\n",
    "import xml.dom.minidom\n",
    "\n",
    "# Optimize to not go past level 2\n",
    "# Problems getting all German categories because of umlauts in the URL \n",
    "\n",
    "# Base URL for the API endpoints\n",
    "BASE_URL = \"https://www.oekobaudat.de/OEKOBAU.DAT/resource/processes/categories/\"\n",
    "\n",
    "def get_categories(path):\n",
    "    \"\"\"\n",
    "    Fetches categories from the API.\n",
    "    - If path is empty, gets top-level categories.\n",
    "    - Otherwise, retrieves subcategories for the given path.\n",
    "    \"\"\"\n",
    "    if path:\n",
    "        encoded_path = \"/\".join(quote(segment) for segment in path.split(\"/\"))\n",
    "        url = f\"{BASE_URL}{encoded_path}/subcategories/?catSystem=oekobau.dat&lang=en&sort=id\"\n",
    "    else:\n",
    "        url = f\"{BASE_URL}?catSystem=oekobau.dat&lang=en&sort=id\"\n",
    "    \n",
    "    print(f\"Fetching URL: {url}\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise error for bad responses\n",
    "    return ET.fromstring(response.content)\n",
    "\n",
    "def build_tree(path, xml_parent, level=0):\n",
    "    \"\"\"\n",
    "    Recursively builds the XML tree:\n",
    "    - 'path' is used to construct the API URL.\n",
    "    - 'xml_parent' is the XML element to which child categories will be added.\n",
    "    - 'level' indicates the recursion depth (for debugging).\n",
    "    \"\"\"\n",
    "    indent = \"  \" * level\n",
    "    print(f\"{indent}Processing path: '{path or 'Top-level'}'\")\n",
    "    \n",
    "    try:\n",
    "        root = get_categories(path)\n",
    "    except Exception as e:\n",
    "        print(f\"{indent}Error retrieving categories for path '{path}': {e}\")\n",
    "        return\n",
    "\n",
    "    ns = {\"sapi\": \"http://www.ilcd-network.org/ILCD/ServiceAPI\"}\n",
    "    for cat in root.findall(\".//sapi:category\", ns):\n",
    "        cat_name = cat.text.strip() if cat.text else \"Unknown\"\n",
    "        cat_id = cat.get(\"classId\", \"N/A\")\n",
    "        print(f\"{indent}  Found category: {cat_name} (ID: {cat_id})\")\n",
    "        \n",
    "        new_elem = ET.SubElement(xml_parent, \"category\", id=cat_id, name=cat_name)\n",
    "        new_path = cat_name if not path else f\"{path}/{cat_name}\"\n",
    "        build_tree(new_path, new_elem, level + 1)\n",
    "\n",
    "def main():\n",
    "    # Register namespaces for the output file\n",
    "    ET.register_namespace(\"\", \"http://lca.jrc.it/ILCD/Categories\")\n",
    "    ET.register_namespace(\"common\", \"http://lca.jrc.it/ILCD/Common\")\n",
    "    ET.register_namespace(\"xsi\", \"http://www.w3.org/2001/XMLSchema-instance\")\n",
    "    \n",
    "    # Create the root element <CategorySystem> with attribute name=\"OEKOBAU.DAT\"\n",
    "    root_elem = ET.Element(\"CategorySystem\", {\"name\": \"OEKOBAU.DAT\"})\n",
    "    # Create the <categories> container with dataType=\"Process\"\n",
    "    categories_elem = ET.SubElement(root_elem, \"categories\", {\"dataType\": \"Process\"})\n",
    "    \n",
    "    print(\"Starting category tree traversal...\")\n",
    "    build_tree(\"\", categories_elem)\n",
    "    \n",
    "    # Convert the ElementTree to a string and pretty print it using minidom\n",
    "    xml_str = ET.tostring(root_elem, encoding=\"utf-8\")\n",
    "    parsed = xml.dom.minidom.parseString(xml_str)\n",
    "    pretty_xml_str = parsed.toprettyxml(indent=\"    \", encoding=\"UTF-8\")\n",
    "    \n",
    "    # Write the pretty XML to file\n",
    "    with open(\"../../data/pipeline2/xml/OEKOBAU.DAT_Categories_EN_API.xml\", \"wb\") as f:\n",
    "        f.write(pretty_xml_str)\n",
    "    \n",
    "    print(\"Finished. Categories saved to categories.xml\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
