{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract generic and missing categories from SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from IPython.display import display  # For Jupyter Notebook display\n",
    "\n",
    "# SQLite database file path\n",
    "DB_PATH = \"../../data/pipeline2/sql/epd_database.sqlite\"\n",
    "OUTPUT_FILE_PATH = \"../../data/pipeline2/sql/filtered_epd_data.csv\"\n",
    "\n",
    "# Allowed Classifications (normalized)\n",
    "ALLOWED_CATEGORIES = {\n",
    "    \"construction products, infrastructure and buildings\",\n",
    "    \"construction products\",\n",
    "    \"N/A\"  # Handling missing categories\n",
    "}\n",
    "\n",
    "# Synonym & Unit Normalization Dictionary\n",
    "SYNONYM_DICT = {\n",
    "    \"kilogram\": \"kg\",\n",
    "    \"kilograms\": \"kg\",\n",
    "    \"gram\": \"g\",\n",
    "    \"grams\": \"g\",\n",
    "    \"liter\": \"L\",\n",
    "    \"liters\": \"L\",\n",
    "    \"metre\": \"m\",\n",
    "    \"meter\": \"m\",\n",
    "    \"centimetre\": \"cm\",\n",
    "    \"centimeter\": \"cm\",\n",
    "    \"millimetre\": \"mm\",\n",
    "    \"millimeter\": \"mm\"\n",
    "}\n",
    "\n",
    "def clean_text(text, lowercase=True):\n",
    "    \"\"\"Cleans and normalizes text: removes special characters, extra spaces, and normalizes synonyms & units.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"N/A\"\n",
    "\n",
    "    text = text.strip().replace(\"\\n\", \" \")  # Remove newlines and trim spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove excessive spaces\n",
    "\n",
    "    if lowercase:\n",
    "        text = text.lower()  # Convert to lowercase for consistency\n",
    "    \n",
    "    # Normalize punctuation (replace \"&\" with \"and\")\n",
    "    text = text.replace(\"&\", \"and\")\n",
    "\n",
    "    # Remove special characters (except spaces, common separators)\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,\\-_/()\\s]', '', text)\n",
    "\n",
    "    # Normalize synonyms and units\n",
    "    for key, value in SYNONYM_DICT.items():\n",
    "        text = text.replace(key, value)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Function to extract relevant fields from JSON\n",
    "def extract_epd_data(json_data):\n",
    "    \"\"\"Extracts key fields from the EPD JSON document and applies preprocessing & filters.\"\"\"\n",
    "    try:\n",
    "        process_info = json_data.get(\"processInformation\", {})\n",
    "        dataset_info = process_info.get(\"dataSetInformation\", {})\n",
    "        classification_info = dataset_info.get(\"classificationInformation\", {}).get(\"classification\", [])\n",
    "        technology_info = process_info.get(\"technology\", {})\n",
    "\n",
    "        # Extract base name (product name)\n",
    "        base_name = clean_text(dataset_info.get(\"name\", {}).get(\"baseName\", [{}])[0].get(\"value\", \"N/A\"), lowercase=False)\n",
    "\n",
    "        # Extract classification\n",
    "        classification = \"N/A\"\n",
    "        if classification_info:\n",
    "            classification = clean_text(classification_info[0][\"class\"][0].get(\"value\", \"N/A\"))\n",
    "\n",
    "        # Debugging: Print all unique classifications to check raw values\n",
    "        # print(f\"DEBUG: Found classification -> {classification}\")\n",
    "\n",
    "        # Extract technology description (manufacturing process)\n",
    "        technology_description = \"N/A\"\n",
    "        if \"technologyDescriptionAndIncludedProcesses\" in technology_info:\n",
    "            tech_desc_list = technology_info[\"technologyDescriptionAndIncludedProcesses\"]\n",
    "            if tech_desc_list:\n",
    "                technology_description = clean_text(tech_desc_list[0].get(\"value\", \"N/A\"), lowercase=False)\n",
    "\n",
    "        # **Filtering Conditions**\n",
    "        if classification in ALLOWED_CATEGORIES and technology_description != \"N/A\":\n",
    "            return {\n",
    "                \"Product Name\": base_name,\n",
    "                \"Classification\": classification,\n",
    "                \"Technology Description\": technology_description\n",
    "            }\n",
    "        else:\n",
    "            return None  # Skip EPDs that don't meet the conditions\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Connect to SQLite and retrieve JSON data\n",
    "def fetch_epd_data():\n",
    "    \"\"\"Fetches and filters EPD data from SQLite.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(DB_PATH)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Fetch all rows from the table\n",
    "        cursor.execute(\"SELECT document FROM epd_documents\")\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        extracted_data = []\n",
    "        unique_classifications = set()\n",
    "\n",
    "        for row in rows:\n",
    "            try:\n",
    "                json_data = json.loads(row[0])  # Parse JSON from SQLite\n",
    "                epd_info = extract_epd_data(json_data)\n",
    "                if epd_info:\n",
    "                    extracted_data.append(epd_info)\n",
    "                    unique_classifications.add(epd_info[\"Classification\"])  # Store unique classifications for debugging\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Skipping invalid JSON record.\")\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "        # Print all unique classification values found\n",
    "        print(\"\\n### Unique Classifications Found ###\")\n",
    "        for cls in sorted(unique_classifications):\n",
    "            print(f\"- {cls}\")\n",
    "\n",
    "        # Convert extracted data into a DataFrame\n",
    "        df = pd.DataFrame(extracted_data)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"SQLite error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run the extraction and display results\n",
    "df_epd = fetch_epd_data()\n",
    "\n",
    "# Check if the DataFrame is valid\n",
    "if df_epd is not None and not df_epd.empty:\n",
    "    display(df_epd)  # Show the data in Jupyter Notebook\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    output_dir = os.path.dirname(OUTPUT_FILE_PATH)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save DataFrame to CSV\n",
    "    try:\n",
    "        df_epd.to_csv(OUTPUT_FILE_PATH, index=False, encoding=\"utf-8\")\n",
    "        print(f\"Filtered EPD data saved to: {OUTPUT_FILE_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving CSV file: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"No matching EPDs found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from IPython.display import display  # For Jupyter Notebook display\n",
    "\n",
    "# SQLite database file path\n",
    "DB_PATH = \"../../data/pipeline2/sql/epd_database.sqlite\"\n",
    "OUTPUT_FILE_PATH = \"../../data/pipeline2/sql/filtered_epd_data03.csv\"\n",
    "\n",
    "# Allowed Classifications (normalized)\n",
    "ALLOWED_CATEGORIES = {\n",
    "    \"construction products, infrastructure and buildings\",\n",
    "    \"construction products\",\n",
    "    \"\"  # Handling missing categories\n",
    "}\n",
    "\n",
    "# Synonym & Unit Normalization Dictionary\n",
    "SYNONYM_DICT = {\n",
    "    \"kilogram\": \"kg\",\n",
    "    \"kilograms\": \"kg\",\n",
    "    \"gram\": \"g\",\n",
    "    \"grams\": \"g\",\n",
    "    \"liter\": \"L\",\n",
    "    \"liters\": \"L\",\n",
    "    \"metre\": \"m\",\n",
    "    \"meter\": \"m\",\n",
    "    \"centimetre\": \"cm\",\n",
    "    \"centimeter\": \"cm\",\n",
    "    \"millimetre\": \"mm\",\n",
    "    \"millimeter\": \"mm\"\n",
    "}\n",
    "\n",
    "def clean_text(text, lowercase=True):\n",
    "    \"\"\"Cleans and normalizes text: removes newlines, extra spaces, normalizes punctuation, and standardizes synonyms/units.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"N/A\"\n",
    "\n",
    "    text = text.strip().replace(\"\\n\", \" \")          # Remove newlines and trim spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)                  # Remove excessive spaces\n",
    "    if lowercase:\n",
    "        text = text.lower()                           # Lowercase text if desired\n",
    "    # Normalize punctuation (replace \"&\" with \"and\")\n",
    "    text = text.replace(\"&\", \"and\")\n",
    "    # Remove special characters (except spaces and common separators)\n",
    "    text = re.sub(r'[^a-zA-Z0-9.:,\\-_/()\\s]', '', text)\n",
    "    # Normalize synonyms and units\n",
    "    for key, value in SYNONYM_DICT.items():\n",
    "        text = text.replace(key, value)\n",
    "    return text\n",
    "\n",
    "def extract_epd_data(json_data):\n",
    "    \"\"\"Extracts key fields from the EPD JSON document, including additional fields.\"\"\"\n",
    "    try:\n",
    "        process_info = json_data.get(\"processInformation\", {})\n",
    "        dataset_info = process_info.get(\"dataSetInformation\", {})\n",
    "        classification_info = dataset_info.get(\"classificationInformation\", {}).get(\"classification\", [])\n",
    "        geography_info = process_info.get(\"geography\", {}).get(\"locationOfOperationSupplyOrProduction\", {})\n",
    "        technology_info = process_info.get(\"technology\", {})\n",
    "        modelling_info = json_data.get(\"modellingAndValidation\", {})\n",
    "        exchanges_info = json_data.get(\"exchanges\", {})\n",
    "\n",
    "        # Extract key fields\n",
    "        uuid = dataset_info.get(\"UUID\", \"\")\n",
    "        base_name = clean_text(dataset_info.get(\"name\", {}).get(\"baseName\", [{}])[0].get(\"value\", \"\"), lowercase=False)\n",
    "        classification = \"\"\n",
    "        if classification_info:\n",
    "            classification = clean_text(classification_info[0][\"class\"][0].get(\"value\", \"\"))\n",
    "        geographic_location = geography_info.get(\"location\", \"\")\n",
    "        technology_description = \"\"\n",
    "        if \"technologyDescriptionAndIncludedProcesses\" in technology_info:\n",
    "            tech_desc_list = technology_info[\"technologyDescriptionAndIncludedProcesses\"]\n",
    "            if tech_desc_list:\n",
    "                technology_description = clean_text(tech_desc_list[0].get(\"value\", \"\"), lowercase=False)\n",
    "        technological_applicability = \"\"\n",
    "        if \"technologicalApplicability\" in technology_info:\n",
    "            tech_applicability_list = technology_info[\"technologicalApplicability\"]\n",
    "            if tech_applicability_list and isinstance(tech_applicability_list, list) and len(tech_applicability_list) > 0:\n",
    "                technological_applicability = clean_text(tech_applicability_list[0].get(\"value\", \"\"), lowercase=False)\n",
    "        lca_method_details = \"\"\n",
    "        lci_method_allocation = modelling_info.get(\"LCIMethodAndAllocation\", {})\n",
    "        ref_to_lca_method_details = lci_method_allocation.get(\"referenceToLCAMethodDetails\", [])\n",
    "        if ref_to_lca_method_details and isinstance(ref_to_lca_method_details, list) and len(ref_to_lca_method_details) > 0:\n",
    "            short_desc_list = ref_to_lca_method_details[0].get(\"shortDescription\", [])\n",
    "            if short_desc_list and isinstance(short_desc_list, list) and len(short_desc_list) > 0:\n",
    "                lca_method_details = clean_text(short_desc_list[0].get(\"value\", \"\"), lowercase=False)\n",
    "        flow_dataset_short_desc = \"\"\n",
    "        exchanges = exchanges_info.get(\"exchange\", [])\n",
    "        if exchanges and isinstance(exchanges, list) and len(exchanges) > 0:\n",
    "            ref_to_flow_dataset = exchanges[0].get(\"referenceToFlowDataSet\", {})\n",
    "            short_desc_list = ref_to_flow_dataset.get(\"shortDescription\", [])\n",
    "            if short_desc_list and isinstance(short_desc_list, list) and len(short_desc_list) > 0:\n",
    "                flow_dataset_short_desc = clean_text(short_desc_list[0].get(\"value\", \"\"), lowercase=False)\n",
    "        flow_property_name = \"\"\n",
    "        if exchanges and isinstance(exchanges, list) and len(exchanges) > 0:\n",
    "            flow_properties = exchanges[0].get(\"flowProperties\", [])\n",
    "            if flow_properties and isinstance(flow_properties, list) and len(flow_properties) > 0:\n",
    "                name_list = flow_properties[0].get(\"name\", [])\n",
    "                if name_list and isinstance(name_list, list) and len(name_list) > 0:\n",
    "                    flow_property_name = clean_text(name_list[0].get(\"value\", \"\"), lowercase=False)\n",
    "        flow_property_mean_value = \"\"\n",
    "        if exchanges and isinstance(exchanges, list) and len(exchanges) > 0:\n",
    "            flow_properties = exchanges[0].get(\"flowProperties\", [])\n",
    "            if flow_properties and isinstance(flow_properties, list) and len(flow_properties) > 0:\n",
    "                flow_property_mean_value = flow_properties[0].get(\"meanValue\", \"\")\n",
    "        flow_property_reference_unit = \"\"\n",
    "        if exchanges and isinstance(exchanges, list) and len(exchanges) > 0:\n",
    "            flow_properties = exchanges[0].get(\"flowProperties\", [])\n",
    "            if flow_properties and isinstance(flow_properties, list) and len(flow_properties) > 0:\n",
    "                flow_property_reference_unit = flow_properties[0].get(\"referenceUnit\", \"\")\n",
    "        \n",
    "        # Filter the data based on allowed classifications.\n",
    "        if classification in ALLOWED_CATEGORIES:\n",
    "            return {\n",
    "                \"Product Name\": base_name,\n",
    "                \"Technology Description\": technology_description,\n",
    "                \"Classification\": classification,\n",
    "                \"Technological Applicability\": technological_applicability,\n",
    "                \"LCA Method Details\": lca_method_details,\n",
    "                \"Flow Dataset Short Description\": flow_dataset_short_desc,\n",
    "                \"Flow Property Name\": flow_property_name,\n",
    "                \"Flow Property Mean Value\": flow_property_mean_value,\n",
    "                \"Flow Property Reference Unit\": flow_property_reference_unit,\n",
    "                \"Geographic Location\": geographic_location,\n",
    "                \"UUID\": uuid,\n",
    "            }\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_epd_data():\n",
    "    \"\"\"Fetches and filters EPD data from SQLite.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(DB_PATH)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Fetch all rows from the table\n",
    "        cursor.execute(\"SELECT document FROM epd_documents\")\n",
    "        rows = cursor.fetchall()\n",
    "\n",
    "        extracted_data = []\n",
    "        unique_classifications = set()\n",
    "\n",
    "        for row in rows:\n",
    "            try:\n",
    "                json_data = json.loads(row[0])\n",
    "                epd_info = extract_epd_data(json_data)\n",
    "                if epd_info:\n",
    "                    extracted_data.append(epd_info)\n",
    "                    unique_classifications.add(epd_info[\"Classification\"])\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Skipping invalid JSON record.\")\n",
    "\n",
    "        conn.close()\n",
    "\n",
    "        print(\"\\n### Unique Classifications Found ###\")\n",
    "        for cls in sorted(unique_classifications):\n",
    "            print(f\"- {cls}\")\n",
    "\n",
    "        df = pd.DataFrame(extracted_data)\n",
    "        return df\n",
    "\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"SQLite error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run extraction and display results.\n",
    "df_epd = fetch_epd_data()\n",
    "\n",
    "if df_epd is not None and not df_epd.empty:\n",
    "    display(df_epd)\n",
    "    output_dir = os.path.dirname(OUTPUT_FILE_PATH)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    try:\n",
    "        df_epd.to_csv(OUTPUT_FILE_PATH, index=False, encoding=\"utf-8\")\n",
    "        print(f\"Filtered EPD data saved to: {OUTPUT_FILE_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving CSV file: {e}\")\n",
    "else:\n",
    "    print(\"No matching EPDs found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# File path\n",
    "file_path = \"../../data/pipeline2/sql/filtered_epd_data_test.csv\"\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found: {file_path}\")\n",
    "else:\n",
    "    # Read CSV file\n",
    "    df_epd = pd.read_csv(file_path)\n",
    "\n",
    "    # **STEP 1: Clean Text Fields to Remove Extra Spaces & Special Characters**\n",
    "    def clean_text(text):\n",
    "        \"\"\"Ensure uniform text formatting by removing extra spaces and special characters.\"\"\"\n",
    "        if isinstance(text, str):\n",
    "            text = text.strip()  # Remove leading/trailing spaces\n",
    "            text = text.replace(\"\\n\", \" \")  # Replace newlines with spaces\n",
    "            text = \" \".join(text.split())  # Remove excessive spaces\n",
    "            return text\n",
    "        return \"N/A\"\n",
    "\n",
    "    df_epd = df_epd.map(clean_text)  # Apply cleaning to all text fields\n",
    "\n",
    "    # **STEP 2: Force Left Alignment for ALL Content**\n",
    "    def left_align_column(column, width):\n",
    "        \"\"\"Ensure consistent left alignment by padding each entry to a fixed width.\"\"\"\n",
    "        return column.astype(str).str.ljust(width)\n",
    "\n",
    "    # Define dynamic column widths based on max length in each column\n",
    "    col_widths = {col: max(df_epd[col].astype(str).apply(len).max(), len(col)) + 2 for col in df_epd.columns}\n",
    "\n",
    "    # Apply left-alignment formatting to each column\n",
    "    for col, width in col_widths.items():\n",
    "        df_epd[col] = left_align_column(df_epd[col], width)\n",
    "\n",
    "    # **STEP 3: Print Properly Aligned Table**\n",
    "    print(df_epd.head(20).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Category Distribuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define file path\n",
    "file_path = \"../../data/pipeline2/sql/filtered_epd_data02.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found: {file_path}\")\n",
    "else:\n",
    "    # Read the CSV file\n",
    "    df_epd = pd.read_csv(file_path)\n",
    "\n",
    "    # Replace missing values in the Classification column with \"Missing\"\n",
    "    df_epd[\"Classification\"] = df_epd[\"Classification\"].fillna(\"Missing\")\n",
    "\n",
    "    # Get classification counts\n",
    "    classification_counts = df_epd[\"Classification\"].value_counts()\n",
    "\n",
    "    # Plot the classification distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ax = classification_counts.plot(kind=\"bar\", edgecolor=\"black\", color=\"green\")\n",
    "\n",
    "    # Formatting the plot\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Distribution of Problematic EPD Categories (Including Missing)\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Add count labels inside the bars\n",
    "    for p in ax.patches:\n",
    "        # Get bar dimensions\n",
    "        width = p.get_width()\n",
    "        height = p.get_height()\n",
    "        x, y = p.get_xy()\n",
    "        # Place label in the middle of the bar\n",
    "        ax.text(x + width/2, y + height/2, int(height), \n",
    "                horizontalalignment='center', \n",
    "                verticalalignment='center', \n",
    "                color='white', fontsize=12)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Print category counts in text formats\n",
    "    print(\"\\n### Category Counts ###\")\n",
    "    for classification, count in classification_counts.items():\n",
    "        print(f\"{classification}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Character and Token Counts of `technologyDescriptionAndIncludedProcesses`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import statistics\n",
    "import os\n",
    "\n",
    "# File path for the processed EPD data\n",
    "file_path = \"../../data/pipeline2/sql/filtered_epd_data.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found: {file_path}\")\n",
    "else:\n",
    "    # Read the CSV file\n",
    "    df_epd = pd.read_csv(file_path)\n",
    "\n",
    "    # Combine relevant text columns into a single string per row\n",
    "    df_epd[\"tech_descr\"] = df_epd[[\"Technology Description\"]].astype(str).agg(\" \".join, axis=1)\n",
    "\n",
    "    # Compute character and token counts\n",
    "    lengths = df_epd[\"tech_descr\"].apply(len).tolist()\n",
    "\n",
    "    # Initialize tokenizer\n",
    "    enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "    token_counts = [len(enc.encode(text)) for text in df_epd[\"tech_descr\"]]\n",
    "\n",
    "    # Basic statistics\n",
    "    mean_length = statistics.mean(lengths) if lengths else 0\n",
    "    mean_tokens = statistics.mean(token_counts) if token_counts else 0\n",
    "\n",
    "    median_length = np.median(lengths) if lengths else 0\n",
    "    median_tokens = np.median(token_counts) if token_counts else 0\n",
    "\n",
    "    std_length = np.std(lengths, ddof=1) if len(lengths) > 1 else 0\n",
    "    std_tokens = np.std(token_counts, ddof=1) if len(token_counts) > 1 else 0\n",
    "\n",
    "    percentiles_length = np.percentile(lengths, [25, 50, 75, 90, 95]) if lengths else [0] * 5\n",
    "    percentiles_tokens = np.percentile(token_counts, [25, 50, 75, 90, 95]) if token_counts else [0] * 5\n",
    "\n",
    "    iqr_length = percentiles_length[2] - percentiles_length[0]\n",
    "    iqr_tokens = percentiles_tokens[2] - percentiles_tokens[0]\n",
    "\n",
    "    # Print statistical measures\n",
    "    print(f\"Mean character count: {mean_length:.2f}\")\n",
    "    print(f\"Median character count: {median_length:.2f}\")\n",
    "    print(f\"Standard deviation (chars): {std_length:.2f}\")\n",
    "    print(f\"25th, 50th, 75th, 90th, 95th percentiles (chars): {percentiles_length}\")\n",
    "    print(f\"IQR (chars): {iqr_length:.2f}\\n\")\n",
    "\n",
    "    print(f\"Mean token count: {mean_tokens:.2f}\")\n",
    "    print(f\"Median token count: {median_tokens:.2f}\")\n",
    "    print(f\"Standard deviation (tokens): {std_tokens:.2f}\")\n",
    "    print(f\"25th, 50th, 75th, 90th, 95th percentiles (tokens): {percentiles_tokens}\")\n",
    "    print(f\"IQR (tokens): {iqr_tokens:.2f}\\n\")\n",
    "\n",
    "    # Plot histograms for visual inspection\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(lengths, bins=20, edgecolor=\"black\")\n",
    "    plt.title(\"Character Count Distribution\")\n",
    "    plt.xlabel(\"Character Count\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(token_counts, bins=20, edgecolor=\"black\")\n",
    "    plt.title(\"Token Count Distribution\")\n",
    "    plt.xlabel(\"Token Count\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print histogram data in numeric format\n",
    "    def print_histogram_data(data, bins=20, title=\"Histogram\"):\n",
    "        counts, bin_edges = np.histogram(data, bins=bins)\n",
    "        print(title)\n",
    "        for i in range(len(counts)):\n",
    "            lower = bin_edges[i]\n",
    "            upper = bin_edges[i + 1]\n",
    "            print(f\"{lower:5.1f} - {upper:5.1f}: {counts[i]}\")\n",
    "\n",
    "    print_histogram_data(lengths, bins=20, title=\"Character Count Histogram Data\")\n",
    "    print_histogram_data(token_counts, bins=20, title=\"Token Count Histogram Data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize `technologyDescriptionAndIncludedProcesses`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama  # Use your Ollama integration\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a JSON schema for the summarization response.\n",
    "json_schema = {\n",
    "    \"title\": \"SummarizationResponse\",\n",
    "    \"description\": \"Response containing a summarized version of the technology description.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A concise summary of the technology description emphasizing key raw materials and its use in construction.\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"summary\"]\n",
    "}\n",
    "\n",
    "# Create a prompt template that only uses the Technology Description.\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert in summarizing technical manufacturing processes for building material products.\n",
    "Summarize the following technology description into a concise paragraph that emphasizes:\n",
    "- The key raw materials (e.g., PMMA, titanium dioxide, aluminum hydroxide, etc.)\n",
    "Avoid excessive process details; focus on the material composition and intended use.\n",
    "\n",
    "Technology Description:\n",
    "{text}\n",
    "\n",
    "Return your answer in JSON format following this schema:\n",
    "{json_schema}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Set your Ollama model name (adjust as needed for your environment)\n",
    "llm_model = \"deepseek-r1:8b\"  # e.g., \"deepseek-r1:8b\"\n",
    "model = ChatOllama(model=llm_model)\n",
    "\n",
    "def summarize_technology_description(text):\n",
    "    \"\"\"Uses Ollama to summarize a long Technology Description.\"\"\"\n",
    "    final_prompt = prompt_template.format_prompt(\n",
    "        text=text, json_schema=json.dumps(json_schema)\n",
    "    ).to_string()\n",
    "    \n",
    "    structured_llm = model.with_structured_output(\n",
    "        json_schema, method=\"json_schema\", include_raw=True\n",
    "    )\n",
    "    raw_response = structured_llm.invoke(final_prompt)\n",
    "    structured_response = raw_response.get(\"parsed\", None)\n",
    "    if structured_response and \"summary\" in structured_response:\n",
    "        return structured_response[\"summary\"]\n",
    "    return None\n",
    "\n",
    "# Path to the CSV file.\n",
    "csv_path = \"../../data/pipeline2/sql/filtered_epd_data.csv\"\n",
    "\n",
    "number_entries = 100\n",
    "\n",
    "# Read the CSV file and select only the first 100 entries.\n",
    "df = pd.read_csv(csv_path).head(number_entries)\n",
    "\n",
    "# Container to hold all responses.\n",
    "summaries = []\n",
    "\n",
    "# Process each row.\n",
    "for idx, row in df.iterrows():\n",
    "    tech_description = row.get(\"Technology Description\", \"\")\n",
    "    if tech_description and isinstance(tech_description, str):\n",
    "        summary = summarize_technology_description(tech_description)\n",
    "        # Create a dictionary for this row, including a row index for reference.\n",
    "        summaries.append({\n",
    "            \"row_index\": idx,\n",
    "            \"Product Name\": row.get(\"Product Name\", \"\"),\n",
    "            \"Classification\": row.get(\"Classification\", \"\"),\n",
    "            \"Technology Description Summary\": summary\n",
    "        })\n",
    "        print(f\"Summary for row {idx}:\\n{summary}\\n{'-'*50}\\n\")\n",
    "    else:\n",
    "        print(f\"Row {idx} has no Technology Description.\")\n",
    "\n",
    "# Ensure the output directory exists.\n",
    "output_dir = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f\"{number_entries}_technology_summaries.json\")\n",
    "\n",
    "# Write the JSON responses to file.\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summaries, f, indent=2)\n",
    "\n",
    "print(f\"All summaries saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama  # Use your Ollama integration\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a JSON schema for the summarization response.\n",
    "json_schema = {\n",
    "    \"title\": \"SummarizationResponse\",\n",
    "    \"description\": \"Response containing a summarized version of the technology description.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A concise summary of the technology description emphasizing key raw materials and its use in construction.\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"summary\"]\n",
    "}\n",
    "\n",
    "# Create a prompt template that only uses the Technology Description.\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert in summarizing technical documents of building material products to facilitate categorization.\n",
    "Summarize the following technology description into a concise paragraph emphasizing key raw materials and its use in construction.\n",
    "If no materials are mentioned, avoid excessive process details.\n",
    "\n",
    "Technology Description:\n",
    "{text}\n",
    "\n",
    "Return your answer in JSON format following this schema:\n",
    "{json_schema}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Set your Ollama model name (adjust as needed for your environment)\n",
    "llm_model = \"deepseek-r1:8b\"  # e.g., \"deepseek-r1:8b\"\n",
    "model = ChatOllama(model=llm_model)\n",
    "\n",
    "def summarize_technology_description(text):\n",
    "    \"\"\"Uses Ollama to summarize a long Technology Description.\"\"\"\n",
    "    final_prompt = prompt_template.format_prompt(\n",
    "        text=text, json_schema=json.dumps(json_schema)\n",
    "    ).to_string()\n",
    "    \n",
    "    structured_llm = model.with_structured_output(\n",
    "        json_schema, method=\"json_schema\", include_raw=True\n",
    "    )\n",
    "    raw_response = structured_llm.invoke(final_prompt)\n",
    "    structured_response = raw_response.get(\"parsed\", None)\n",
    "    if structured_response and \"summary\" in structured_response:\n",
    "        return structured_response[\"summary\"]\n",
    "    return None\n",
    "\n",
    "# Path to the CSV file.\n",
    "csv_path = \"../../data/pipeline2/sql/filtered_epd_data.csv\"\n",
    "\n",
    "number_entries = 100\n",
    "\n",
    "# Read the CSV file and select only the first 100 entries.\n",
    "df = pd.read_csv(csv_path).head(number_entries)\n",
    "\n",
    "# Container to hold all responses.\n",
    "summaries = []\n",
    "\n",
    "# Process each row.\n",
    "for idx, row in df.iterrows():\n",
    "    tech_description = row.get(\"Technology Description\", \"\")\n",
    "    if tech_description and isinstance(tech_description, str):\n",
    "        summary = summarize_technology_description(tech_description)\n",
    "        # Create a dictionary for this row, including a row index for reference.\n",
    "        summaries.append({\n",
    "            \"row_index\": idx,\n",
    "            \"Product Name\": row.get(\"Product Name\", \"\"),\n",
    "            \"Classification\": row.get(\"Classification\", \"\"),\n",
    "            \"Technology Description Summary\": summary\n",
    "        })\n",
    "        print(f\"Summary for row {idx}:\\n{summary}\\n{'-'*50}\\n\")\n",
    "    else:\n",
    "        print(f\"Row {idx} has no Technology Description.\")\n",
    "\n",
    "# Ensure the output directory exists.\n",
    "output_dir = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f\"{number_entries}_tech_sum_one_paragraph.json\")\n",
    "\n",
    "# Write the JSON responses to file.\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summaries, f, indent=2)\n",
    "\n",
    "print(f\"All summaries saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Composition Materials from `technologyDescriptionAndIncludedProcesses`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama  # Use your Ollama integration\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a JSON schema for the composition extraction response.\n",
    "json_schema = {\n",
    "    \"title\": \"CompositionExtractionResponse\",\n",
    "    \"description\": \"Response containing a list of key composition materials extracted from the technology description.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"composition\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A list of key raw materials mentioned in the technology description, e.g., PMMA, titanium dioxide, aluminum hydroxide.\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"composition\"]\n",
    "}\n",
    "\n",
    "# Create a prompt template that focuses on extracting composition materials from the Technology Description.\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert in technical manufacturing processes for building material products.\n",
    "Extract and list the key composition materials from the following technology description. \n",
    "Focus exclusively on the raw materials (e.g., PMMA, titanium dioxide, aluminum hydroxide, etc.) used in the product.\n",
    "Avoid including unnecessary process details; only list the materials.\n",
    "\n",
    "Technology Description:\n",
    "{text}\n",
    "\n",
    "Return your answer in JSON format following this schema:\n",
    "{json_schema}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Set your Ollama model name (adjust as needed for your environment)\n",
    "llm_model = \"deepseek-r1:8b\"  # for example, \"deepseek-r1:8b\"\n",
    "model = ChatOllama(model=llm_model)\n",
    "\n",
    "def extract_composition_materials(text):\n",
    "    \"\"\"Uses Ollama to extract the composition materials from a Technology Description.\"\"\"\n",
    "    final_prompt = prompt_template.format_prompt(\n",
    "        text=text, json_schema=json.dumps(json_schema)\n",
    "    ).to_string()\n",
    "    \n",
    "    structured_llm = model.with_structured_output(\n",
    "        json_schema, method=\"json_schema\", include_raw=True\n",
    "    )\n",
    "    raw_response = structured_llm.invoke(final_prompt)\n",
    "    structured_response = raw_response.get(\"parsed\", None)\n",
    "    if structured_response and \"composition\" in structured_response:\n",
    "        return structured_response[\"composition\"]\n",
    "    return None\n",
    "\n",
    "# Path to the CSV file.\n",
    "csv_path = \"../../data/pipeline2/sql/filtered_epd_data.csv\"\n",
    "\n",
    "number_entries = 100\n",
    "\n",
    "# Read the CSV file and select only the first 100 entries.\n",
    "df = pd.read_csv(csv_path).head(number_entries)\n",
    "\n",
    "# Container to hold all responses.\n",
    "composition_extractions = []\n",
    "\n",
    "# Process each row.\n",
    "for idx, row in df.iterrows():\n",
    "    tech_description = row.get(\"Technology Description\", \"\")\n",
    "    if tech_description and isinstance(tech_description, str):\n",
    "        composition = extract_composition_materials(tech_description)\n",
    "        # Create a dictionary for this row, including a row index for reference.\n",
    "        composition_extractions.append({\n",
    "            \"row_index\": idx,\n",
    "            \"Product Name\": row.get(\"Product Name\", \"\"),\n",
    "            \"Classification\": row.get(\"Classification\", \"\"),\n",
    "            \"Extracted Composition\": composition\n",
    "        })\n",
    "        print(f\"Extracted composition for row {idx}:\\n{composition}\\n{'-'*50}\\n\")\n",
    "    else:\n",
    "        print(f\"Row {idx} has no Technology Description.\\n{'-'*50}\\n\")\n",
    "\n",
    "# Ensure the output directory exists.\n",
    "output_dir = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f\"{number_entries}_technology_compositions.json\")\n",
    "\n",
    "# Write the JSON responses to file.\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(composition_extractions, f, indent=2)\n",
    "\n",
    "print(f\"All composition extractions saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Both Summarization and Composition Material Extraction from `technologyDescriptionAndIncludedProcesses`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama  # Use your Ollama integration\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a JSON schema for the combined summarization and composition extraction response.\n",
    "json_schema = {\n",
    "    \"title\": \"SummarizationAndCompositionResponse\",\n",
    "    \"description\": \"Response containing a summarized version of the technology description and a list of key composition materials.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A concise summary of the technology description emphasizing key raw materials and its use in construction.\"\n",
    "        },\n",
    "        \"composition\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A list of key raw materials mentioned in the technology description, e.g., PMMA, titanium dioxide, aluminum hydroxide.\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"summary\", \"composition\"]\n",
    "}\n",
    "\n",
    "# Create a prompt template that instructs the model to provide both outputs.\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert in technical manufacturing processes for building material products.\n",
    "Given the following technology description, perform two tasks:\n",
    "1. Summarize the description into a concise paragraph that emphasizes the key raw materials.\n",
    "2. Extract and list the key composition materials mentioned (e.g., PMMA, titanium dioxide, aluminum hydroxide).\n",
    "\n",
    "Avoid excessive process details; focus on material composition and intended use.\n",
    "\n",
    "Technology Description:\n",
    "{text}\n",
    "\n",
    "Return your answer in JSON format following this schema:\n",
    "{json_schema}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Set your Ollama model name (this is dynamic so you can test different models)\n",
    "llm_model = \"deepseek-r1:8b\"  # For example, \"deepseek-r1:8b\", \"falcon3:7b-instruct-q4_K_M\",\n",
    "model = ChatOllama(model=llm_model)\n",
    "\n",
    "def process_technology_description(text):\n",
    "    \"\"\"Uses Ollama to get both a summary and the extracted composition materials from a Technology Description.\"\"\"\n",
    "    final_prompt = prompt_template.format_prompt(\n",
    "        text=text, json_schema=json.dumps(json_schema)\n",
    "    ).to_string()\n",
    "\n",
    "    structured_llm = model.with_structured_output(\n",
    "        json_schema, method=\"json_schema\", include_raw=True\n",
    "    )\n",
    "    raw_response = structured_llm.invoke(final_prompt)\n",
    "    structured_response = raw_response.get(\"parsed\", None)\n",
    "    if structured_response and \"summary\" in structured_response and \"composition\" in structured_response:\n",
    "        return structured_response\n",
    "    return {\"summary\": None, \"composition\": None}\n",
    "\n",
    "# Path to the CSV file.\n",
    "csv_path = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"sql\", \"filtered_epd_data.csv\")\n",
    "\n",
    "number_entries = 100\n",
    "\n",
    "# Read the CSV file and select only the first 100 entries.\n",
    "df = pd.read_csv(csv_path).head(number_entries)\n",
    "\n",
    "# Container to hold all responses.\n",
    "results = []\n",
    "\n",
    "# Process each row.\n",
    "for idx, row in df.iterrows():\n",
    "    tech_description = row.get(\"Technology Description\", \"\")\n",
    "    if tech_description and isinstance(tech_description, str):\n",
    "        response = process_technology_description(tech_description)\n",
    "        results.append({\n",
    "            \"row_index\": idx,\n",
    "            \"Product Name\": row.get(\"Product Name\", \"\"),\n",
    "            \"Classification\": row.get(\"Classification\", \"\"),\n",
    "            \"Technology Description Summary\": response.get(\"summary\"),\n",
    "            \"Extracted Composition\": response.get(\"composition\")\n",
    "        })\n",
    "        print(f\"Summary and material composition for row {idx}:\\n{response.get(\"summary\")}\\n{response.get(\"composition\")}\\n{'-'*50}\\n\")\n",
    "    else:\n",
    "        print(f\"Row {idx} has no Technology Description.\")\n",
    "\n",
    "# Ensure the output directory exists.\n",
    "output_dir = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Include the model name in the output file name.\n",
    "model_name = llm_model.split(\":\")[0]\n",
    "output_file = os.path.join(output_dir, f\"{number_entries}_technology_summary_composition_{model_name}.json\")\n",
    "\n",
    "# Write the JSON responses to file.\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"All responses saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize starting with Product Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama  # Use your Ollama integration\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a JSON schema for the summarization response (only a summary field).\n",
    "json_schema = {\n",
    "    \"title\": \"TechnologySummaryResponse\",\n",
    "    \"description\": \"Response containing a concise technology summary that begins with the product name.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A concise summary of the technology description that starts with the product name.\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"summary\"]\n",
    "}\n",
    "\n",
    "# Create a prompt template that uses both the Product Name and Technology Description.\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert in technical manufacturing processes for building material products.\n",
    "Using the information provided, produce a concise technology summary that begins with the product name.\n",
    "The summary should start with \"The product {product_name}\" and then describe the manufacturing process and key raw materials.\n",
    "\n",
    "Technology Description: {text}\n",
    "\n",
    "Return your answer in JSON format following this schema:\n",
    "{json_schema}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Set your Ollama model name (this can be changed dynamically)\n",
    "llm_model = \"deepseek-r1:8b\"  # e.g., \"deepseek-r1:8b\" or another model name\n",
    "model = ChatOllama(model=llm_model)\n",
    "\n",
    "def summarize_technology_description(product_name, text):\n",
    "    \"\"\"Uses Ollama to generate a summary for the technology description that starts with the product name.\"\"\"\n",
    "    final_prompt = prompt_template.format_prompt(\n",
    "        product_name=product_name,\n",
    "        text=text,\n",
    "        json_schema=json.dumps(json_schema)\n",
    "    ).to_string()\n",
    "    \n",
    "    structured_llm = model.with_structured_output(\n",
    "        json_schema, method=\"json_schema\", include_raw=True\n",
    "    )\n",
    "    raw_response = structured_llm.invoke(final_prompt)\n",
    "    structured_response = raw_response.get(\"parsed\", None)\n",
    "    if structured_response and \"summary\" in structured_response:\n",
    "        return structured_response[\"summary\"]\n",
    "    return None\n",
    "\n",
    "# Path to the CSV file.\n",
    "csv_path = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"sql\", \"filtered_epd_data.csv\")\n",
    "number_entries = 100\n",
    "\n",
    "# Read the CSV file and select only the first 100 entries.\n",
    "df = pd.read_csv(csv_path).head(number_entries)\n",
    "\n",
    "# Container to hold all responses.\n",
    "results = []\n",
    "\n",
    "# Process each row.\n",
    "for idx, row in df.iterrows():\n",
    "    product_name = row.get(\"Product Name\", \"\")\n",
    "    tech_description = row.get(\"Technology Description\", \"\")\n",
    "    if product_name and tech_description and isinstance(tech_description, str):\n",
    "        summary = summarize_technology_description(product_name, tech_description)\n",
    "        results.append({\n",
    "            \"row_index\": idx,\n",
    "            \"Product Name\": product_name,\n",
    "            \"Classification\": row.get(\"Classification\", \"\"),\n",
    "            \"Technology Description Summary\": summary\n",
    "        })\n",
    "        print(f\"Processed row {idx}\\n{product_name}\\n\\n{summary}\\n{'-'*50}\\n\")\n",
    "    else:\n",
    "        print(f\"Row {idx} missing product name or technology description.\")\n",
    "\n",
    "# Ensure the output directory exists.\n",
    "output_dir = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Include the model name in the output file name.\n",
    "model_name = llm_model.split(\":\")[0]\n",
    "output_file = os.path.join(output_dir, f\"{number_entries}_tech_sum_prodname_{model_name}.json\")\n",
    "\n",
    "# Write the JSON responses to file.\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"All summaries saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama  # Use your Ollama integration\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a JSON schema for the summarization response (only a summary field).\n",
    "json_schema = {\n",
    "    \"title\": \"TechnologySummaryResponse\",\n",
    "    \"description\": \"Response containing a concise technology summary that begins with the product name.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A concise summary of the technology description that starts with the product name.\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"summary\"]\n",
    "}\n",
    "\n",
    "# Create a prompt template that uses both the Product Name and Technology Description.\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert in technical manufacturing processes for building material products.\n",
    "Using the information provided, produce a one sentence summary that begins with the product name.\n",
    "The summary should start with \"The product {product_name}\" and then describe concisely the product.\n",
    "\n",
    "Technology Description: {text}\n",
    "\n",
    "Return your answer in JSON format following this schema:\n",
    "{json_schema}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Set your Ollama model name (this can be changed dynamically)\n",
    "llm_model = \"deepseek-r1:8b\"  # e.g., \"deepseek-r1:8b\" or another model name\n",
    "model = ChatOllama(model=llm_model)\n",
    "\n",
    "def summarize_technology_description(product_name, text):\n",
    "    \"\"\"Uses Ollama to generate a summary for the technology description that starts with the product name.\"\"\"\n",
    "    final_prompt = prompt_template.format_prompt(\n",
    "        product_name=product_name,\n",
    "        text=text,\n",
    "        json_schema=json.dumps(json_schema)\n",
    "    ).to_string()\n",
    "    \n",
    "    structured_llm = model.with_structured_output(\n",
    "        json_schema, method=\"json_schema\", include_raw=True\n",
    "    )\n",
    "    raw_response = structured_llm.invoke(final_prompt)\n",
    "    structured_response = raw_response.get(\"parsed\", None)\n",
    "    if structured_response and \"summary\" in structured_response:\n",
    "        return structured_response[\"summary\"]\n",
    "    return None\n",
    "\n",
    "# Path to the CSV file.\n",
    "csv_path = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"sql\", \"filtered_epd_data.csv\")\n",
    "number_entries = 100\n",
    "\n",
    "# Read the CSV file and select only the first 100 entries.\n",
    "df = pd.read_csv(csv_path).head(number_entries)\n",
    "\n",
    "# Container to hold all responses.\n",
    "results = []\n",
    "\n",
    "# Process each row.\n",
    "for idx, row in df.iterrows():\n",
    "    product_name = row.get(\"Product Name\", \"\")\n",
    "    tech_description = row.get(\"Technology Description\", \"\")\n",
    "    if product_name and tech_description and isinstance(tech_description, str):\n",
    "        summary = summarize_technology_description(product_name, tech_description)\n",
    "        results.append({\n",
    "            \"row_index\": idx,\n",
    "            \"Product Name\": product_name,\n",
    "            \"Classification\": row.get(\"Classification\", \"\"),\n",
    "            \"Technology Description Summary\": summary\n",
    "        })\n",
    "        print(f\"Processed row {idx}\\n{product_name}\\n\\n{summary}\\n{'-'*50}\\n\")\n",
    "    else:\n",
    "        print(f\"Row {idx} missing product name or technology description.\")\n",
    "\n",
    "# Ensure the output directory exists.\n",
    "output_dir = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Include the model name in the output file name.\n",
    "model_name = llm_model.split(\":\")[0]\n",
    "output_file = os.path.join(output_dir, f\"{number_entries}_tech_sum_prodname_{model_name}.json\")\n",
    "\n",
    "# Write the JSON responses to file.\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"All summaries saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Technology Descriptions in one Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama  # Use your Ollama integration\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a JSON schema for the summarization response.\n",
    "json_schema = {\n",
    "    \"title\": \"SummarizationResponse\",\n",
    "    \"description\": \"Response containing a summarized version of the technology description.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A concise summary of the technology description emphasizing key raw materials and its use in construction.\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"summary\"]\n",
    "}\n",
    "\n",
    "# Create a prompt template that only uses the Technology Description.\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert in summarizing technical documents of building material products.\n",
    "Summarize the following technology description into a concise sentence that facilitates the categorization of the building product.\n",
    "\n",
    "Technology Description:\n",
    "{text}\n",
    "\n",
    "Return your answer in JSON format following this schema:\n",
    "{json_schema}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Set your Ollama model name (adjust as needed for your environment)\n",
    "llm_model = \"deepseek-r1:8b\"  # e.g., \"deepseek-r1:8b\"\n",
    "model = ChatOllama(model=llm_model)\n",
    "\n",
    "def summarize_technology_description(text):\n",
    "    \"\"\"Uses Ollama to summarize a long Technology Description.\"\"\"\n",
    "    final_prompt = prompt_template.format_prompt(\n",
    "        text=text, json_schema=json.dumps(json_schema)\n",
    "    ).to_string()\n",
    "    \n",
    "    structured_llm = model.with_structured_output(\n",
    "        json_schema, method=\"json_schema\", include_raw=True\n",
    "    )\n",
    "    raw_response = structured_llm.invoke(final_prompt)\n",
    "    structured_response = raw_response.get(\"parsed\", None)\n",
    "    if structured_response and \"summary\" in structured_response:\n",
    "        return structured_response[\"summary\"]\n",
    "    return None\n",
    "\n",
    "# Path to the CSV file.\n",
    "csv_path = \"../../data/pipeline2/sql/filtered_epd_data.csv\"\n",
    "\n",
    "number_entries = 100\n",
    "\n",
    "# Read the CSV file and select only the first 100 entries.\n",
    "df = pd.read_csv(csv_path).head(number_entries)\n",
    "\n",
    "# Container to hold all responses.\n",
    "summaries = []\n",
    "\n",
    "# Process each row.\n",
    "for idx, row in df.iterrows():\n",
    "    tech_description = row.get(\"Technology Description\", \"\")\n",
    "    if tech_description and isinstance(tech_description, str):\n",
    "        summary = summarize_technology_description(tech_description)\n",
    "        # Create a dictionary for this row, including a row index for reference.\n",
    "        summaries.append({\n",
    "            \"row_index\": idx,\n",
    "            \"Product Name\": row.get(\"Product Name\", \"\"),\n",
    "            \"Classification\": row.get(\"Classification\", \"\"),\n",
    "            \"Technology Description Summary\": summary\n",
    "        })\n",
    "        print(f\"Summary for row {idx}:\\n{summary}\\n{'-'*50}\\n\")\n",
    "    else:\n",
    "        print(f\"Row {idx} has no Technology Description.\")\n",
    "\n",
    "# Ensure the output directory exists.\n",
    "output_dir = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f\"{number_entries}_tech_sum_one_sentence.json\")\n",
    "\n",
    "# Write the JSON responses to file.\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summaries, f, indent=2)\n",
    "\n",
    "print(f\"All summaries saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama  # Use your Ollama integration\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define a JSON schema for the summarization response.\n",
    "json_schema = {\n",
    "    \"title\": \"SummarizationResponse\",\n",
    "    \"description\": \"Response containing a summarized version of the technology description.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"A concise summary of the technology description emphasizing key raw materials and its use in construction.\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"summary\"]\n",
    "}\n",
    "\n",
    "# Create a prompt template that only uses the Technology Description.\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert in summarizing technical documents of building material products to facilitate categorization.\n",
    "Summarize the following technology description into a concise sentence emphasizing key raw materials and its use in construction.\n",
    "\n",
    "Technology Description:\n",
    "{text}\n",
    "\n",
    "Return your answer in JSON format following this schema:\n",
    "{json_schema}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Set your Ollama model name (adjust as needed for your environment)\n",
    "llm_model = \"deepseek-r1:8b\"  # e.g., \"deepseek-r1:8b\"\n",
    "model = ChatOllama(model=llm_model)\n",
    "\n",
    "def summarize_technology_description(text):\n",
    "    \"\"\"Uses Ollama to summarize a long Technology Description.\"\"\"\n",
    "    final_prompt = prompt_template.format_prompt(\n",
    "        text=text, json_schema=json.dumps(json_schema)\n",
    "    ).to_string()\n",
    "    \n",
    "    structured_llm = model.with_structured_output(\n",
    "        json_schema, method=\"json_schema\", include_raw=True\n",
    "    )\n",
    "    raw_response = structured_llm.invoke(final_prompt)\n",
    "    structured_response = raw_response.get(\"parsed\", None)\n",
    "    if structured_response and \"summary\" in structured_response:\n",
    "        return structured_response[\"summary\"]\n",
    "    return None\n",
    "\n",
    "# Path to the CSV file.\n",
    "csv_path = \"../../data/pipeline2/sql/filtered_epd_data.csv\"\n",
    "\n",
    "number_entries = 100\n",
    "\n",
    "# Read the CSV file and select only the first 100 entries.\n",
    "df = pd.read_csv(csv_path).head(number_entries)\n",
    "\n",
    "# Container to hold all responses.\n",
    "summaries = []\n",
    "\n",
    "# Process each row.\n",
    "for idx, row in df.iterrows():\n",
    "    tech_description = row.get(\"Technology Description\", \"\")\n",
    "    if tech_description and isinstance(tech_description, str):\n",
    "        summary = summarize_technology_description(tech_description)\n",
    "        # Create a dictionary for this row, including a row index for reference.\n",
    "        summaries.append({\n",
    "            \"row_index\": idx,\n",
    "            \"Product Name\": row.get(\"Product Name\", \"\"),\n",
    "            \"Classification\": row.get(\"Classification\", \"\"),\n",
    "            \"Technology Description Summary\": summary\n",
    "        })\n",
    "        print(f\"Summary for row {idx}:\\n{summary}\\n{'-'*50}\\n\")\n",
    "    else:\n",
    "        print(f\"Row {idx} has no Technology Description.\")\n",
    "\n",
    "# Ensure the output directory exists.\n",
    "output_dir = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f\"{number_entries}_tech_sum_one_sentence02.json\")\n",
    "\n",
    "# Write the JSON responses to file.\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summaries, f, indent=2)\n",
    "\n",
    "print(f\"All summaries saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Character and Token Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import statistics\n",
    "import os\n",
    "\n",
    "# File path for the JSON summaries\n",
    "json_file_path = os.path.join(\"..\", \"..\", \"data\", \"pipeline2\", \"json\", \"100_technology_summaries.json\")\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    print(f\"File not found: {json_file_path}\")\n",
    "else:\n",
    "    # Load JSON data\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        summaries = json.load(f)\n",
    "    \n",
    "    # Convert JSON list to a DataFrame\n",
    "    df = pd.DataFrame(summaries)\n",
    "    \n",
    "    # We assume that the summarized text is under the key \"Technology Description Summary\"\n",
    "    # If your key is different (e.g., \"summary\"), adjust accordingly.\n",
    "    if \"Technology Description Summary\" not in df.columns:\n",
    "        print(\"Key 'Technology Description Summary' not found in JSON data.\")\n",
    "    else:\n",
    "        # Compute the character count of each summary.\n",
    "        df[\"char_count\"] = df[\"Technology Description Summary\"].astype(str).apply(len)\n",
    "        \n",
    "        # Initialize the tokenizer.\n",
    "        enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "        df[\"token_count\"] = df[\"Technology Description Summary\"].astype(str).apply(lambda text: len(enc.encode(text)))\n",
    "        \n",
    "        # Collect counts into lists.\n",
    "        lengths = df[\"char_count\"].tolist()\n",
    "        token_counts = df[\"token_count\"].tolist()\n",
    "        \n",
    "        # Calculate basic statistics.\n",
    "        mean_length = statistics.mean(lengths) if lengths else 0\n",
    "        mean_tokens = statistics.mean(token_counts) if token_counts else 0\n",
    "        median_length = np.median(lengths) if lengths else 0\n",
    "        median_tokens = np.median(token_counts) if token_counts else 0\n",
    "        std_length = np.std(lengths, ddof=1) if len(lengths) > 1 else 0\n",
    "        std_tokens = np.std(token_counts, ddof=1) if len(token_counts) > 1 else 0\n",
    "        percentiles_length = np.percentile(lengths, [25, 50, 75, 90, 95]) if lengths else [0] * 5\n",
    "        percentiles_tokens = np.percentile(token_counts, [25, 50, 75, 90, 95]) if token_counts else [0] * 5\n",
    "        iqr_length = percentiles_length[2] - percentiles_length[0]\n",
    "        iqr_tokens = percentiles_tokens[2] - percentiles_tokens[0]\n",
    "        \n",
    "        # Print statistical measures.\n",
    "        print(f\"Mean character count: {mean_length:.2f}\")\n",
    "        print(f\"Median character count: {median_length:.2f}\")\n",
    "        print(f\"Standard deviation (chars): {std_length:.2f}\")\n",
    "        print(f\"25th, 50th, 75th, 90th, 95th percentiles (chars): {percentiles_length}\")\n",
    "        print(f\"IQR (chars): {iqr_length:.2f}\\n\")\n",
    "        \n",
    "        print(f\"Mean token count: {mean_tokens:.2f}\")\n",
    "        print(f\"Median token count: {median_tokens:.2f}\")\n",
    "        print(f\"Standard deviation (tokens): {std_tokens:.2f}\")\n",
    "        print(f\"25th, 50th, 75th, 90th, 95th percentiles (tokens): {percentiles_tokens}\")\n",
    "        print(f\"IQR (tokens): {iqr_tokens:.2f}\\n\")\n",
    "        \n",
    "        # Plot histogram for character counts.\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(lengths, bins=20, edgecolor=\"black\")\n",
    "        plt.title(\"Character Count Distribution of Summaries\")\n",
    "        plt.xlabel(\"Character Count\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot histogram for token counts.\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(token_counts, bins=20, edgecolor=\"black\")\n",
    "        plt.title(\"Token Count Distribution of Summaries\")\n",
    "        plt.xlabel(\"Token Count\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Function to print histogram data numerically.\n",
    "        def print_histogram_data(data, bins=20, title=\"Histogram\"):\n",
    "            counts, bin_edges = np.histogram(data, bins=bins)\n",
    "            print(title)\n",
    "            for i in range(len(counts)):\n",
    "                lower = bin_edges[i]\n",
    "                upper = bin_edges[i + 1]\n",
    "                print(f\"{lower:5.1f} - {upper:5.1f}: {counts[i]}\")\n",
    "        \n",
    "        print_histogram_data(lengths, bins=20, title=\"Character Count Histogram Data\")\n",
    "        print_histogram_data(token_counts, bins=20, title=\"Token Count Histogram Data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.parse import quote\n",
    "\n",
    "# Base URL for the API endpoints\n",
    "BASE_URL = \"https://www.oekobaudat.de/OEKOBAU.DAT/resource/processes/categories/\"\n",
    "\n",
    "def get_categories(path):\n",
    "    \"\"\"\n",
    "    Fetches categories from the API.\n",
    "    - If path is empty, gets top-level categories.\n",
    "    - Otherwise, retrieves subcategories for the given path.\n",
    "    \"\"\"\n",
    "    if path:\n",
    "        # Encode each segment in the path to handle spaces and special characters\n",
    "        encoded_path = \"/\".join(quote(segment) for segment in path.split(\"/\"))\n",
    "        url = f\"{BASE_URL}{encoded_path}/subcategories/?catSystem=oekobau.dat&lang=en&sort=id\"\n",
    "    else:\n",
    "        url = f\"{BASE_URL}?catSystem=oekobau.dat&lang=en&sort=id\"\n",
    "    \n",
    "    # Debug print: Show the URL being requested\n",
    "    print(f\"Fetching URL: {url}\")\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Stop if the request fails\n",
    "    return ET.fromstring(response.content)\n",
    "\n",
    "def build_tree(path, xml_parent, level=0):\n",
    "    \"\"\"\n",
    "    Recursively builds the XML tree:\n",
    "    - 'path' is used to construct the API URL.\n",
    "    - 'xml_parent' is the XML element to which child categories will be added.\n",
    "    - 'level' indicates the depth for debugging purposes.\n",
    "    \"\"\"\n",
    "    indent = \"  \" * level\n",
    "    print(f\"{indent}Processing path: '{path or 'Top-level'}'\")\n",
    "    \n",
    "    try:\n",
    "        root = get_categories(path)\n",
    "    except Exception as e:\n",
    "        print(f\"{indent}Error retrieving categories for path '{path}': {e}\")\n",
    "        return\n",
    "\n",
    "    # Define namespace for parsing API XML\n",
    "    ns = {\"sapi\": \"http://www.ilcd-network.org/ILCD/ServiceAPI\"}\n",
    "    # Find all category elements regardless of their level\n",
    "    for cat in root.findall(\".//sapi:category\", ns):\n",
    "        cat_name = cat.text.strip() if cat.text else \"Unknown\"\n",
    "        cat_id = cat.get(\"classId\", \"N/A\")\n",
    "        print(f\"{indent}  Found category: {cat_name} (ID: {cat_id})\")\n",
    "        \n",
    "        # Create a new XML element in our output with attributes id and name\n",
    "        new_elem = ET.SubElement(xml_parent, \"category\", id=cat_id, name=cat_name)\n",
    "        # Construct new path for recursion\n",
    "        new_path = cat_name if not path else f\"{path}/{cat_name}\"\n",
    "        # Recursively build tree for subcategories\n",
    "        build_tree(new_path, new_elem, level + 1)\n",
    "\n",
    "def main():\n",
    "    # Register namespaces for the output file\n",
    "    ET.register_namespace(\"\", \"http://lca.jrc.it/ILCD/Categories\")\n",
    "    ET.register_namespace(\"common\", \"http://lca.jrc.it/ILCD/Common\")\n",
    "    ET.register_namespace(\"xsi\", \"http://www.w3.org/2001/XMLSchema-instance\")\n",
    "    \n",
    "    # Create the root element <CategorySystem> with attribute name=\"OEKOBAU.DAT\"\n",
    "    root_elem = ET.Element(\"CategorySystem\", {\"name\": \"OEKOBAU.DAT\"})\n",
    "    # Create the <categories> container with dataType=\"Process\"\n",
    "    categories_elem = ET.SubElement(root_elem, \"categories\", {\"dataType\": \"Process\"})\n",
    "    \n",
    "    print(\"Starting category tree traversal...\")\n",
    "    # Start building the tree from the top-level (empty path)\n",
    "    build_tree(\"\", categories_elem)\n",
    "    \n",
    "    # Write the constructed XML tree to a file with an XML declaration\n",
    "    tree = ET.ElementTree(root_elem)\n",
    "    output_file = \"categories.xml\"\n",
    "    tree.write(output_file, encoding=\"UTF-8\", xml_declaration=True)\n",
    "    print(f\"Finished. Categories saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.dom.minidom\n",
    "\n",
    "# Read the existing XML file\n",
    "with open(\"categories.xml\", \"rb\") as f:\n",
    "    xml_data = f.read()\n",
    "\n",
    "# Parse the XML using minidom\n",
    "dom = xml.dom.minidom.parseString(xml_data)\n",
    "\n",
    "# Pretty print the XML with desired indentation\n",
    "pretty_xml = dom.toprettyxml(indent=\"    \", encoding=\"UTF-8\")\n",
    "\n",
    "# Write the prettified XML to a new file\n",
    "with open(\"categories_pretty.xml\", \"wb\") as f:\n",
    "    f.write(pretty_xml)\n",
    "\n",
    "print(\"Prettified XML saved to categories_pretty.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.parse import quote\n",
    "import xml.dom.minidom\n",
    "\n",
    "# Optimize to not go past level 2\n",
    "# Problems getting all German categories because of umlauts in the URL \n",
    "\n",
    "# Base URL for the API endpoints\n",
    "BASE_URL = \"https://www.oekobaudat.de/OEKOBAU.DAT/resource/processes/categories/\"\n",
    "\n",
    "def get_categories(path):\n",
    "    \"\"\"\n",
    "    Fetches categories from the API.\n",
    "    - If path is empty, gets top-level categories.\n",
    "    - Otherwise, retrieves subcategories for the given path.\n",
    "    \"\"\"\n",
    "    if path:\n",
    "        encoded_path = \"/\".join(quote(segment) for segment in path.split(\"/\"))\n",
    "        url = f\"{BASE_URL}{encoded_path}/subcategories/?catSystem=oekobau.dat&lang=en&sort=id\"\n",
    "    else:\n",
    "        url = f\"{BASE_URL}?catSystem=oekobau.dat&lang=en&sort=id\"\n",
    "    \n",
    "    print(f\"Fetching URL: {url}\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise error for bad responses\n",
    "    return ET.fromstring(response.content)\n",
    "\n",
    "def build_tree(path, xml_parent, level=0):\n",
    "    \"\"\"\n",
    "    Recursively builds the XML tree:\n",
    "    - 'path' is used to construct the API URL.\n",
    "    - 'xml_parent' is the XML element to which child categories will be added.\n",
    "    - 'level' indicates the recursion depth (for debugging).\n",
    "    \"\"\"\n",
    "    indent = \"  \" * level\n",
    "    print(f\"{indent}Processing path: '{path or 'Top-level'}'\")\n",
    "    \n",
    "    try:\n",
    "        root = get_categories(path)\n",
    "    except Exception as e:\n",
    "        print(f\"{indent}Error retrieving categories for path '{path}': {e}\")\n",
    "        return\n",
    "\n",
    "    ns = {\"sapi\": \"http://www.ilcd-network.org/ILCD/ServiceAPI\"}\n",
    "    for cat in root.findall(\".//sapi:category\", ns):\n",
    "        cat_name = cat.text.strip() if cat.text else \"Unknown\"\n",
    "        cat_id = cat.get(\"classId\", \"N/A\")\n",
    "        print(f\"{indent}  Found category: {cat_name} (ID: {cat_id})\")\n",
    "        \n",
    "        new_elem = ET.SubElement(xml_parent, \"category\", id=cat_id, name=cat_name)\n",
    "        new_path = cat_name if not path else f\"{path}/{cat_name}\"\n",
    "        build_tree(new_path, new_elem, level + 1)\n",
    "\n",
    "def main():\n",
    "    # Register namespaces for the output file\n",
    "    ET.register_namespace(\"\", \"http://lca.jrc.it/ILCD/Categories\")\n",
    "    ET.register_namespace(\"common\", \"http://lca.jrc.it/ILCD/Common\")\n",
    "    ET.register_namespace(\"xsi\", \"http://www.w3.org/2001/XMLSchema-instance\")\n",
    "    \n",
    "    # Create the root element <CategorySystem> with attribute name=\"OEKOBAU.DAT\"\n",
    "    root_elem = ET.Element(\"CategorySystem\", {\"name\": \"OEKOBAU.DAT\"})\n",
    "    # Create the <categories> container with dataType=\"Process\"\n",
    "    categories_elem = ET.SubElement(root_elem, \"categories\", {\"dataType\": \"Process\"})\n",
    "    \n",
    "    print(\"Starting category tree traversal...\")\n",
    "    build_tree(\"\", categories_elem)\n",
    "    \n",
    "    # Convert the ElementTree to a string and pretty print it using minidom\n",
    "    xml_str = ET.tostring(root_elem, encoding=\"utf-8\")\n",
    "    parsed = xml.dom.minidom.parseString(xml_str)\n",
    "    pretty_xml_str = parsed.toprettyxml(indent=\"    \", encoding=\"UTF-8\")\n",
    "    \n",
    "    # Write the pretty XML to file\n",
    "    with open(\"categories_en_pretty.xml\", \"wb\") as f:\n",
    "        f.write(pretty_xml_str)\n",
    "    \n",
    "    print(\"Finished. Categories saved to categories.xml\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
