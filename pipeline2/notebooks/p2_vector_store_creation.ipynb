{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import statistics\n",
    "\n",
    "def traverse_category(element, parent_path, ns):\n",
    "    name = element.get(\"name\") or \"Unnamed\"\n",
    "    current_path = f\"{parent_path} > {name}\" if parent_path else name\n",
    "    children = element.findall(\"ns:category\", ns)\n",
    "    # Only return the current path if there are no child categories\n",
    "    if not children:\n",
    "        return [current_path]\n",
    "    texts = []\n",
    "    for child in children:\n",
    "        texts.extend(traverse_category(child, current_path, ns))\n",
    "    return texts\n",
    "\n",
    "# Parse XML file\n",
    "xml_file = \"../../data/pipeline2/xml/OEKOBAU.DAT_Categories.xml\"\n",
    "ns = {\"ns\": \"http://lca.jrc.it/ILCD/Categories\"}\n",
    "tree = ET.parse(xml_file)\n",
    "root = tree.getroot()\n",
    "categories_elem = root.find(\"ns:categories\", ns)\n",
    "texts = []\n",
    "if categories_elem is not None:\n",
    "    for cat in categories_elem.findall(\"ns:category\", ns):\n",
    "        texts.extend(traverse_category(cat, \"\", ns))\n",
    "else:\n",
    "    print(\"No categories found.\")\n",
    "\n",
    "# print(texts)\n",
    "\n",
    "# Calculate character and token counts\n",
    "lengths = [len(text) for text in texts]\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "token_counts = [len(enc.encode(text)) for text in texts]\n",
    "\n",
    "# Basic measures: Mean\n",
    "mean_length = statistics.mean(lengths) if lengths else 0\n",
    "mean_tokens = statistics.mean(token_counts) if token_counts else 0\n",
    "\n",
    "# Additional measures: Median, Standard Deviation, Percentiles, IQR\n",
    "median_length = np.median(lengths) if lengths else 0\n",
    "median_tokens = np.median(token_counts) if token_counts else 0\n",
    "\n",
    "std_length = np.std(lengths, ddof=1) if len(lengths) > 1 else 0\n",
    "std_tokens = np.std(token_counts, ddof=1) if len(token_counts) > 1 else 0\n",
    "\n",
    "percentiles_length = np.percentile(lengths, [25, 50, 75, 90, 95]) if lengths else [0]*5\n",
    "percentiles_tokens = np.percentile(token_counts, [25, 50, 75, 90, 95]) if token_counts else [0]*5\n",
    "\n",
    "iqr_length = percentiles_length[2] - percentiles_length[0]\n",
    "iqr_tokens = percentiles_tokens[2] - percentiles_tokens[0]\n",
    "\n",
    "# Print statistical measures\n",
    "print(f\"Mean character count: {mean_length:.2f}\")\n",
    "print(f\"Median character count: {median_length:.2f}\")\n",
    "print(f\"Standard deviation (chars): {std_length:.2f}\")\n",
    "print(f\"25th, 50th, 75th, 90th, 95th percentiles (chars): {percentiles_length}\")\n",
    "print(f\"IQR (chars): {iqr_length:.2f}\\n\")\n",
    "\n",
    "print(f\"Mean token count: {mean_tokens:.2f}\")\n",
    "print(f\"Median token count: {median_tokens:.2f}\")\n",
    "print(f\"Standard deviation (tokens): {std_tokens:.2f}\")\n",
    "print(f\"25th, 50th, 75th, 90th, 95th percentiles (tokens): {percentiles_tokens}\")\n",
    "print(f\"IQR (tokens): {iqr_tokens:.2f}\\n\")\n",
    "\n",
    "# Plot histograms for visual inspection\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(lengths, bins=20, edgecolor=\"black\")\n",
    "plt.title(\"Character Count Distribution\")\n",
    "plt.xlabel(\"Character Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(token_counts, bins=20, edgecolor=\"black\")\n",
    "plt.title(\"Token Count Distribution\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Print histogram data in numeric format\n",
    "def print_histogram_data(data, bins=20, title=\"Histogram\"):\n",
    "    counts, bin_edges = np.histogram(data, bins=bins)\n",
    "    print(title)\n",
    "    for i in range(len(counts)):\n",
    "        lower = bin_edges[i]\n",
    "        upper = bin_edges[i+1]\n",
    "        print(f\"{lower:5.1f} - {upper:5.1f}: {counts[i]}\")\n",
    "\n",
    "print_histogram_data(lengths, bins=20, title=\"Character Count Histogram Data\")\n",
    "print_histogram_data(token_counts, bins=20, title=\"Token Count Histogram Data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "\n",
    "\n",
    "class CustomSentenceTransformerEmbeddings(Embeddings):\n",
    "    \"\"\"\n",
    "    Allows using a SentenceTransformer model within a LangChain-based FAISS store.\n",
    "    Handles initialization of different models with specific arguments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model_name: str):\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.model = self._initialize_model()\n",
    "\n",
    "    def _initialize_model(self) -> SentenceTransformer:\n",
    "        \"\"\"\n",
    "        Initializes the SentenceTransformer model based on the embedding_model_name.\n",
    "        \"\"\"\n",
    "        # Define initialization configurations for each model\n",
    "        model_configs = {\n",
    "            \"jinaai/jina-embeddings-v3\": {\n",
    "                \"trust_remote_code\": True,\n",
    "                \"revision\": \"main\",\n",
    "                \"device\": \"cuda\",\n",
    "                \"model_kwargs\": {\"use_flash_attn\": False},\n",
    "            },\n",
    "            \"HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5\": {\n",
    "                \"local_files_only\": True,\n",
    "                \"device\": \"cuda\",\n",
    "                \"model_kwargs\": {\"attn_implementation\": \"eager\"},\n",
    "            },\n",
    "            \"Alibaba-NLP/gte-large-en-v1.5\": {\n",
    "                \"trust_remote_code\": True,\n",
    "                \"revision\": \"main\",\n",
    "                \"device\": \"cuda\",\n",
    "                \"model_kwargs\": {\"attn_implementation\": \"eager\"},\n",
    "            },\n",
    "        }\n",
    "\n",
    "        config = model_configs.get(\n",
    "            self.embedding_model_name,\n",
    "            {\"device\": \"cuda\", \"model_kwargs\": {}},  # Default device\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            model = SentenceTransformer(self.embedding_model_name, **config)\n",
    "            print(f\"Initialized SentenceTransformer model: {self.embedding_model_name}\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing model {self.embedding_model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text).tolist()\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(texts).tolist()\n",
    "\n",
    "    def unload_model(self):\n",
    "        \"\"\"\n",
    "        Remove the model from memory after processing to free up GPU resources.\n",
    "        \"\"\"\n",
    "        if self.model:\n",
    "            del self.model  # Delete the model instance\n",
    "            self.model = None  # Ensure the reference is cleared\n",
    "            torch.cuda.empty_cache()  # Clear the GPU cache\n",
    "            gc.collect()  # Run garbage collection\n",
    "            print(f\"Unloaded embedding model: {self.embedding_model_name}\")\n",
    "        else:\n",
    "            print(\"[DEBUG] Embedding model was already None or not set.\")\n",
    "\n",
    "\n",
    "def get_documents_from_xml(xml_path: str) -> List[Document]:\n",
    "    ns = {\"ns\": \"http://lca.jrc.it/ILCD/Categories\"}\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    categories_elem = root.find(\"ns:categories\", ns)\n",
    "    documents = []\n",
    "    if categories_elem is None:\n",
    "        print(\"No categories found in the XML file.\")\n",
    "        return documents\n",
    "    for cat in categories_elem.findall(\"ns:category\", ns):\n",
    "        documents.extend(traverse_leaf_category(cat, \"\", ns))\n",
    "    return documents\n",
    "\n",
    "\n",
    "def traverse_leaf_category(element, parent_path, ns) -> List[Document]:\n",
    "    name = element.get(\"name\") or \"Unnamed\"\n",
    "    current_path = f\"{parent_path} > {name}\" if parent_path else name\n",
    "    children = element.findall(\"ns:category\", ns)\n",
    "    if children:\n",
    "        docs = []\n",
    "        for child in children:\n",
    "            docs.extend(traverse_leaf_category(child, current_path, ns))\n",
    "        return docs\n",
    "    else:\n",
    "        parts = current_path.split(\" > \")\n",
    "        metadata = {}\n",
    "        if len(parts) >= 1:\n",
    "            metadata[\"h1\"] = parts[0]\n",
    "        if len(parts) >= 2:\n",
    "            metadata[\"h2\"] = parts[1]\n",
    "        if len(parts) >= 3:\n",
    "            metadata[\"h3\"] = parts[2]\n",
    "        return [Document(page_content=current_path, metadata=metadata)]\n",
    "\n",
    "\n",
    "def save_chunks(chunks: List[Document], output_file: str):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            f.write(\n",
    "                f\"Chunk {i+1}:\\n{chunk.page_content}\\nMetadata: {chunk.metadata}\\n{'-'*50}\\n\"\n",
    "            )\n",
    "    print(f\"Chunks saved to {output_file}\")\n",
    "\n",
    "\n",
    "def process_vectorstore(\n",
    "    chunks: List[Document],\n",
    "    model: str,\n",
    "    vectorstore_path: str,\n",
    "    chunk_size: int = 150,\n",
    "):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=0\n",
    "    )\n",
    "    all_splits = text_splitter.split_documents(chunks)\n",
    "\n",
    "    # Load the embedding model\n",
    "    if model in [\n",
    "        \"jinaai/jina-embeddings-v3\",\n",
    "        \"HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5\",\n",
    "        \"Alibaba-NLP/gte-large-en-v1.5\",\n",
    "    ]:\n",
    "        embeddings = CustomSentenceTransformerEmbeddings(\n",
    "            embedding_model_name=model,\n",
    "        )\n",
    "    else:\n",
    "        embeddings = OllamaEmbeddings(model=model)\n",
    "        print(f\"Initialized Ollama embedding model: {model}\")\n",
    "\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        all_splits,\n",
    "        embedding=embeddings,\n",
    "        distance_strategy=DistanceStrategy.COSINE,\n",
    "    )\n",
    "    vectorstore.save_local(vectorstore_path)\n",
    "    print(f\"Vector store saved to {vectorstore_path}\")\n",
    "\n",
    "    if isinstance(embeddings, CustomSentenceTransformerEmbeddings):\n",
    "        embeddings.unload_model()\n",
    "\n",
    "\n",
    "#########\n",
    "# Usage\n",
    "#########\n",
    "xml_file = \"../../data/pipeline2/xml/OEKOBAU.DAT_Categories_EN_aligned.xml\"\n",
    "documents = get_documents_from_xml(xml_file)\n",
    "chunk_size = 150  # Chosen to cover the maximum token count of a category\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "output_dir = \"../../data/pipeline2/chunks\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "chunk_file = os.path.join(output_dir, f\"chunks_cs{chunk_size}.txt\")\n",
    "save_chunks(chunks, chunk_file)\n",
    "\n",
    "models = [\n",
    "    # \"bge-m3:latest\",\n",
    "    # \"snowflake-arctic-embed2:latest\",\n",
    "    # \"jina/jina-embeddings-v2-base-de:latest\",\n",
    "    # \"paraphrase-multilingual:latest\",\n",
    "    # \"jeffh/intfloat-multilingual-e5-large-instruct:f32\",\n",
    "    \"granite-embedding:278m\",\n",
    "    \"granite-embedding:30m\",\n",
    "    # \"bge-large:latest\",\n",
    "    # \"mxbai-embed-large:latest\",\n",
    "    # \"jinaai/jina-embeddings-v3\",\n",
    "    # \"HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5\",\n",
    "    # \"Alibaba-NLP/gte-large-en-v1.5\",\n",
    "]\n",
    "for model in models:\n",
    "    if model == \"jinaai/jina-embeddings-v3\":\n",
    "        model_name = model.replace(\"/\", \"_\")\n",
    "    elif model.startswith(\"HIT-TMG\") or model.startswith(\"Alibaba-NLP\"):\n",
    "        model_name = model.replace(\"/\", \"_\")\n",
    "    elif model.startswith(\"granite-embedding\"):\n",
    "        model_name = model.replace(\":\", \"-\")\n",
    "    elif \":\" in model:\n",
    "        model_name = model.split(\":\")[0].replace(\"/\", \"_\")\n",
    "    else:\n",
    "        model_name = model.replace(\"/\", \"_\")\n",
    "    # chunk_strat = f\"row_cs{chunk_size}_co0\"\n",
    "    vectorstore_dir = os.path.join(\n",
    "        \"..\", \"..\", \"embeddings\", \"pipeline2\", model_name, f\"faiss_index_COS_EN\"\n",
    "    )\n",
    "    process_vectorstore(\n",
    "        chunks,\n",
    "        model,\n",
    "        vectorstore_dir,\n",
    "        chunk_size=chunk_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [print(chunk.page_content) for chunk in chunks]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
