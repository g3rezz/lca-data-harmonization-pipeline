{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIN 276 cost groups matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, AfterValidator\n",
    "from typing import Annotated, List\n",
    "\n",
    "# 1) Load your API key\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment.\")\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Pydantic model to enforce structured output: we expect a list of DIN 276 codes\n",
    "def validate_three_digit_numeric(v: List[str]) -> List[str]:\n",
    "    for item in v:\n",
    "        # Enforce that each code is a three-digit numeric string\n",
    "        if not (item.isdigit() and len(item) == 3):\n",
    "            raise ValueError(f\"'{item}' is not a three-digit number.\")\n",
    "    return v\n",
    "\n",
    "# Use Annotated with AfterValidator to enforce the constraint on cost_group_codes\n",
    "CostGroupCodes = Annotated[List[str], AfterValidator(validate_three_digit_numeric)]\n",
    "\n",
    "class Din276CostGroupResponse(BaseModel):\n",
    "    cost_group_codes: CostGroupCodes\n",
    "\n",
    "\n",
    "# 3) Read the DIN 276 cost groups from CSV\n",
    "din276_costgroups = []\n",
    "csv_file_path = \"../../data/pipeline2/csv/din276_concrete_sub.csv\"\n",
    "with open(csv_file_path, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        din276_costgroups.append({\n",
    "            \"nr\": row[\"Nr\"],\n",
    "            \"cg_name\": row[\"Cost group (CG)\"],\n",
    "            \"notes\": row[\"Notes\"]\n",
    "        })\n",
    "\n",
    "# 4) Prepare your EPD data.\n",
    "epd_file_path = \"../../data/pipeline2/json/edited_epds.jsonl\"\n",
    "epds = []\n",
    "with open(epd_file_path, \"r\", encoding=\"utf-8\") as epd_file:\n",
    "    for line in epd_file:\n",
    "        if line.strip():\n",
    "            epds.append(json.loads(line))\n",
    "\n",
    "# Get only the first N EPDs\n",
    "# epds = epds[:3]\n",
    "\n",
    "# 5) Define a function to extract EPD details according to the provided paths\n",
    "def extract_epd_details(epd_item: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Extract required EPD fields:\n",
    "      - Product\n",
    "      - Category\n",
    "      - Description\n",
    "      - Applicability\n",
    "      - Compressive Strength\n",
    "      - Bulk Density\n",
    "      - Flow Property\n",
    "    \"\"\"\n",
    "    doc = epd_item.get(\"document\", {})\n",
    "    process_info = doc.get(\"processInformation\", {})\n",
    "    \n",
    "    # Product path\n",
    "    product = (\n",
    "        process_info\n",
    "        .get(\"dataSetInformation\", {})\n",
    "        .get(\"name\", {})\n",
    "        .get(\"baseName\", [{}])[0]\n",
    "        .get(\"value\", \"\")\n",
    "    )\n",
    "\n",
    "    # Category (static for demonstration, or can be read from the EPD if needed)\n",
    "    category = \"Mineral building products > Mortar and Concrete > Ready mixed concrete\"\n",
    "\n",
    "    # Description path\n",
    "    description = (\n",
    "        process_info\n",
    "        .get(\"technology\", {})\n",
    "        .get(\"technologyDescriptionAndIncludedProcesses\", [{}])[0]\n",
    "        .get(\"value\", \"\")\n",
    "    )\n",
    "\n",
    "    # Applicability path\n",
    "    applicability = (\n",
    "        process_info\n",
    "        .get(\"technology\", {})\n",
    "        .get(\"technologicalApplicability\", [{}])[0]\n",
    "        .get(\"value\", \"\")\n",
    "    )\n",
    "\n",
    "    # Compressive Strength & Bulk Density\n",
    "    compressive_strength = \"\"\n",
    "    bulk_density = \"\"\n",
    "    exchanges = doc.get(\"exchanges\", {}).get(\"exchange\", [])\n",
    "    \n",
    "    if exchanges:\n",
    "        material_props = exchanges[0].get(\"materialProperties\", [])\n",
    "        for prop in material_props:\n",
    "            prop_name = prop.get(\"name\", [{}]).lower()\n",
    "            prop_value = prop.get(\"value\", \"\")\n",
    "            prop_unit = prop.get(\"unit\", \"\")\n",
    "            if \"compressive\" in prop_name:\n",
    "                compressive_strength = f\"{prop_value} {prop_unit}\"\n",
    "            elif \"density\" in prop_name:\n",
    "                bulk_density = f\"{prop_value} {prop_unit}\"\n",
    "\n",
    "    # Flow Property\n",
    "    flow_property = \"\"\n",
    "    if exchanges:\n",
    "        flow_props = exchanges[0].get(\"flowProperties\", [])\n",
    "        if flow_props:\n",
    "            fp = flow_props[0]\n",
    "            fp_name = fp.get(\"name\", [{}])[0].get(\"value\", \"\")\n",
    "            fp_mean_val = fp.get(\"meanValue\", \"\")\n",
    "            fp_ref_unit = fp.get(\"referenceUnit\", \"\")\n",
    "            flow_property = f\"{fp_mean_val} {fp_ref_unit} ({fp_name})\"\n",
    "\n",
    "    # Return a dictionary of all extracted data\n",
    "    return {\n",
    "        \"Product\": product,\n",
    "        \"Category\": category,\n",
    "        \"Description\": description,\n",
    "        \"Applicability\": applicability,\n",
    "        \"Compressive Strength\": compressive_strength,\n",
    "        \"Bulk Density\": bulk_density,\n",
    "        \"Flow Property\": flow_property\n",
    "    }\n",
    "\n",
    "# 6) Build our base system prompt\n",
    "system_prompt = \"\"\"\\\n",
    "You are an expert in construction cost classification with in-depth knowledge of DIN 276 cost groups and Environmental Product Declarations (EPDs). In the DIN 276 classification, cost groups are organized hierarchically into parent and child groups. The parent groups are identified by the codes 310, 320, 330, 340, 350, 360, and 370. If you list a parent group, you must also include at least one corresponding child group to ensure a thorough classification. Your task is to thoroughly analyze the provided EPD details, evaluate both primary and secondary cost factors, and determine all applicable DIN 276 cost group codes by listing both parent and child groups where relevant.\n",
    "\"\"\"\n",
    "\n",
    "# 7) Utility function to build each request\n",
    "def build_request(i, epd_item):\n",
    "    # Extract EPD details\n",
    "    details = extract_epd_details(epd_item)\n",
    "    # Remove keys with \"N/A\" or empty strings\n",
    "    clean_details = {k: v for k, v in details.items() if v and v != \"\"}\n",
    "\n",
    "    # Build context string using the specified fields\n",
    "    fields = [\n",
    "        (\"Product\", \"Product\"),\n",
    "        (\"Category\", \"Category\"),\n",
    "        (\"Description\", \"Description\"),\n",
    "        (\"Applicability\", \"Applicability\"),\n",
    "        (\"Compressive Strength\", \"Compressive Strength\"),\n",
    "        (\"Bulk Density\", \"Bulk Density\"),\n",
    "        (\"Flow Property\", \"Flow Property\"),\n",
    "    ]\n",
    "    context_parts = [\n",
    "        f\"- {label}: {clean_details.get(key, '')}\"\n",
    "        for key, label in fields if clean_details.get(key, '')\n",
    "    ]\n",
    "\n",
    "    # Build cost groups context from CSV data\n",
    "    cost_groups_str = \"\\n\".join(\n",
    "        f\"- ({group['nr']}) {group['cg_name']}: {group['notes']}\"\n",
    "        for group in din276_costgroups\n",
    "    )\n",
    "\n",
    "    # Assemble the final prompt\n",
    "    final_prompt = (\n",
    "        \"Product Details:\\n\"\n",
    "        + \"\\n\".join(context_parts)\n",
    "        + \"\\n\\nDIN 276 Cost Groups:\\n\"\n",
    "        + cost_groups_str\n",
    "        + \"\\n\\nWhich cost group codes are applicable?\"\n",
    "        + \"\\n\\nPlease respond in valid JSON format exactly as specified, \"\n",
    "        \"with only a key 'cost_group_codes' whose value is a list of three-digit numeric codes.\"\n",
    "    )\n",
    "\n",
    "    # Print final prompt for debugging\n",
    "    print(\"\\n============ Final Prompt ============\")\n",
    "    print(final_prompt)\n",
    "    print(\"======================================\\n\")\n",
    "\n",
    "    # Generate JSON schema from the Pydantic model and set additionalProperties to false\n",
    "    schema = Din276CostGroupResponse.model_json_schema()\n",
    "    schema[\"additionalProperties\"] = False\n",
    "\n",
    "    # Return a request dictionary\n",
    "    return {\n",
    "        \"custom_id\": f\"{i}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"o3-mini\",\n",
    "            \"reasoning_effort\": \"high\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": final_prompt},\n",
    "            ],\n",
    "            \"response_format\": {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"Din276CostGroupResponse\",\n",
    "                    \"schema\": schema,\n",
    "                    \"strict\": True,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "# 8) Create the .jsonl file with one request per EPD\n",
    "input_jsonl_file = \"../../data/pipeline2/json/openai/batch_input.jsonl\"\n",
    "with open(input_jsonl_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    for i, epd in enumerate(epds, start=1):\n",
    "        request_obj = build_request(i, epd)\n",
    "        out_file.write(json.dumps(request_obj) + \"\\n\")\n",
    "\n",
    "print(f\"Created '{input_jsonl_file}' with {len(epds)} lines for batch processing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Upload the input file to OpenAI for batch processing\n",
    "batch_input_file = client.files.create(\n",
    "    file=open(input_jsonl_file, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "print(\"Uploaded file:\", batch_input_file)\n",
    "\n",
    "# 5) Create the batch job (with a 24-hour completion window)\n",
    "batch = client.batches.create(\n",
    "    input_file_id=batch_input_file.id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\"description\": \"EPD DIN 276 cost group classification (2025-04-05_1728)\"}\n",
    ")\n",
    "print(\"Created Batch:\", batch)\n",
    "\n",
    "# 6) Save the batch id for later status checks\n",
    "batch_id = batch.id\n",
    "print(f\"Batch submitted with id: {batch_id}.\")\n",
    "print(\"You can check the status later with:\")\n",
    "print(f\"client.batches.retrieve('{batch_id}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual batch id if needed\n",
    "batch_id = \"batch_68178796ad60819093950a4807f772f9\"  # for example\n",
    "batch_status = client.batches.retrieve(batch_id)\n",
    "print(\"Current batch status:\", batch_status.status)\n",
    "\n",
    "if batch_status.status == \"completed\":\n",
    "    output_file_id = batch_status.output_file_id\n",
    "    if output_file_id:\n",
    "        output_jsonl_file = f\"../../data/pipeline2/json/openai/{batch_id}_output.jsonl\"\n",
    "        file_response = client.files.content(output_file_id)\n",
    "        with open(output_jsonl_file, \"wb\") as f:\n",
    "            f.write(file_response.content)\n",
    "        print(f\"Batch results saved to {output_jsonl_file}\")\n",
    "    else:\n",
    "        print(\"No output file available yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the file path into directory and file name\n",
    "dir_name, file_name = os.path.split(input_jsonl_file)\n",
    "\n",
    "# Create a new file name with batch_id in front\n",
    "new_file_name = f\"{batch_id}_input.jsonl\"\n",
    "new_input_jsonl_file = os.path.join(dir_name, new_file_name)\n",
    "\n",
    "# Rename the file\n",
    "os.rename(input_jsonl_file, new_input_jsonl_file)\n",
    "\n",
    "print(f\"Input file renamed to: {new_input_jsonl_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "for batch in list(client.batches.list()):\n",
    "    pprint.pprint(batch.model_dump())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check error file\n",
    "error_file_id = \"file-5iENHkNczXuUZTTNXgP7qu\"\n",
    "error_file = client.files.content(error_file_id)\n",
    "print(error_file.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input_jsonl_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "from pydantic import ValidationError\n",
    "\n",
    "# List to hold validated outputs\n",
    "validated_outputs = []\n",
    "\n",
    "new_input_jsonl_file = \"../../data/pipeline2/json/openai/batch_67d5a00f7f2c8190a0e2cdc3cf04382b_output.jsonl\"\n",
    "\n",
    "# Open the JSONL output file using jsonlines\n",
    "with jsonlines.open(new_input_jsonl_file, mode='r') as reader:\n",
    "    for record in reader:\n",
    "        # For debugging: print the keys in the record\n",
    "        print(\"Record custom_id:\", record.get(\"custom_id\"), \"keys:\", record.keys())\n",
    "        \n",
    "        try:\n",
    "            # Extract the LLM answer from the nested structure\n",
    "            answer_content = record[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        except (KeyError, IndexError) as e:\n",
    "            print(\"Error extracting answer for record with custom_id:\", record.get(\"custom_id\"))\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Validate the response using the Pydantic model\n",
    "            validated = Din276CostGroupResponse.model_validate_json(answer_content)\n",
    "            validated_outputs.append(validated)\n",
    "            print(\"Valid output for custom_id\", record.get(\"custom_id\"), \":\", validated)\n",
    "        except ValidationError as e:\n",
    "            print(\"Validation error for record custom_id:\", record.get(\"custom_id\"), \"Error:\", e)\n",
    "\n",
    "# validated_outputs now holds all successfully validated responses\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
