{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = \"../../data/pipeline2/sql/filtered_epd_data05.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Replace missing database names with a placeholder and convert to lowercase\n",
    "df['Database Name Lower'] = df['Database Name'].fillna(\"Missing\").str.lower()\n",
    "\n",
    "# Count the occurrences of each unique database name (case insensitive)\n",
    "db_counts = df['Database Name Lower'].value_counts()\n",
    "\n",
    "# Plotting the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "db_counts.plot(kind='bar')\n",
    "plt.xlabel('Database Name')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Bar Chart of Database Name (Case Insensitive) Including Missing Values')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = \"../../data/pipeline2/sql/filtered_epd_data05.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Replace missing LCA Method Details with a placeholder \"Missing\"\n",
    "df['LCA Method Details Clean'] = df['LCA Method Details'].fillna(\"Missing\")\n",
    "\n",
    "# Count the occurrences of each unique LCA Method Details value\n",
    "lca_counts = df['LCA Method Details Clean'].value_counts()\n",
    "\n",
    "# Get distinct colors from the tab20 colormap\n",
    "cmap = plt.get_cmap(\"tab20\", len(lca_counts))\n",
    "colors = [cmap(i) for i in range(len(lca_counts))]\n",
    "\n",
    "# Create a stacked bar chart with a single bar segmented by each category\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "left = 0  # starting point for stacking each segment\n",
    "\n",
    "for cat, count, color in zip(lca_counts.index, lca_counts.values, colors):\n",
    "    ax.bar(\"LCA Method Details\", count, bottom=left, color=color, label=cat)\n",
    "    left += count\n",
    "\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Stacked Bar Chart of LCA Method Details (Including Missing Values)\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title=\"LCA Method Details\")\n",
    "plt.show()\n",
    "\n",
    "# Print the counts for each LCA Method Details\n",
    "print(\"Counts for each LCA Method Details:\")\n",
    "for category, count in lca_counts.items():\n",
    "    print(f\"{category}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. READ AND CLEAN DATA\n",
    "# ------------------------------------------------------------\n",
    "file_path = \"../../data/pipeline2/sql/filtered_epd_data05.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Working column for the LCA Method Details\n",
    "df[\"LCA_Method\"] = df[\"LCA Method Details\"].fillna(\"Missing\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. IDENTIFY SMALL CATEGORIES AND MERGE THEM\n",
    "# ------------------------------------------------------------\n",
    "THRESHOLD = 20  # categories with fewer than 20 items become \"Other\"\n",
    "category_counts = df[\"LCA_Method\"].value_counts()\n",
    "small_categories = category_counts[category_counts < THRESHOLD].index\n",
    "df[\"PCR_Group\"] = df[\"LCA_Method\"].where(~df[\"LCA_Method\"].isin(small_categories), \"Other\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. SETUP FOR STRATIFIED SAMPLING\n",
    "# ------------------------------------------------------------\n",
    "SAMPLE_SIZE = 100\n",
    "\n",
    "merged_counts = df[\"PCR_Group\"].value_counts()\n",
    "total_count = merged_counts.sum()\n",
    "\n",
    "# We'll store final sample sizes in a dictionary like {group_name: n_desired}\n",
    "allocations = {}\n",
    "fractions = {}\n",
    "\n",
    "# 3a. Compute fractional (floating) allocation for each group\n",
    "for pcr_category, count in merged_counts.items():\n",
    "    proportion = count / total_count\n",
    "    frac_allocation = proportion * SAMPLE_SIZE\n",
    "    fractions[pcr_category] = frac_allocation\n",
    "\n",
    "# 3b. First pass: floor allocations\n",
    "#    We'll keep track of how many rows each group can possibly provide\n",
    "floor_allocations = {}\n",
    "total_floors = 0\n",
    "for group, frac in fractions.items():\n",
    "    floor_val = int(np.floor(frac))\n",
    "    floor_allocations[group] = floor_val\n",
    "    total_floors += floor_val\n",
    "\n",
    "# 3c. Distribute leftover based on largest fractional remainders\n",
    "leftover = SAMPLE_SIZE - total_floors  # how many we still need to add\n",
    "if leftover > 0:\n",
    "    # Sort categories by fractional remainder (descending)\n",
    "    # remainder = frac_allocation - floor_allocation\n",
    "    remainders = [(group, fractions[group] - floor_allocations[group]) for group in fractions]\n",
    "    remainders.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # We add 1 to the top \"leftover\" groups, if they have capacity\n",
    "    idx = 0\n",
    "    while leftover > 0 and idx < len(remainders):\n",
    "        group, remainder_val = remainders[idx]\n",
    "        idx += 1\n",
    "        \n",
    "        # See if group has enough rows to support +1\n",
    "        group_df = df[df[\"PCR_Group\"] == group]\n",
    "        if floor_allocations[group] + 1 <= len(group_df):\n",
    "            floor_allocations[group] += 1\n",
    "            leftover -= 1\n",
    "\n",
    "# Now floor_allocations should sum to between SAMPLE_SIZE and more, \n",
    "# but we might still be short if some groups didn't have capacity. \n",
    "# We'll handle that next.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. SECOND PASS TO ENSURE EXACT SAMPLE_SIZE\n",
    "#    Check if any group can't fulfill its allocated n_desired.\n",
    "#    Reallocate shortfalls to other groups that have leftover capacity.\n",
    "# ------------------------------------------------------------\n",
    "def capacity(group):\n",
    "    \"\"\"Return how many rows are available for a group.\"\"\"\n",
    "    return len(df[df[\"PCR_Group\"] == group])\n",
    "\n",
    "# 4a. Identify if some groups are over-allocated (alloc > capacity)\n",
    "#     or if there's still leftover to add because we didn't reach 100 \n",
    "#     (in case groups didn't have capacity).\n",
    "over_allocated_groups = []\n",
    "underfilled_shortfall = 0\n",
    "\n",
    "for group, alloc in floor_allocations.items():\n",
    "    cap = capacity(group)\n",
    "    if alloc > cap:\n",
    "        # We'll take as many as we can\n",
    "        underfilled_shortfall += (alloc - cap)\n",
    "        floor_allocations[group] = cap\n",
    "\n",
    "# 4b. Re-check leftover or shortage\n",
    "current_total = sum(floor_allocations.values())\n",
    "difference = SAMPLE_SIZE - current_total\n",
    "\n",
    "if difference > 0:\n",
    "    # We still need to add 'difference' samples to some groups if possible.\n",
    "    # Let's see if there's capacity among the other groups to add more.\n",
    "    # We'll sort them by leftover capacity (descending).\n",
    "    groups_by_capacity = [(g, capacity(g) - floor_allocations[g]) for g in floor_allocations]\n",
    "    groups_by_capacity.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    idx = 0\n",
    "    while difference > 0 and idx < len(groups_by_capacity):\n",
    "        g, free_cap = groups_by_capacity[idx]\n",
    "        idx += 1\n",
    "        if free_cap > 0:\n",
    "            # We can allocate up to 'free_cap' extra to group g\n",
    "            add_amount = min(free_cap, difference)\n",
    "            floor_allocations[g] += add_amount\n",
    "            difference -= add_amount\n",
    "\n",
    "elif difference < 0:\n",
    "    # We allocated too many somehow; remove extras from groups with the biggest allocations\n",
    "    # or the largest difference from capacity.\n",
    "    # We'll do a simple approach: remove from the largest groups until we're at 100.\n",
    "    difference = -difference  # make it positive\n",
    "    groups_by_alloc = [(g, floor_allocations[g]) for g in floor_allocations]\n",
    "    groups_by_alloc.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    idx = 0\n",
    "    while difference > 0 and idx < len(groups_by_alloc):\n",
    "        g, allocated = groups_by_alloc[idx]\n",
    "        idx += 1\n",
    "        if allocated > 0:\n",
    "            remove_amt = min(allocated, difference)\n",
    "            floor_allocations[g] -= remove_amt\n",
    "            difference -= remove_amt\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. DRAW SAMPLES\n",
    "# ------------------------------------------------------------\n",
    "final_samples = []\n",
    "\n",
    "for group, n_desired in floor_allocations.items():\n",
    "    group_df = df[df[\"PCR_Group\"] == group]\n",
    "    # If n_desired <= length of group_df, sample n_desired\n",
    "    # else sample the full group (though we tried to avoid this scenario).\n",
    "    n_take = min(len(group_df), n_desired)\n",
    "    sample_df = group_df.sample(n=n_take, random_state=42)\n",
    "    final_samples.append(sample_df)\n",
    "\n",
    "final_sample_df = pd.concat(final_samples, ignore_index=True)\n",
    "\n",
    "# Sanity check the final size\n",
    "final_count = len(final_sample_df)\n",
    "print(f\"Final sample size = {final_count}\")\n",
    "\n",
    "if final_count != SAMPLE_SIZE:\n",
    "    print(f\"WARNING: We still did not get exactly {SAMPLE_SIZE} items. Possibly some categories lacked enough rows to fill up.\")\n",
    "    print(\"Consider investigating or relaxing constraints / threshold.\")\n",
    "else:\n",
    "    print(\"Success! Reached the exact sample size.\")\n",
    "\n",
    "print(\"\\n------ Sampling Summary ------\")\n",
    "# Assuming merged_counts and total_count have already been computed as in your code.\n",
    "# Here, total_count is the sum of all groups (i.e., merged_counts.sum()).\n",
    "total_capacity = total_count  # Total count across all groups after merging\n",
    "\n",
    "# Create a list to hold our summary data\n",
    "summary_data = []\n",
    "\n",
    "# Print the header and a separator\n",
    "print(f\"| {'PCR':70} | {'Count':>10} | {'% of Total':>10} | {'EPDs to Sample':>15} |\")\n",
    "print(\"|\" + \"-\"*72 + \"|\" + \"-\"*12 + \"|\" + \"-\"*12 + \"|\" + \"-\"*17 + \"|\")\n",
    "\n",
    "# For each group, get the capacity (i.e., how many EPDs are available),\n",
    "# calculate the percentage, and print the allocated (sampled) number.\n",
    "for group, allocated in floor_allocations.items():\n",
    "    cap = capacity(group)\n",
    "    pct = cap / total_capacity * 100\n",
    "    print(f\"| {group[:70]:70} | {cap:10d} | {pct:9.0f}% | {allocated:15d} |\")\n",
    "    summary_data.append({\n",
    "        \"PCR\": group,\n",
    "        \"Count\": cap,\n",
    "        \"% of Total\": f\"{pct:.0f}%\",  # format percentage with no decimal places\n",
    "        \"EPDs to Sample\": allocated\n",
    "    })\n",
    "\n",
    "# Create a new DataFrame from the summary data list\n",
    "allocation_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Drop temporary columns\n",
    "final_sample_df = final_sample_df.drop(columns=[\"LCA_Method\", \"PCR_Group\"])\n",
    "\n",
    "# Add an 'id' column starting at 1 and incrementing by 1 for each row.\n",
    "final_sample_df.insert(0, 'Id', range(1, len(final_sample_df) + 1))\n",
    "\n",
    "# (Optional) Save\n",
    "final_sample_df.to_csv(\"../../data/pipeline2/csv/sample_100_repres_EPDs.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allocation_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
